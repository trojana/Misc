{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN network for sentiment analysis\n",
    "network inspired by this [post](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=1.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 1.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from sentiment RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load('../deep-learning/sentiment-rnn/train_x.npy')\n",
    "x_dev = np.load('../deep-learning/sentiment-rnn/val_x.npy')\n",
    "x_test = np.load('../deep-learning/sentiment-rnn/test_x.npy')\n",
    "y_train = np.load('../deep-learning/sentiment-rnn/train_y.npy')\n",
    "y_dev = np.load('../deep-learning/sentiment-rnn/val_y.npy')\n",
    "y_test = np.load('../deep-learning/sentiment-rnn/test_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ..., 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\n",
      "\n",
      "2017-08-02T10:26:48.543603: step 1, loss 5.05179, acc 0.484375\n",
      "2017-08-02T10:26:49.125661: step 2, loss 6.23726, acc 0.453125\n",
      "2017-08-02T10:26:49.694718: step 3, loss 4.558, acc 0.546875\n",
      "2017-08-02T10:26:50.257775: step 4, loss 4.50958, acc 0.46875\n",
      "2017-08-02T10:26:50.823831: step 5, loss 4.71148, acc 0.4375\n",
      "2017-08-02T10:26:51.387888: step 6, loss 5.00293, acc 0.53125\n",
      "2017-08-02T10:26:51.946943: step 7, loss 4.67384, acc 0.546875\n",
      "2017-08-02T10:26:52.509000: step 8, loss 5.30527, acc 0.375\n",
      "2017-08-02T10:26:53.057054: step 9, loss 4.45393, acc 0.515625\n",
      "2017-08-02T10:26:53.619111: step 10, loss 4.35755, acc 0.578125\n",
      "2017-08-02T10:26:54.182167: step 11, loss 5.10907, acc 0.453125\n",
      "2017-08-02T10:26:54.750224: step 12, loss 5.00317, acc 0.453125\n",
      "2017-08-02T10:26:55.372286: step 13, loss 4.62078, acc 0.390625\n",
      "2017-08-02T10:26:55.937342: step 14, loss 4.60473, acc 0.546875\n",
      "2017-08-02T10:26:56.494398: step 15, loss 4.97766, acc 0.515625\n",
      "2017-08-02T10:26:57.050454: step 16, loss 4.66501, acc 0.484375\n",
      "2017-08-02T10:26:57.609510: step 17, loss 3.70671, acc 0.703125\n",
      "2017-08-02T10:26:58.161565: step 18, loss 4.31096, acc 0.515625\n",
      "2017-08-02T10:26:58.719621: step 19, loss 4.19469, acc 0.65625\n",
      "2017-08-02T10:26:59.267675: step 20, loss 4.36349, acc 0.5\n",
      "2017-08-02T10:26:59.847733: step 21, loss 4.1633, acc 0.59375\n",
      "2017-08-02T10:27:00.431792: step 22, loss 4.34395, acc 0.484375\n",
      "2017-08-02T10:27:00.980847: step 23, loss 4.76203, acc 0.453125\n",
      "2017-08-02T10:27:01.539903: step 24, loss 4.56993, acc 0.4375\n",
      "2017-08-02T10:27:02.111960: step 25, loss 4.49349, acc 0.578125\n",
      "2017-08-02T10:27:02.736022: step 26, loss 4.51756, acc 0.5\n",
      "2017-08-02T10:27:03.292078: step 27, loss 3.87522, acc 0.484375\n",
      "2017-08-02T10:27:03.853134: step 28, loss 3.58347, acc 0.578125\n",
      "2017-08-02T10:27:04.417190: step 29, loss 4.73392, acc 0.421875\n",
      "2017-08-02T10:27:04.990248: step 30, loss 3.28638, acc 0.625\n",
      "2017-08-02T10:27:05.546303: step 31, loss 4.78384, acc 0.328125\n",
      "2017-08-02T10:27:06.123361: step 32, loss 3.95811, acc 0.46875\n",
      "2017-08-02T10:27:06.717420: step 33, loss 4.47813, acc 0.5\n",
      "2017-08-02T10:27:07.271476: step 34, loss 4.76461, acc 0.390625\n",
      "2017-08-02T10:27:07.827531: step 35, loss 3.83072, acc 0.5\n",
      "2017-08-02T10:27:08.393588: step 36, loss 3.8354, acc 0.515625\n",
      "2017-08-02T10:27:08.965645: step 37, loss 3.01548, acc 0.640625\n",
      "2017-08-02T10:27:09.547703: step 38, loss 3.90238, acc 0.5\n",
      "2017-08-02T10:27:10.110760: step 39, loss 4.22235, acc 0.46875\n",
      "2017-08-02T10:27:10.675816: step 40, loss 3.49299, acc 0.59375\n",
      "2017-08-02T10:27:11.343883: step 41, loss 3.92054, acc 0.453125\n",
      "2017-08-02T10:27:11.902939: step 42, loss 3.56353, acc 0.546875\n",
      "2017-08-02T10:27:12.456994: step 43, loss 3.57259, acc 0.46875\n",
      "2017-08-02T10:27:13.009049: step 44, loss 3.51378, acc 0.53125\n",
      "2017-08-02T10:27:13.567105: step 45, loss 3.18436, acc 0.5625\n",
      "2017-08-02T10:27:14.122161: step 46, loss 3.89433, acc 0.484375\n",
      "2017-08-02T10:27:14.683217: step 47, loss 3.81288, acc 0.546875\n",
      "2017-08-02T10:27:15.239272: step 48, loss 3.16214, acc 0.5625\n",
      "2017-08-02T10:27:15.793328: step 49, loss 3.13091, acc 0.640625\n",
      "2017-08-02T10:27:16.347383: step 50, loss 4.10503, acc 0.546875\n",
      "2017-08-02T10:27:16.900439: step 51, loss 3.47565, acc 0.546875\n",
      "2017-08-02T10:27:17.453494: step 52, loss 3.98828, acc 0.515625\n",
      "2017-08-02T10:27:18.009549: step 53, loss 4.49061, acc 0.515625\n",
      "2017-08-02T10:27:18.562605: step 54, loss 3.24131, acc 0.5\n",
      "2017-08-02T10:27:19.117660: step 55, loss 3.87393, acc 0.421875\n",
      "2017-08-02T10:27:19.671716: step 56, loss 3.39311, acc 0.46875\n",
      "2017-08-02T10:27:20.226771: step 57, loss 2.93453, acc 0.609375\n",
      "2017-08-02T10:27:20.777826: step 58, loss 3.56853, acc 0.484375\n",
      "2017-08-02T10:27:21.335882: step 59, loss 2.86244, acc 0.65625\n",
      "2017-08-02T10:27:21.888937: step 60, loss 3.17875, acc 0.5625\n",
      "2017-08-02T10:27:22.445993: step 61, loss 4.91524, acc 0.359375\n",
      "2017-08-02T10:27:23.003049: step 62, loss 3.27176, acc 0.640625\n",
      "2017-08-02T10:27:23.560104: step 63, loss 3.30671, acc 0.546875\n",
      "2017-08-02T10:27:24.116160: step 64, loss 3.40096, acc 0.578125\n",
      "2017-08-02T10:27:24.675216: step 65, loss 3.29586, acc 0.546875\n",
      "2017-08-02T10:27:25.223271: step 66, loss 3.52029, acc 0.5\n",
      "2017-08-02T10:27:25.783327: step 67, loss 3.54405, acc 0.484375\n",
      "2017-08-02T10:27:26.384387: step 68, loss 3.17137, acc 0.578125\n",
      "2017-08-02T10:27:26.948443: step 69, loss 3.41459, acc 0.546875\n",
      "2017-08-02T10:27:27.516500: step 70, loss 3.2366, acc 0.53125\n",
      "2017-08-02T10:27:28.077556: step 71, loss 2.96282, acc 0.578125\n",
      "2017-08-02T10:27:28.636612: step 72, loss 3.02418, acc 0.484375\n",
      "2017-08-02T10:27:29.255674: step 73, loss 3.13477, acc 0.546875\n",
      "2017-08-02T10:27:29.812730: step 74, loss 2.94734, acc 0.515625\n",
      "2017-08-02T10:27:30.362785: step 75, loss 3.27912, acc 0.53125\n",
      "2017-08-02T10:27:30.917840: step 76, loss 3.10435, acc 0.5\n",
      "2017-08-02T10:27:31.477896: step 77, loss 2.87981, acc 0.5625\n",
      "2017-08-02T10:27:32.044953: step 78, loss 3.15524, acc 0.484375\n",
      "2017-08-02T10:27:32.611009: step 79, loss 2.96235, acc 0.53125\n",
      "2017-08-02T10:27:33.185067: step 80, loss 3.27146, acc 0.515625\n",
      "2017-08-02T10:27:33.758124: step 81, loss 2.68359, acc 0.515625\n",
      "2017-08-02T10:27:34.365185: step 82, loss 3.11309, acc 0.515625\n",
      "2017-08-02T10:27:34.998248: step 83, loss 3.0456, acc 0.53125\n",
      "2017-08-02T10:27:35.660314: step 84, loss 2.81424, acc 0.578125\n",
      "2017-08-02T10:27:36.311379: step 85, loss 2.91158, acc 0.5\n",
      "2017-08-02T10:27:36.961444: step 86, loss 2.8172, acc 0.515625\n",
      "2017-08-02T10:27:37.527501: step 87, loss 2.5693, acc 0.53125\n",
      "2017-08-02T10:27:38.093558: step 88, loss 3.22701, acc 0.546875\n",
      "2017-08-02T10:27:38.653614: step 89, loss 2.66089, acc 0.453125\n",
      "2017-08-02T10:27:39.221670: step 90, loss 2.47475, acc 0.59375\n",
      "2017-08-02T10:27:39.785727: step 91, loss 3.19279, acc 0.359375\n",
      "2017-08-02T10:27:40.342783: step 92, loss 3.36114, acc 0.453125\n",
      "2017-08-02T10:27:40.906839: step 93, loss 2.61468, acc 0.59375\n",
      "2017-08-02T10:27:41.486897: step 94, loss 2.79631, acc 0.5\n",
      "2017-08-02T10:27:42.064955: step 95, loss 2.85053, acc 0.46875\n",
      "2017-08-02T10:27:42.633012: step 96, loss 3.09396, acc 0.515625\n",
      "2017-08-02T10:27:43.190067: step 97, loss 2.70997, acc 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:27:43.752123: step 98, loss 2.29522, acc 0.734375\n",
      "2017-08-02T10:27:44.330181: step 99, loss 2.74733, acc 0.5\n",
      "2017-08-02T10:27:44.907239: step 100, loss 3.05019, acc 0.421875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:27:48.579606: step 100, loss 1.6785, acc 0.6696\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-100\n",
      "\n",
      "2017-08-02T10:27:51.792927: step 101, loss 2.29843, acc 0.5625\n",
      "2017-08-02T10:27:52.373986: step 102, loss 2.73785, acc 0.5\n",
      "2017-08-02T10:27:52.958044: step 103, loss 2.60496, acc 0.4375\n",
      "2017-08-02T10:27:53.509099: step 104, loss 2.65396, acc 0.4375\n",
      "2017-08-02T10:27:54.059154: step 105, loss 2.84236, acc 0.5\n",
      "2017-08-02T10:27:54.637212: step 106, loss 3.26774, acc 0.4375\n",
      "2017-08-02T10:27:55.194268: step 107, loss 2.53183, acc 0.53125\n",
      "2017-08-02T10:27:55.764325: step 108, loss 2.51604, acc 0.515625\n",
      "2017-08-02T10:27:56.309379: step 109, loss 2.48767, acc 0.546875\n",
      "2017-08-02T10:27:56.863434: step 110, loss 2.41627, acc 0.546875\n",
      "2017-08-02T10:27:57.416490: step 111, loss 2.67371, acc 0.5\n",
      "2017-08-02T10:27:57.968545: step 112, loss 2.36481, acc 0.53125\n",
      "2017-08-02T10:27:58.519600: step 113, loss 2.31101, acc 0.515625\n",
      "2017-08-02T10:27:59.073655: step 114, loss 2.38221, acc 0.5625\n",
      "2017-08-02T10:27:59.620710: step 115, loss 2.17431, acc 0.609375\n",
      "2017-08-02T10:28:00.166765: step 116, loss 2.68017, acc 0.5\n",
      "2017-08-02T10:28:00.725821: step 117, loss 2.28053, acc 0.578125\n",
      "2017-08-02T10:28:01.275876: step 118, loss 2.02348, acc 0.5625\n",
      "2017-08-02T10:28:01.832931: step 119, loss 2.26177, acc 0.546875\n",
      "2017-08-02T10:28:02.408989: step 120, loss 2.0952, acc 0.625\n",
      "2017-08-02T10:28:03.004048: step 121, loss 2.08128, acc 0.609375\n",
      "2017-08-02T10:28:03.552103: step 122, loss 2.55653, acc 0.515625\n",
      "2017-08-02T10:28:04.108159: step 123, loss 2.68056, acc 0.453125\n",
      "2017-08-02T10:28:04.667215: step 124, loss 2.10511, acc 0.65625\n",
      "2017-08-02T10:28:05.222270: step 125, loss 2.31477, acc 0.515625\n",
      "2017-08-02T10:28:05.784326: step 126, loss 2.07314, acc 0.609375\n",
      "2017-08-02T10:28:06.336382: step 127, loss 2.21289, acc 0.53125\n",
      "2017-08-02T10:28:06.905439: step 128, loss 2.03005, acc 0.59375\n",
      "2017-08-02T10:28:07.485497: step 129, loss 2.19321, acc 0.5\n",
      "2017-08-02T10:28:08.045553: step 130, loss 2.0023, acc 0.640625\n",
      "2017-08-02T10:28:08.601608: step 131, loss 2.65321, acc 0.421875\n",
      "2017-08-02T10:28:09.162664: step 132, loss 1.94803, acc 0.640625\n",
      "2017-08-02T10:28:09.709719: step 133, loss 1.93956, acc 0.65625\n",
      "2017-08-02T10:28:10.260774: step 134, loss 2.18326, acc 0.515625\n",
      "2017-08-02T10:28:10.817830: step 135, loss 1.77195, acc 0.6875\n",
      "2017-08-02T10:28:11.376886: step 136, loss 2.01359, acc 0.65625\n",
      "2017-08-02T10:28:11.929941: step 137, loss 2.33396, acc 0.46875\n",
      "2017-08-02T10:28:12.482996: step 138, loss 2.27119, acc 0.5\n",
      "2017-08-02T10:28:13.042052: step 139, loss 2.10694, acc 0.53125\n",
      "2017-08-02T10:28:13.599108: step 140, loss 2.35287, acc 0.546875\n",
      "2017-08-02T10:28:14.160164: step 141, loss 2.09843, acc 0.5625\n",
      "2017-08-02T10:28:14.722220: step 142, loss 1.71806, acc 0.578125\n",
      "2017-08-02T10:28:15.273275: step 143, loss 1.65123, acc 0.671875\n",
      "2017-08-02T10:28:15.826331: step 144, loss 2.14731, acc 0.578125\n",
      "2017-08-02T10:28:16.375385: step 145, loss 1.88419, acc 0.5625\n",
      "2017-08-02T10:28:16.924440: step 146, loss 2.14512, acc 0.515625\n",
      "2017-08-02T10:28:17.474495: step 147, loss 1.86254, acc 0.515625\n",
      "2017-08-02T10:28:18.024550: step 148, loss 2.0241, acc 0.546875\n",
      "2017-08-02T10:28:18.570605: step 149, loss 2.01479, acc 0.546875\n",
      "2017-08-02T10:28:19.119660: step 150, loss 1.84664, acc 0.546875\n",
      "2017-08-02T10:28:19.668715: step 151, loss 2.08049, acc 0.484375\n",
      "2017-08-02T10:28:20.219770: step 152, loss 1.63173, acc 0.59375\n",
      "2017-08-02T10:28:20.772825: step 153, loss 1.72013, acc 0.578125\n",
      "2017-08-02T10:28:21.317880: step 154, loss 2.03348, acc 0.515625\n",
      "2017-08-02T10:28:21.866935: step 155, loss 1.52708, acc 0.671875\n",
      "2017-08-02T10:28:22.412989: step 156, loss 1.38377, acc 0.6875\n",
      "2017-08-02T10:28:22.964044: step 157, loss 1.68606, acc 0.65625\n",
      "2017-08-02T10:28:23.588107: step 158, loss 1.90097, acc 0.546875\n",
      "2017-08-02T10:28:24.143162: step 159, loss 1.71784, acc 0.53125\n",
      "2017-08-02T10:28:24.685216: step 160, loss 1.86051, acc 0.5\n",
      "2017-08-02T10:28:25.234271: step 161, loss 2.02948, acc 0.546875\n",
      "2017-08-02T10:28:25.783326: step 162, loss 1.92822, acc 0.578125\n",
      "2017-08-02T10:28:26.334381: step 163, loss 1.89056, acc 0.609375\n",
      "2017-08-02T10:28:26.897438: step 164, loss 1.93253, acc 0.5\n",
      "2017-08-02T10:28:27.490497: step 165, loss 2.09117, acc 0.4375\n",
      "2017-08-02T10:28:28.035551: step 166, loss 1.43663, acc 0.625\n",
      "2017-08-02T10:28:28.588607: step 167, loss 2.01053, acc 0.515625\n",
      "2017-08-02T10:28:29.147663: step 168, loss 1.53378, acc 0.625\n",
      "2017-08-02T10:28:29.754723: step 169, loss 1.76767, acc 0.421875\n",
      "2017-08-02T10:28:30.360784: step 170, loss 1.44956, acc 0.59375\n",
      "2017-08-02T10:28:30.949843: step 171, loss 1.95101, acc 0.484375\n",
      "2017-08-02T10:28:31.548903: step 172, loss 1.69573, acc 0.578125\n",
      "2017-08-02T10:28:32.123960: step 173, loss 1.91505, acc 0.5\n",
      "2017-08-02T10:28:32.690017: step 174, loss 1.68116, acc 0.578125\n",
      "2017-08-02T10:28:33.255073: step 175, loss 1.62667, acc 0.578125\n",
      "2017-08-02T10:28:33.822130: step 176, loss 1.41796, acc 0.671875\n",
      "2017-08-02T10:28:34.389187: step 177, loss 1.83394, acc 0.515625\n",
      "2017-08-02T10:28:34.971245: step 178, loss 1.97563, acc 0.53125\n",
      "2017-08-02T10:28:35.561304: step 179, loss 1.76175, acc 0.515625\n",
      "2017-08-02T10:28:36.193367: step 180, loss 1.46209, acc 0.578125\n",
      "2017-08-02T10:28:36.879436: step 181, loss 1.30968, acc 0.65625\n",
      "2017-08-02T10:28:37.476495: step 182, loss 1.66501, acc 0.578125\n",
      "2017-08-02T10:28:38.029551: step 183, loss 1.73326, acc 0.453125\n",
      "2017-08-02T10:28:38.578606: step 184, loss 1.66989, acc 0.515625\n",
      "2017-08-02T10:28:39.133661: step 185, loss 1.85197, acc 0.546875\n",
      "2017-08-02T10:28:39.767724: step 186, loss 1.99923, acc 0.46875\n",
      "2017-08-02T10:28:40.319780: step 187, loss 1.82639, acc 0.484375\n",
      "2017-08-02T10:28:40.877835: step 188, loss 1.77745, acc 0.546875\n",
      "2017-08-02T10:28:41.438892: step 189, loss 1.53113, acc 0.515625\n",
      "2017-08-02T10:28:42.050953: step 190, loss 1.43124, acc 0.53125\n",
      "2017-08-02T10:28:42.689017: step 191, loss 1.32035, acc 0.640625\n",
      "2017-08-02T10:28:43.261074: step 192, loss 1.41691, acc 0.640625\n",
      "2017-08-02T10:28:43.821130: step 193, loss 1.39889, acc 0.625\n",
      "2017-08-02T10:28:44.446192: step 194, loss 1.3864, acc 0.609375\n",
      "2017-08-02T10:28:45.087256: step 195, loss 1.32729, acc 0.640625\n",
      "2017-08-02T10:28:45.650313: step 196, loss 1.50404, acc 0.546875\n",
      "2017-08-02T10:28:46.207368: step 197, loss 1.3943, acc 0.59375\n",
      "2017-08-02T10:28:46.775425: step 198, loss 1.47161, acc 0.515625\n",
      "2017-08-02T10:28:47.346482: step 199, loss 1.47747, acc 0.625\n",
      "2017-08-02T10:28:47.905538: step 200, loss 1.4627, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:28:51.650913: step 200, loss 1.11748, acc 0.6324\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-200\n",
      "\n",
      "2017-08-02T10:28:55.482296: step 201, loss 1.50143, acc 0.5\n",
      "2017-08-02T10:28:56.051353: step 202, loss 1.61539, acc 0.515625\n",
      "2017-08-02T10:28:56.624410: step 203, loss 1.15869, acc 0.640625\n",
      "2017-08-02T10:28:57.179465: step 204, loss 1.55631, acc 0.515625\n",
      "2017-08-02T10:28:57.791527: step 205, loss 1.40577, acc 0.546875\n",
      "2017-08-02T10:28:58.413589: step 206, loss 1.57091, acc 0.59375\n",
      "2017-08-02T10:28:58.992647: step 207, loss 1.53549, acc 0.53125\n",
      "2017-08-02T10:28:59.563704: step 208, loss 1.5041, acc 0.515625\n",
      "2017-08-02T10:29:00.130761: step 209, loss 1.43632, acc 0.5\n",
      "2017-08-02T10:29:00.714819: step 210, loss 1.20927, acc 0.625\n",
      "2017-08-02T10:29:01.333881: step 211, loss 1.42068, acc 0.59375\n",
      "2017-08-02T10:29:01.893937: step 212, loss 1.47144, acc 0.5625\n",
      "2017-08-02T10:29:02.455993: step 213, loss 1.43959, acc 0.53125\n",
      "2017-08-02T10:29:03.017049: step 214, loss 1.21226, acc 0.6875\n",
      "2017-08-02T10:29:03.576105: step 215, loss 1.41908, acc 0.5625\n",
      "2017-08-02T10:29:04.177165: step 216, loss 1.39615, acc 0.5625\n",
      "2017-08-02T10:29:04.761224: step 217, loss 1.25593, acc 0.59375\n",
      "2017-08-02T10:29:05.416289: step 218, loss 1.29039, acc 0.578125\n",
      "2017-08-02T10:29:06.026350: step 219, loss 1.33165, acc 0.46875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:29:06.615409: step 220, loss 1.26836, acc 0.5625\n",
      "2017-08-02T10:29:07.175465: step 221, loss 1.2504, acc 0.53125\n",
      "2017-08-02T10:29:07.819529: step 222, loss 1.3062, acc 0.484375\n",
      "2017-08-02T10:29:08.397587: step 223, loss 1.20091, acc 0.59375\n",
      "2017-08-02T10:29:08.956643: step 224, loss 1.38064, acc 0.515625\n",
      "2017-08-02T10:29:09.513699: step 225, loss 1.43573, acc 0.546875\n",
      "2017-08-02T10:29:10.070754: step 226, loss 1.27575, acc 0.5625\n",
      "2017-08-02T10:29:10.622810: step 227, loss 1.33985, acc 0.453125\n",
      "2017-08-02T10:29:11.176865: step 228, loss 1.29191, acc 0.578125\n",
      "2017-08-02T10:29:11.730920: step 229, loss 1.26866, acc 0.5625\n",
      "2017-08-02T10:29:12.288976: step 230, loss 1.15978, acc 0.671875\n",
      "2017-08-02T10:29:12.857033: step 231, loss 1.31217, acc 0.5625\n",
      "2017-08-02T10:29:13.423090: step 232, loss 1.22366, acc 0.609375\n",
      "2017-08-02T10:29:13.981145: step 233, loss 1.19563, acc 0.65625\n",
      "2017-08-02T10:29:14.538201: step 234, loss 1.3383, acc 0.546875\n",
      "2017-08-02T10:29:15.098257: step 235, loss 1.28288, acc 0.5625\n",
      "2017-08-02T10:29:15.648312: step 236, loss 1.15211, acc 0.625\n",
      "2017-08-02T10:29:16.205368: step 237, loss 1.27383, acc 0.59375\n",
      "2017-08-02T10:29:16.756423: step 238, loss 1.33588, acc 0.46875\n",
      "2017-08-02T10:29:17.310478: step 239, loss 1.25074, acc 0.546875\n",
      "2017-08-02T10:29:17.869534: step 240, loss 1.0889, acc 0.65625\n",
      "2017-08-02T10:29:18.428590: step 241, loss 1.06612, acc 0.65625\n",
      "2017-08-02T10:29:18.980645: step 242, loss 1.13484, acc 0.640625\n",
      "2017-08-02T10:29:19.548702: step 243, loss 1.02715, acc 0.609375\n",
      "2017-08-02T10:29:20.104758: step 244, loss 1.18872, acc 0.625\n",
      "2017-08-02T10:29:20.658813: step 245, loss 1.16865, acc 0.5\n",
      "2017-08-02T10:29:21.213869: step 246, loss 1.17117, acc 0.625\n",
      "2017-08-02T10:29:21.768924: step 247, loss 1.17035, acc 0.546875\n",
      "2017-08-02T10:29:22.322980: step 248, loss 1.16618, acc 0.53125\n",
      "2017-08-02T10:29:22.881035: step 249, loss 1.15915, acc 0.59375\n",
      "2017-08-02T10:29:23.438091: step 250, loss 1.2432, acc 0.484375\n",
      "2017-08-02T10:29:23.992146: step 251, loss 1.0848, acc 0.625\n",
      "2017-08-02T10:29:24.549202: step 252, loss 1.13689, acc 0.5\n",
      "2017-08-02T10:29:25.101257: step 253, loss 1.06978, acc 0.640625\n",
      "2017-08-02T10:29:25.657313: step 254, loss 1.15036, acc 0.5625\n",
      "2017-08-02T10:29:26.215369: step 255, loss 1.23606, acc 0.5625\n",
      "2017-08-02T10:29:26.773425: step 256, loss 1.25019, acc 0.453125\n",
      "2017-08-02T10:29:27.331480: step 257, loss 1.07636, acc 0.578125\n",
      "2017-08-02T10:29:27.885536: step 258, loss 1.13611, acc 0.5625\n",
      "2017-08-02T10:29:28.440591: step 259, loss 1.10441, acc 0.5625\n",
      "2017-08-02T10:29:28.995647: step 260, loss 1.0966, acc 0.625\n",
      "2017-08-02T10:29:29.553703: step 261, loss 1.19735, acc 0.484375\n",
      "2017-08-02T10:29:30.103758: step 262, loss 1.2147, acc 0.515625\n",
      "2017-08-02T10:29:30.656813: step 263, loss 1.08125, acc 0.609375\n",
      "2017-08-02T10:29:31.208868: step 264, loss 1.09093, acc 0.578125\n",
      "2017-08-02T10:29:31.757923: step 265, loss 1.24563, acc 0.46875\n",
      "2017-08-02T10:29:32.310978: step 266, loss 1.19147, acc 0.53125\n",
      "2017-08-02T10:29:32.868034: step 267, loss 0.986377, acc 0.640625\n",
      "2017-08-02T10:29:33.428090: step 268, loss 1.0204, acc 0.59375\n",
      "2017-08-02T10:29:33.983145: step 269, loss 1.07502, acc 0.609375\n",
      "2017-08-02T10:29:34.542201: step 270, loss 1.21284, acc 0.53125\n",
      "2017-08-02T10:29:35.095257: step 271, loss 0.927107, acc 0.71875\n",
      "2017-08-02T10:29:35.667314: step 272, loss 1.1381, acc 0.53125\n",
      "2017-08-02T10:29:36.234371: step 273, loss 1.08325, acc 0.46875\n",
      "2017-08-02T10:29:36.800427: step 274, loss 1.07606, acc 0.609375\n",
      "2017-08-02T10:29:37.358483: step 275, loss 1.15023, acc 0.46875\n",
      "2017-08-02T10:29:37.913538: step 276, loss 0.946355, acc 0.640625\n",
      "2017-08-02T10:29:38.470594: step 277, loss 1.13492, acc 0.515625\n",
      "2017-08-02T10:29:39.030650: step 278, loss 0.944448, acc 0.65625\n",
      "2017-08-02T10:29:39.590706: step 279, loss 1.15015, acc 0.53125\n",
      "2017-08-02T10:29:40.147762: step 280, loss 1.1052, acc 0.53125\n",
      "2017-08-02T10:29:40.702817: step 281, loss 1.23683, acc 0.453125\n",
      "2017-08-02T10:29:41.262873: step 282, loss 1.07079, acc 0.59375\n",
      "2017-08-02T10:29:41.811928: step 283, loss 0.996711, acc 0.625\n",
      "2017-08-02T10:29:42.368984: step 284, loss 1.0275, acc 0.515625\n",
      "2017-08-02T10:29:42.919039: step 285, loss 1.05321, acc 0.578125\n",
      "2017-08-02T10:29:43.475095: step 286, loss 1.0958, acc 0.484375\n",
      "2017-08-02T10:29:44.032150: step 287, loss 1.00837, acc 0.5625\n",
      "2017-08-02T10:29:44.591206: step 288, loss 1.01622, acc 0.5\n",
      "2017-08-02T10:29:45.153262: step 289, loss 0.889547, acc 0.671875\n",
      "2017-08-02T10:29:45.709318: step 290, loss 1.04676, acc 0.609375\n",
      "2017-08-02T10:29:46.274374: step 291, loss 1.02867, acc 0.5\n",
      "2017-08-02T10:29:46.833430: step 292, loss 0.916432, acc 0.65625\n",
      "2017-08-02T10:29:47.384485: step 293, loss 0.984928, acc 0.5625\n",
      "2017-08-02T10:29:47.943541: step 294, loss 0.908537, acc 0.625\n",
      "2017-08-02T10:29:48.496597: step 295, loss 1.05938, acc 0.46875\n",
      "2017-08-02T10:29:49.051652: step 296, loss 1.03854, acc 0.53125\n",
      "2017-08-02T10:29:49.609708: step 297, loss 0.979229, acc 0.546875\n",
      "2017-08-02T10:29:50.164763: step 298, loss 1.00972, acc 0.59375\n",
      "2017-08-02T10:29:50.721819: step 299, loss 0.986812, acc 0.59375\n",
      "2017-08-02T10:29:51.275875: step 300, loss 0.876895, acc 0.640625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:29:55.117259: step 300, loss 0.87806, acc 0.6544\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-300\n",
      "\n",
      "2017-08-02T10:30:00.281775: step 301, loss 0.977365, acc 0.625\n",
      "2017-08-02T10:30:00.836831: step 302, loss 0.913814, acc 0.609375\n",
      "2017-08-02T10:30:01.390886: step 303, loss 1.12745, acc 0.484375\n",
      "2017-08-02T10:30:01.949942: step 304, loss 0.952638, acc 0.640625\n",
      "2017-08-02T10:30:02.502997: step 305, loss 1.00705, acc 0.578125\n",
      "2017-08-02T10:30:03.055052: step 306, loss 1.16028, acc 0.421875\n",
      "2017-08-02T10:30:03.605107: step 307, loss 0.967313, acc 0.59375\n",
      "2017-08-02T10:30:04.159163: step 308, loss 1.05414, acc 0.46875\n",
      "2017-08-02T10:30:04.716218: step 309, loss 0.991759, acc 0.578125\n",
      "2017-08-02T10:30:05.278275: step 310, loss 0.906233, acc 0.578125\n",
      "2017-08-02T10:30:05.835330: step 311, loss 0.997341, acc 0.546875\n",
      "2017-08-02T10:30:06.391386: step 312, loss 1.03376, acc 0.484375\n",
      "2017-08-02T10:30:06.802427: step 313, loss 0.944341, acc 0.59375\n",
      "2017-08-02T10:30:07.358483: step 314, loss 1.00405, acc 0.546875\n",
      "2017-08-02T10:30:07.917539: step 315, loss 0.948022, acc 0.59375\n",
      "2017-08-02T10:30:08.481595: step 316, loss 0.836569, acc 0.640625\n",
      "2017-08-02T10:30:09.057653: step 317, loss 0.875236, acc 0.625\n",
      "2017-08-02T10:30:09.620709: step 318, loss 0.955701, acc 0.515625\n",
      "2017-08-02T10:30:10.176764: step 319, loss 0.866952, acc 0.59375\n",
      "2017-08-02T10:30:10.734820: step 320, loss 0.848498, acc 0.625\n",
      "2017-08-02T10:30:11.298877: step 321, loss 0.939658, acc 0.53125\n",
      "2017-08-02T10:30:11.882935: step 322, loss 1.03162, acc 0.40625\n",
      "2017-08-02T10:30:12.443991: step 323, loss 0.983554, acc 0.484375\n",
      "2017-08-02T10:30:13.000047: step 324, loss 0.918251, acc 0.546875\n",
      "2017-08-02T10:30:13.559103: step 325, loss 0.880366, acc 0.578125\n",
      "2017-08-02T10:30:14.120159: step 326, loss 0.885964, acc 0.671875\n",
      "2017-08-02T10:30:14.682215: step 327, loss 0.73546, acc 0.8125\n",
      "2017-08-02T10:30:15.262273: step 328, loss 0.870184, acc 0.65625\n",
      "2017-08-02T10:30:15.827329: step 329, loss 0.987762, acc 0.578125\n",
      "2017-08-02T10:30:16.381385: step 330, loss 0.783644, acc 0.65625\n",
      "2017-08-02T10:30:16.941441: step 331, loss 0.993128, acc 0.5625\n",
      "2017-08-02T10:30:17.499497: step 332, loss 0.883289, acc 0.65625\n",
      "2017-08-02T10:30:18.052552: step 333, loss 0.80289, acc 0.640625\n",
      "2017-08-02T10:30:18.611608: step 334, loss 0.83364, acc 0.6875\n",
      "2017-08-02T10:30:19.167663: step 335, loss 0.856505, acc 0.640625\n",
      "2017-08-02T10:30:19.726719: step 336, loss 0.949522, acc 0.5625\n",
      "2017-08-02T10:30:20.283775: step 337, loss 0.906492, acc 0.609375\n",
      "2017-08-02T10:30:20.833830: step 338, loss 0.843205, acc 0.5625\n",
      "2017-08-02T10:30:21.391886: step 339, loss 0.861303, acc 0.609375\n",
      "2017-08-02T10:30:21.954942: step 340, loss 0.855046, acc 0.609375\n",
      "2017-08-02T10:30:22.515998: step 341, loss 0.862329, acc 0.625\n",
      "2017-08-02T10:30:23.076054: step 342, loss 0.830384, acc 0.65625\n",
      "2017-08-02T10:30:23.633110: step 343, loss 0.89285, acc 0.609375\n",
      "2017-08-02T10:30:24.183165: step 344, loss 0.822475, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:30:24.742221: step 345, loss 0.872481, acc 0.59375\n",
      "2017-08-02T10:30:25.291276: step 346, loss 0.923791, acc 0.546875\n",
      "2017-08-02T10:30:25.848331: step 347, loss 0.844692, acc 0.671875\n",
      "2017-08-02T10:30:26.405387: step 348, loss 0.994808, acc 0.546875\n",
      "2017-08-02T10:30:26.959443: step 349, loss 0.897281, acc 0.59375\n",
      "2017-08-02T10:30:27.536500: step 350, loss 0.965204, acc 0.53125\n",
      "2017-08-02T10:30:28.098556: step 351, loss 0.86976, acc 0.59375\n",
      "2017-08-02T10:30:28.653612: step 352, loss 0.991244, acc 0.53125\n",
      "2017-08-02T10:30:29.209668: step 353, loss 0.875825, acc 0.640625\n",
      "2017-08-02T10:30:29.772724: step 354, loss 0.835198, acc 0.625\n",
      "2017-08-02T10:30:30.323779: step 355, loss 0.858524, acc 0.625\n",
      "2017-08-02T10:30:30.881835: step 356, loss 0.813104, acc 0.65625\n",
      "2017-08-02T10:30:31.431890: step 357, loss 0.845312, acc 0.578125\n",
      "2017-08-02T10:30:31.989946: step 358, loss 0.902421, acc 0.59375\n",
      "2017-08-02T10:30:32.548001: step 359, loss 0.72644, acc 0.75\n",
      "2017-08-02T10:30:33.107057: step 360, loss 0.844822, acc 0.640625\n",
      "2017-08-02T10:30:33.670114: step 361, loss 0.937015, acc 0.53125\n",
      "2017-08-02T10:30:34.229169: step 362, loss 0.846534, acc 0.609375\n",
      "2017-08-02T10:30:34.784225: step 363, loss 0.891573, acc 0.59375\n",
      "2017-08-02T10:30:35.347281: step 364, loss 0.85188, acc 0.640625\n",
      "2017-08-02T10:30:35.902337: step 365, loss 0.865411, acc 0.53125\n",
      "2017-08-02T10:30:36.457392: step 366, loss 0.876189, acc 0.5625\n",
      "2017-08-02T10:30:37.014448: step 367, loss 0.81049, acc 0.609375\n",
      "2017-08-02T10:30:37.569503: step 368, loss 0.797606, acc 0.640625\n",
      "2017-08-02T10:30:38.129559: step 369, loss 0.890606, acc 0.625\n",
      "2017-08-02T10:30:38.688615: step 370, loss 0.841387, acc 0.546875\n",
      "2017-08-02T10:30:39.236670: step 371, loss 0.675718, acc 0.8125\n",
      "2017-08-02T10:30:39.793726: step 372, loss 0.916867, acc 0.453125\n",
      "2017-08-02T10:30:40.349781: step 373, loss 0.770741, acc 0.6875\n",
      "2017-08-02T10:30:40.901837: step 374, loss 0.719227, acc 0.703125\n",
      "2017-08-02T10:30:41.456892: step 375, loss 0.688009, acc 0.75\n",
      "2017-08-02T10:30:42.015948: step 376, loss 0.834137, acc 0.609375\n",
      "2017-08-02T10:30:42.573004: step 377, loss 0.728123, acc 0.734375\n",
      "2017-08-02T10:30:43.127059: step 378, loss 0.800187, acc 0.671875\n",
      "2017-08-02T10:30:43.685115: step 379, loss 0.902208, acc 0.515625\n",
      "2017-08-02T10:30:44.240170: step 380, loss 0.896466, acc 0.59375\n",
      "2017-08-02T10:30:44.804227: step 381, loss 0.826237, acc 0.609375\n",
      "2017-08-02T10:30:45.357282: step 382, loss 0.856783, acc 0.59375\n",
      "2017-08-02T10:30:45.904337: step 383, loss 0.796359, acc 0.640625\n",
      "2017-08-02T10:30:46.456392: step 384, loss 0.811406, acc 0.59375\n",
      "2017-08-02T10:30:47.008447: step 385, loss 0.813869, acc 0.65625\n",
      "2017-08-02T10:30:47.573504: step 386, loss 0.947658, acc 0.5625\n",
      "2017-08-02T10:30:48.130559: step 387, loss 0.81284, acc 0.65625\n",
      "2017-08-02T10:30:48.680614: step 388, loss 0.762999, acc 0.65625\n",
      "2017-08-02T10:30:49.236670: step 389, loss 0.801478, acc 0.640625\n",
      "2017-08-02T10:30:49.793726: step 390, loss 0.753953, acc 0.734375\n",
      "2017-08-02T10:30:50.348781: step 391, loss 0.763191, acc 0.65625\n",
      "2017-08-02T10:30:50.904837: step 392, loss 0.764012, acc 0.6875\n",
      "2017-08-02T10:30:51.460892: step 393, loss 0.857408, acc 0.640625\n",
      "2017-08-02T10:30:52.015948: step 394, loss 0.785306, acc 0.671875\n",
      "2017-08-02T10:30:52.575004: step 395, loss 0.82619, acc 0.640625\n",
      "2017-08-02T10:30:53.131059: step 396, loss 0.801498, acc 0.625\n",
      "2017-08-02T10:30:53.687115: step 397, loss 0.85252, acc 0.59375\n",
      "2017-08-02T10:30:54.244171: step 398, loss 0.823498, acc 0.625\n",
      "2017-08-02T10:30:54.805227: step 399, loss 0.760264, acc 0.65625\n",
      "2017-08-02T10:30:55.367283: step 400, loss 0.777273, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:30:58.962643: step 400, loss 0.772704, acc 0.6216\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-400\n",
      "\n",
      "2017-08-02T10:31:04.193166: step 401, loss 0.708783, acc 0.671875\n",
      "2017-08-02T10:31:04.766223: step 402, loss 0.78445, acc 0.65625\n",
      "2017-08-02T10:31:05.319278: step 403, loss 0.792401, acc 0.625\n",
      "2017-08-02T10:31:05.875334: step 404, loss 0.729609, acc 0.6875\n",
      "2017-08-02T10:31:06.443391: step 405, loss 0.744711, acc 0.640625\n",
      "2017-08-02T10:31:07.008447: step 406, loss 0.768076, acc 0.65625\n",
      "2017-08-02T10:31:07.567503: step 407, loss 0.838647, acc 0.5625\n",
      "2017-08-02T10:31:08.119558: step 408, loss 0.74357, acc 0.640625\n",
      "2017-08-02T10:31:08.674614: step 409, loss 0.706443, acc 0.75\n",
      "2017-08-02T10:31:09.245671: step 410, loss 0.795035, acc 0.625\n",
      "2017-08-02T10:31:09.821728: step 411, loss 0.749469, acc 0.65625\n",
      "2017-08-02T10:31:10.375784: step 412, loss 0.79496, acc 0.59375\n",
      "2017-08-02T10:31:10.932839: step 413, loss 0.90135, acc 0.53125\n",
      "2017-08-02T10:31:11.480894: step 414, loss 0.797398, acc 0.640625\n",
      "2017-08-02T10:31:12.035950: step 415, loss 0.752509, acc 0.65625\n",
      "2017-08-02T10:31:12.589005: step 416, loss 0.735912, acc 0.671875\n",
      "2017-08-02T10:31:13.141060: step 417, loss 0.77254, acc 0.59375\n",
      "2017-08-02T10:31:13.696116: step 418, loss 0.751301, acc 0.671875\n",
      "2017-08-02T10:31:14.250171: step 419, loss 0.72821, acc 0.734375\n",
      "2017-08-02T10:31:14.808227: step 420, loss 0.749989, acc 0.703125\n",
      "2017-08-02T10:31:15.365283: step 421, loss 0.756461, acc 0.609375\n",
      "2017-08-02T10:31:15.927339: step 422, loss 0.699278, acc 0.71875\n",
      "2017-08-02T10:31:16.486395: step 423, loss 0.735085, acc 0.609375\n",
      "2017-08-02T10:31:17.039450: step 424, loss 0.726824, acc 0.6875\n",
      "2017-08-02T10:31:17.593505: step 425, loss 0.74329, acc 0.671875\n",
      "2017-08-02T10:31:18.148561: step 426, loss 0.810908, acc 0.609375\n",
      "2017-08-02T10:31:18.702616: step 427, loss 0.664827, acc 0.734375\n",
      "2017-08-02T10:31:19.258672: step 428, loss 0.752988, acc 0.640625\n",
      "2017-08-02T10:31:19.815728: step 429, loss 0.819812, acc 0.59375\n",
      "2017-08-02T10:31:20.371783: step 430, loss 0.801438, acc 0.65625\n",
      "2017-08-02T10:31:20.923838: step 431, loss 0.739883, acc 0.703125\n",
      "2017-08-02T10:31:21.477894: step 432, loss 0.789961, acc 0.59375\n",
      "2017-08-02T10:31:22.030949: step 433, loss 0.70689, acc 0.75\n",
      "2017-08-02T10:31:22.592005: step 434, loss 0.81118, acc 0.578125\n",
      "2017-08-02T10:31:23.144060: step 435, loss 0.72713, acc 0.703125\n",
      "2017-08-02T10:31:23.699116: step 436, loss 0.728015, acc 0.703125\n",
      "2017-08-02T10:31:24.254171: step 437, loss 0.694468, acc 0.75\n",
      "2017-08-02T10:31:24.813227: step 438, loss 0.832437, acc 0.578125\n",
      "2017-08-02T10:31:25.374283: step 439, loss 0.815047, acc 0.5625\n",
      "2017-08-02T10:31:25.936340: step 440, loss 0.799252, acc 0.609375\n",
      "2017-08-02T10:31:26.498396: step 441, loss 0.770068, acc 0.640625\n",
      "2017-08-02T10:31:27.057452: step 442, loss 0.692543, acc 0.6875\n",
      "2017-08-02T10:31:27.616508: step 443, loss 0.78577, acc 0.640625\n",
      "2017-08-02T10:31:28.177564: step 444, loss 0.741294, acc 0.625\n",
      "2017-08-02T10:31:28.731619: step 445, loss 0.796156, acc 0.640625\n",
      "2017-08-02T10:31:29.290675: step 446, loss 0.777006, acc 0.578125\n",
      "2017-08-02T10:31:29.853731: step 447, loss 0.741604, acc 0.609375\n",
      "2017-08-02T10:31:30.410787: step 448, loss 0.795781, acc 0.625\n",
      "2017-08-02T10:31:30.964842: step 449, loss 0.764596, acc 0.625\n",
      "2017-08-02T10:31:31.516898: step 450, loss 0.727754, acc 0.6875\n",
      "2017-08-02T10:31:32.070953: step 451, loss 0.744265, acc 0.75\n",
      "2017-08-02T10:31:32.630009: step 452, loss 0.71971, acc 0.703125\n",
      "2017-08-02T10:31:33.188065: step 453, loss 0.75412, acc 0.65625\n",
      "2017-08-02T10:31:33.745120: step 454, loss 0.74901, acc 0.609375\n",
      "2017-08-02T10:31:34.309177: step 455, loss 0.720729, acc 0.671875\n",
      "2017-08-02T10:31:34.872233: step 456, loss 0.730968, acc 0.59375\n",
      "2017-08-02T10:31:35.434289: step 457, loss 0.782652, acc 0.515625\n",
      "2017-08-02T10:31:35.998346: step 458, loss 0.831914, acc 0.609375\n",
      "2017-08-02T10:31:36.568403: step 459, loss 0.82418, acc 0.5625\n",
      "2017-08-02T10:31:37.130459: step 460, loss 0.813027, acc 0.59375\n",
      "2017-08-02T10:31:37.687515: step 461, loss 0.746098, acc 0.671875\n",
      "2017-08-02T10:31:38.243570: step 462, loss 0.852318, acc 0.5625\n",
      "2017-08-02T10:31:38.809627: step 463, loss 0.876228, acc 0.5\n",
      "2017-08-02T10:31:39.367683: step 464, loss 0.752239, acc 0.5625\n",
      "2017-08-02T10:31:39.928739: step 465, loss 0.928523, acc 0.5\n",
      "2017-08-02T10:31:40.477794: step 466, loss 0.68479, acc 0.734375\n",
      "2017-08-02T10:31:41.032849: step 467, loss 0.655192, acc 0.6875\n",
      "2017-08-02T10:31:41.586905: step 468, loss 0.82978, acc 0.578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:31:42.141960: step 469, loss 0.650981, acc 0.71875\n",
      "2017-08-02T10:31:42.694015: step 470, loss 0.779108, acc 0.625\n",
      "2017-08-02T10:31:43.265072: step 471, loss 0.664454, acc 0.734375\n",
      "2017-08-02T10:31:43.819128: step 472, loss 0.797251, acc 0.609375\n",
      "2017-08-02T10:31:44.375183: step 473, loss 0.708133, acc 0.71875\n",
      "2017-08-02T10:31:44.935239: step 474, loss 0.727648, acc 0.640625\n",
      "2017-08-02T10:31:45.490295: step 475, loss 0.718257, acc 0.625\n",
      "2017-08-02T10:31:46.049351: step 476, loss 0.776705, acc 0.640625\n",
      "2017-08-02T10:31:46.616407: step 477, loss 0.701417, acc 0.671875\n",
      "2017-08-02T10:31:47.171463: step 478, loss 0.74147, acc 0.640625\n",
      "2017-08-02T10:31:47.733519: step 479, loss 0.816209, acc 0.484375\n",
      "2017-08-02T10:31:48.286574: step 480, loss 0.757392, acc 0.5625\n",
      "2017-08-02T10:31:48.839630: step 481, loss 0.690749, acc 0.65625\n",
      "2017-08-02T10:31:49.398686: step 482, loss 0.78349, acc 0.59375\n",
      "2017-08-02T10:31:49.957742: step 483, loss 0.78944, acc 0.625\n",
      "2017-08-02T10:31:50.513797: step 484, loss 0.714805, acc 0.59375\n",
      "2017-08-02T10:31:51.073853: step 485, loss 0.7459, acc 0.640625\n",
      "2017-08-02T10:31:51.630909: step 486, loss 0.792368, acc 0.5625\n",
      "2017-08-02T10:31:52.189965: step 487, loss 0.697344, acc 0.65625\n",
      "2017-08-02T10:31:52.752021: step 488, loss 0.720291, acc 0.625\n",
      "2017-08-02T10:31:53.311077: step 489, loss 0.655991, acc 0.78125\n",
      "2017-08-02T10:31:53.876133: step 490, loss 0.742777, acc 0.640625\n",
      "2017-08-02T10:31:54.430189: step 491, loss 0.710699, acc 0.671875\n",
      "2017-08-02T10:31:54.986244: step 492, loss 0.750377, acc 0.59375\n",
      "2017-08-02T10:31:55.562302: step 493, loss 0.844145, acc 0.53125\n",
      "2017-08-02T10:31:56.119358: step 494, loss 0.697486, acc 0.65625\n",
      "2017-08-02T10:31:56.678414: step 495, loss 0.701394, acc 0.640625\n",
      "2017-08-02T10:31:57.234469: step 496, loss 0.711568, acc 0.640625\n",
      "2017-08-02T10:31:57.786524: step 497, loss 0.683401, acc 0.640625\n",
      "2017-08-02T10:31:58.341580: step 498, loss 0.729005, acc 0.625\n",
      "2017-08-02T10:31:58.900636: step 499, loss 0.737938, acc 0.65625\n",
      "2017-08-02T10:31:59.455691: step 500, loss 0.666786, acc 0.625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:32:03.082054: step 500, loss 0.680424, acc 0.7408\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-500\n",
      "\n",
      "2017-08-02T10:32:07.747520: step 501, loss 0.669148, acc 0.671875\n",
      "2017-08-02T10:32:08.320578: step 502, loss 0.696592, acc 0.6875\n",
      "2017-08-02T10:32:08.898635: step 503, loss 0.715961, acc 0.625\n",
      "2017-08-02T10:32:09.452691: step 504, loss 0.696133, acc 0.75\n",
      "2017-08-02T10:32:10.032749: step 505, loss 0.767006, acc 0.578125\n",
      "2017-08-02T10:32:10.599806: step 506, loss 0.830932, acc 0.5\n",
      "2017-08-02T10:32:11.161862: step 507, loss 0.73882, acc 0.609375\n",
      "2017-08-02T10:32:11.713917: step 508, loss 0.668133, acc 0.65625\n",
      "2017-08-02T10:32:12.272973: step 509, loss 0.687961, acc 0.625\n",
      "2017-08-02T10:32:12.832029: step 510, loss 0.670111, acc 0.671875\n",
      "2017-08-02T10:32:13.390085: step 511, loss 0.728275, acc 0.671875\n",
      "2017-08-02T10:32:13.950141: step 512, loss 0.791047, acc 0.46875\n",
      "2017-08-02T10:32:14.510197: step 513, loss 0.679419, acc 0.65625\n",
      "2017-08-02T10:32:15.069252: step 514, loss 0.677523, acc 0.65625\n",
      "2017-08-02T10:32:15.622308: step 515, loss 0.679466, acc 0.6875\n",
      "2017-08-02T10:32:16.188364: step 516, loss 0.726856, acc 0.640625\n",
      "2017-08-02T10:32:16.744420: step 517, loss 0.620121, acc 0.734375\n",
      "2017-08-02T10:32:17.301476: step 518, loss 0.736489, acc 0.640625\n",
      "2017-08-02T10:32:17.863532: step 519, loss 0.653519, acc 0.6875\n",
      "2017-08-02T10:32:18.440590: step 520, loss 0.72061, acc 0.625\n",
      "2017-08-02T10:32:18.994645: step 521, loss 0.665923, acc 0.6875\n",
      "2017-08-02T10:32:19.545700: step 522, loss 0.689488, acc 0.671875\n",
      "2017-08-02T10:32:20.103756: step 523, loss 0.764814, acc 0.53125\n",
      "2017-08-02T10:32:20.660812: step 524, loss 0.672151, acc 0.640625\n",
      "2017-08-02T10:32:21.238869: step 525, loss 0.741615, acc 0.609375\n",
      "2017-08-02T10:32:21.791925: step 526, loss 0.749822, acc 0.65625\n",
      "2017-08-02T10:32:22.347980: step 527, loss 0.696951, acc 0.703125\n",
      "2017-08-02T10:32:22.910036: step 528, loss 0.836412, acc 0.5625\n",
      "2017-08-02T10:32:23.477093: step 529, loss 0.804644, acc 0.5625\n",
      "2017-08-02T10:32:24.031149: step 530, loss 0.621458, acc 0.765625\n",
      "2017-08-02T10:32:24.586204: step 531, loss 0.696847, acc 0.6875\n",
      "2017-08-02T10:32:25.144260: step 532, loss 0.668688, acc 0.625\n",
      "2017-08-02T10:32:25.698315: step 533, loss 0.853935, acc 0.5\n",
      "2017-08-02T10:32:26.264372: step 534, loss 0.658419, acc 0.625\n",
      "2017-08-02T10:32:26.825428: step 535, loss 0.663637, acc 0.625\n",
      "2017-08-02T10:32:27.387484: step 536, loss 0.759944, acc 0.65625\n",
      "2017-08-02T10:32:27.949540: step 537, loss 0.698277, acc 0.625\n",
      "2017-08-02T10:32:28.506596: step 538, loss 0.842893, acc 0.53125\n",
      "2017-08-02T10:32:29.065652: step 539, loss 0.852346, acc 0.578125\n",
      "2017-08-02T10:32:29.626708: step 540, loss 0.701709, acc 0.6875\n",
      "2017-08-02T10:32:30.184764: step 541, loss 0.873574, acc 0.5\n",
      "2017-08-02T10:32:30.738819: step 542, loss 0.843432, acc 0.5625\n",
      "2017-08-02T10:32:31.300875: step 543, loss 0.729902, acc 0.609375\n",
      "2017-08-02T10:32:31.870932: step 544, loss 0.690707, acc 0.671875\n",
      "2017-08-02T10:32:32.425988: step 545, loss 0.791757, acc 0.59375\n",
      "2017-08-02T10:32:33.012047: step 546, loss 0.80386, acc 0.515625\n",
      "2017-08-02T10:32:33.570102: step 547, loss 0.666336, acc 0.6875\n",
      "2017-08-02T10:32:34.131158: step 548, loss 0.744108, acc 0.578125\n",
      "2017-08-02T10:32:34.691214: step 549, loss 0.64796, acc 0.71875\n",
      "2017-08-02T10:32:35.246270: step 550, loss 0.656925, acc 0.6875\n",
      "2017-08-02T10:32:35.802326: step 551, loss 0.704646, acc 0.625\n",
      "2017-08-02T10:32:36.362382: step 552, loss 0.654027, acc 0.71875\n",
      "2017-08-02T10:32:36.920437: step 553, loss 0.744476, acc 0.640625\n",
      "2017-08-02T10:32:37.482494: step 554, loss 0.735048, acc 0.59375\n",
      "2017-08-02T10:32:38.050550: step 555, loss 0.698309, acc 0.671875\n",
      "2017-08-02T10:32:38.605606: step 556, loss 0.704131, acc 0.640625\n",
      "2017-08-02T10:32:39.162662: step 557, loss 0.640895, acc 0.71875\n",
      "2017-08-02T10:32:39.715717: step 558, loss 0.733457, acc 0.609375\n",
      "2017-08-02T10:32:40.276773: step 559, loss 0.692486, acc 0.609375\n",
      "2017-08-02T10:32:40.835829: step 560, loss 0.681424, acc 0.6875\n",
      "2017-08-02T10:32:41.395885: step 561, loss 0.666062, acc 0.6875\n",
      "2017-08-02T10:32:41.956941: step 562, loss 0.679801, acc 0.703125\n",
      "2017-08-02T10:32:42.514997: step 563, loss 0.694711, acc 0.625\n",
      "2017-08-02T10:32:43.075053: step 564, loss 0.621922, acc 0.734375\n",
      "2017-08-02T10:32:43.653111: step 565, loss 0.674137, acc 0.671875\n",
      "2017-08-02T10:32:44.225168: step 566, loss 0.738684, acc 0.5625\n",
      "2017-08-02T10:32:44.788224: step 567, loss 0.721942, acc 0.609375\n",
      "2017-08-02T10:32:45.350280: step 568, loss 0.671952, acc 0.65625\n",
      "2017-08-02T10:32:45.911336: step 569, loss 0.751283, acc 0.5625\n",
      "2017-08-02T10:32:46.468392: step 570, loss 0.685072, acc 0.6875\n",
      "2017-08-02T10:32:47.030448: step 571, loss 0.635494, acc 0.75\n",
      "2017-08-02T10:32:47.591504: step 572, loss 0.637698, acc 0.734375\n",
      "2017-08-02T10:32:48.155561: step 573, loss 0.68019, acc 0.671875\n",
      "2017-08-02T10:32:48.710616: step 574, loss 0.698889, acc 0.65625\n",
      "2017-08-02T10:32:49.268672: step 575, loss 0.650473, acc 0.75\n",
      "2017-08-02T10:32:49.827728: step 576, loss 0.763548, acc 0.578125\n",
      "2017-08-02T10:32:50.385784: step 577, loss 0.714307, acc 0.640625\n",
      "2017-08-02T10:32:50.963842: step 578, loss 0.64273, acc 0.734375\n",
      "2017-08-02T10:32:51.521897: step 579, loss 0.660344, acc 0.671875\n",
      "2017-08-02T10:32:52.077953: step 580, loss 0.905083, acc 0.484375\n",
      "2017-08-02T10:32:52.631008: step 581, loss 0.632033, acc 0.6875\n",
      "2017-08-02T10:32:53.191064: step 582, loss 0.62954, acc 0.734375\n",
      "2017-08-02T10:32:53.744120: step 583, loss 0.782186, acc 0.609375\n",
      "2017-08-02T10:32:54.306176: step 584, loss 0.708842, acc 0.609375\n",
      "2017-08-02T10:32:54.867232: step 585, loss 0.719165, acc 0.703125\n",
      "2017-08-02T10:32:55.432288: step 586, loss 0.635691, acc 0.671875\n",
      "2017-08-02T10:32:55.995345: step 587, loss 0.688737, acc 0.6875\n",
      "2017-08-02T10:32:56.557401: step 588, loss 0.6977, acc 0.734375\n",
      "2017-08-02T10:32:57.120457: step 589, loss 0.761792, acc 0.53125\n",
      "2017-08-02T10:32:57.674513: step 590, loss 0.744946, acc 0.59375\n",
      "2017-08-02T10:32:58.241569: step 591, loss 0.682526, acc 0.65625\n",
      "2017-08-02T10:32:58.796625: step 592, loss 0.675956, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:32:59.352680: step 593, loss 0.857102, acc 0.53125\n",
      "2017-08-02T10:32:59.905736: step 594, loss 0.824426, acc 0.5625\n",
      "2017-08-02T10:33:00.461791: step 595, loss 0.624929, acc 0.75\n",
      "2017-08-02T10:33:01.015847: step 596, loss 0.608732, acc 0.734375\n",
      "2017-08-02T10:33:01.573902: step 597, loss 0.771131, acc 0.578125\n",
      "2017-08-02T10:33:02.125958: step 598, loss 0.665621, acc 0.671875\n",
      "2017-08-02T10:33:02.680013: step 599, loss 0.523859, acc 0.78125\n",
      "2017-08-02T10:33:03.237069: step 600, loss 0.709387, acc 0.65625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:33:06.905436: step 600, loss 0.62584, acc 0.7212\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-600\n",
      "\n",
      "2017-08-02T10:33:13.682113: step 601, loss 0.720408, acc 0.625\n",
      "2017-08-02T10:33:14.239169: step 602, loss 0.71488, acc 0.640625\n",
      "2017-08-02T10:33:14.796225: step 603, loss 0.624597, acc 0.6875\n",
      "2017-08-02T10:33:15.347280: step 604, loss 0.592863, acc 0.734375\n",
      "2017-08-02T10:33:15.901335: step 605, loss 0.646037, acc 0.71875\n",
      "2017-08-02T10:33:16.455390: step 606, loss 0.737199, acc 0.609375\n",
      "2017-08-02T10:33:17.023447: step 607, loss 0.524785, acc 0.8125\n",
      "2017-08-02T10:33:17.577503: step 608, loss 0.692918, acc 0.65625\n",
      "2017-08-02T10:33:18.134558: step 609, loss 0.607574, acc 0.703125\n",
      "2017-08-02T10:33:18.690614: step 610, loss 0.61884, acc 0.6875\n",
      "2017-08-02T10:33:19.244669: step 611, loss 0.645756, acc 0.6875\n",
      "2017-08-02T10:33:19.793724: step 612, loss 0.82088, acc 0.53125\n",
      "2017-08-02T10:33:20.355780: step 613, loss 0.699554, acc 0.625\n",
      "2017-08-02T10:33:20.920837: step 614, loss 0.656074, acc 0.625\n",
      "2017-08-02T10:33:21.472892: step 615, loss 0.712085, acc 0.609375\n",
      "2017-08-02T10:33:22.027948: step 616, loss 0.713275, acc 0.65625\n",
      "2017-08-02T10:33:22.580003: step 617, loss 0.721145, acc 0.671875\n",
      "2017-08-02T10:33:23.135058: step 618, loss 0.691047, acc 0.578125\n",
      "2017-08-02T10:33:23.688114: step 619, loss 0.594121, acc 0.734375\n",
      "2017-08-02T10:33:24.245169: step 620, loss 0.776998, acc 0.546875\n",
      "2017-08-02T10:33:24.801225: step 621, loss 0.68846, acc 0.640625\n",
      "2017-08-02T10:33:25.358281: step 622, loss 0.776689, acc 0.53125\n",
      "2017-08-02T10:33:25.920337: step 623, loss 0.68574, acc 0.625\n",
      "2017-08-02T10:33:26.478393: step 624, loss 0.629286, acc 0.703125\n",
      "2017-08-02T10:33:27.034448: step 625, loss 0.70044, acc 0.625\n",
      "2017-08-02T10:33:27.445489: step 626, loss 0.667808, acc 0.65625\n",
      "2017-08-02T10:33:28.001545: step 627, loss 0.605216, acc 0.6875\n",
      "2017-08-02T10:33:28.554600: step 628, loss 0.616973, acc 0.703125\n",
      "2017-08-02T10:33:29.111656: step 629, loss 0.634941, acc 0.6875\n",
      "2017-08-02T10:33:29.666711: step 630, loss 0.654277, acc 0.6875\n",
      "2017-08-02T10:33:30.227768: step 631, loss 0.61577, acc 0.703125\n",
      "2017-08-02T10:33:30.786823: step 632, loss 0.610222, acc 0.703125\n",
      "2017-08-02T10:33:31.338879: step 633, loss 0.641517, acc 0.703125\n",
      "2017-08-02T10:33:31.892934: step 634, loss 0.70277, acc 0.59375\n",
      "2017-08-02T10:33:32.453990: step 635, loss 0.720573, acc 0.65625\n",
      "2017-08-02T10:33:33.015046: step 636, loss 0.658115, acc 0.65625\n",
      "2017-08-02T10:33:33.578103: step 637, loss 0.587728, acc 0.703125\n",
      "2017-08-02T10:33:34.136158: step 638, loss 0.659201, acc 0.6875\n",
      "2017-08-02T10:33:34.695214: step 639, loss 0.711888, acc 0.609375\n",
      "2017-08-02T10:33:35.256270: step 640, loss 0.638212, acc 0.75\n",
      "2017-08-02T10:33:35.850330: step 641, loss 0.557458, acc 0.71875\n",
      "2017-08-02T10:33:36.421387: step 642, loss 0.712769, acc 0.671875\n",
      "2017-08-02T10:33:36.985443: step 643, loss 0.625001, acc 0.703125\n",
      "2017-08-02T10:33:37.546499: step 644, loss 0.763364, acc 0.5625\n",
      "2017-08-02T10:33:38.102555: step 645, loss 0.605036, acc 0.765625\n",
      "2017-08-02T10:33:38.656610: step 646, loss 0.623211, acc 0.71875\n",
      "2017-08-02T10:33:39.212666: step 647, loss 0.712552, acc 0.65625\n",
      "2017-08-02T10:33:39.770722: step 648, loss 0.63168, acc 0.703125\n",
      "2017-08-02T10:33:40.327777: step 649, loss 0.663204, acc 0.640625\n",
      "2017-08-02T10:33:40.886833: step 650, loss 0.706289, acc 0.65625\n",
      "2017-08-02T10:33:41.453890: step 651, loss 0.752163, acc 0.53125\n",
      "2017-08-02T10:33:42.030948: step 652, loss 0.667329, acc 0.65625\n",
      "2017-08-02T10:33:42.606005: step 653, loss 0.570933, acc 0.765625\n",
      "2017-08-02T10:33:43.171062: step 654, loss 0.691747, acc 0.59375\n",
      "2017-08-02T10:33:43.741119: step 655, loss 0.591617, acc 0.734375\n",
      "2017-08-02T10:33:44.306175: step 656, loss 0.717921, acc 0.625\n",
      "2017-08-02T10:33:44.954240: step 657, loss 0.591152, acc 0.703125\n",
      "2017-08-02T10:33:45.578302: step 658, loss 0.703866, acc 0.671875\n",
      "2017-08-02T10:33:46.182363: step 659, loss 0.587307, acc 0.765625\n",
      "2017-08-02T10:33:46.784423: step 660, loss 0.624704, acc 0.71875\n",
      "2017-08-02T10:33:47.384483: step 661, loss 0.571571, acc 0.765625\n",
      "2017-08-02T10:33:47.986543: step 662, loss 0.700821, acc 0.671875\n",
      "2017-08-02T10:33:48.599605: step 663, loss 0.593548, acc 0.71875\n",
      "2017-08-02T10:33:49.213666: step 664, loss 0.601609, acc 0.734375\n",
      "2017-08-02T10:33:49.816726: step 665, loss 0.593964, acc 0.671875\n",
      "2017-08-02T10:33:50.432788: step 666, loss 0.59889, acc 0.703125\n",
      "2017-08-02T10:33:51.048849: step 667, loss 0.580831, acc 0.6875\n",
      "2017-08-02T10:33:51.647909: step 668, loss 0.693671, acc 0.65625\n",
      "2017-08-02T10:33:52.248969: step 669, loss 0.618446, acc 0.640625\n",
      "2017-08-02T10:33:52.850030: step 670, loss 0.610014, acc 0.671875\n",
      "2017-08-02T10:33:53.459090: step 671, loss 0.671565, acc 0.625\n",
      "2017-08-02T10:33:54.068151: step 672, loss 0.65778, acc 0.703125\n",
      "2017-08-02T10:33:54.670212: step 673, loss 0.680836, acc 0.671875\n",
      "2017-08-02T10:33:55.280273: step 674, loss 0.612073, acc 0.6875\n",
      "2017-08-02T10:33:55.888333: step 675, loss 0.669486, acc 0.609375\n",
      "2017-08-02T10:33:56.486393: step 676, loss 0.590457, acc 0.65625\n",
      "2017-08-02T10:33:57.099454: step 677, loss 0.574872, acc 0.78125\n",
      "2017-08-02T10:33:57.703515: step 678, loss 0.590107, acc 0.71875\n",
      "2017-08-02T10:33:58.305575: step 679, loss 0.611623, acc 0.6875\n",
      "2017-08-02T10:33:58.903635: step 680, loss 0.674175, acc 0.6875\n",
      "2017-08-02T10:33:59.514696: step 681, loss 0.59364, acc 0.765625\n",
      "2017-08-02T10:34:00.118756: step 682, loss 0.617578, acc 0.65625\n",
      "2017-08-02T10:34:00.714816: step 683, loss 0.68585, acc 0.640625\n",
      "2017-08-02T10:34:01.322877: step 684, loss 0.655887, acc 0.671875\n",
      "2017-08-02T10:34:01.920937: step 685, loss 0.74046, acc 0.609375\n",
      "2017-08-02T10:34:02.519996: step 686, loss 0.62415, acc 0.640625\n",
      "2017-08-02T10:34:03.118056: step 687, loss 0.610747, acc 0.671875\n",
      "2017-08-02T10:34:03.716116: step 688, loss 0.630487, acc 0.71875\n",
      "2017-08-02T10:34:04.276172: step 689, loss 0.628491, acc 0.703125\n",
      "2017-08-02T10:34:04.832228: step 690, loss 0.637318, acc 0.609375\n",
      "2017-08-02T10:34:05.389283: step 691, loss 0.71368, acc 0.625\n",
      "2017-08-02T10:34:05.941339: step 692, loss 0.616541, acc 0.734375\n",
      "2017-08-02T10:34:06.505395: step 693, loss 0.632578, acc 0.671875\n",
      "2017-08-02T10:34:07.064451: step 694, loss 0.621057, acc 0.625\n",
      "2017-08-02T10:34:07.623507: step 695, loss 0.769912, acc 0.5625\n",
      "2017-08-02T10:34:08.178562: step 696, loss 0.658738, acc 0.703125\n",
      "2017-08-02T10:34:08.747619: step 697, loss 0.601859, acc 0.734375\n",
      "2017-08-02T10:34:09.319676: step 698, loss 0.596406, acc 0.703125\n",
      "2017-08-02T10:34:09.895734: step 699, loss 0.643192, acc 0.609375\n",
      "2017-08-02T10:34:10.457790: step 700, loss 0.740916, acc 0.53125\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:34:14.188163: step 700, loss 0.62245, acc 0.6664\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-700\n",
      "\n",
      "2017-08-02T10:34:19.360680: step 701, loss 0.696801, acc 0.65625\n",
      "2017-08-02T10:34:19.920736: step 702, loss 0.631911, acc 0.671875\n",
      "2017-08-02T10:34:20.477792: step 703, loss 0.649409, acc 0.734375\n",
      "2017-08-02T10:34:21.037848: step 704, loss 0.608564, acc 0.703125\n",
      "2017-08-02T10:34:21.594904: step 705, loss 0.662898, acc 0.671875\n",
      "2017-08-02T10:34:22.155960: step 706, loss 0.614663, acc 0.640625\n",
      "2017-08-02T10:34:22.721016: step 707, loss 0.728309, acc 0.625\n",
      "2017-08-02T10:34:23.290073: step 708, loss 0.639014, acc 0.640625\n",
      "2017-08-02T10:34:23.847129: step 709, loss 0.661854, acc 0.625\n",
      "2017-08-02T10:34:24.406185: step 710, loss 0.615262, acc 0.671875\n",
      "2017-08-02T10:34:24.976242: step 711, loss 0.619611, acc 0.671875\n",
      "2017-08-02T10:34:25.536298: step 712, loss 0.635094, acc 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:34:26.100354: step 713, loss 0.628397, acc 0.640625\n",
      "2017-08-02T10:34:26.658410: step 714, loss 0.666623, acc 0.671875\n",
      "2017-08-02T10:34:27.214466: step 715, loss 0.722114, acc 0.609375\n",
      "2017-08-02T10:34:27.767521: step 716, loss 0.687091, acc 0.65625\n",
      "2017-08-02T10:34:28.332577: step 717, loss 0.689824, acc 0.6875\n",
      "2017-08-02T10:34:28.917636: step 718, loss 0.655845, acc 0.59375\n",
      "2017-08-02T10:34:29.484693: step 719, loss 0.637507, acc 0.59375\n",
      "2017-08-02T10:34:30.038748: step 720, loss 0.55783, acc 0.71875\n",
      "2017-08-02T10:34:30.596804: step 721, loss 0.703931, acc 0.5625\n",
      "2017-08-02T10:34:31.154860: step 722, loss 0.55466, acc 0.71875\n",
      "2017-08-02T10:34:31.709915: step 723, loss 0.711082, acc 0.546875\n",
      "2017-08-02T10:34:32.274972: step 724, loss 0.734907, acc 0.578125\n",
      "2017-08-02T10:34:32.843028: step 725, loss 0.657333, acc 0.625\n",
      "2017-08-02T10:34:33.404085: step 726, loss 0.64453, acc 0.6875\n",
      "2017-08-02T10:34:33.964141: step 727, loss 0.526216, acc 0.734375\n",
      "2017-08-02T10:34:34.526197: step 728, loss 0.559311, acc 0.765625\n",
      "2017-08-02T10:34:35.082252: step 729, loss 0.63102, acc 0.671875\n",
      "2017-08-02T10:34:35.642308: step 730, loss 0.600351, acc 0.734375\n",
      "2017-08-02T10:34:36.202364: step 731, loss 0.579696, acc 0.671875\n",
      "2017-08-02T10:34:36.760420: step 732, loss 0.582784, acc 0.75\n",
      "2017-08-02T10:34:37.345479: step 733, loss 0.705524, acc 0.578125\n",
      "2017-08-02T10:34:37.901534: step 734, loss 0.527352, acc 0.828125\n",
      "2017-08-02T10:34:38.461590: step 735, loss 0.561876, acc 0.734375\n",
      "2017-08-02T10:34:39.019646: step 736, loss 0.592421, acc 0.75\n",
      "2017-08-02T10:34:39.582702: step 737, loss 0.562979, acc 0.71875\n",
      "2017-08-02T10:34:40.147759: step 738, loss 0.606012, acc 0.703125\n",
      "2017-08-02T10:34:40.716816: step 739, loss 0.646928, acc 0.65625\n",
      "2017-08-02T10:34:41.277872: step 740, loss 0.730262, acc 0.59375\n",
      "2017-08-02T10:34:41.840928: step 741, loss 0.640964, acc 0.640625\n",
      "2017-08-02T10:34:42.397984: step 742, loss 0.562748, acc 0.6875\n",
      "2017-08-02T10:34:42.952039: step 743, loss 0.667443, acc 0.640625\n",
      "2017-08-02T10:34:43.504094: step 744, loss 0.572918, acc 0.75\n",
      "2017-08-02T10:34:44.065151: step 745, loss 0.573224, acc 0.78125\n",
      "2017-08-02T10:34:44.621206: step 746, loss 0.660105, acc 0.65625\n",
      "2017-08-02T10:34:45.174261: step 747, loss 0.600115, acc 0.71875\n",
      "2017-08-02T10:34:45.727317: step 748, loss 0.502347, acc 0.78125\n",
      "2017-08-02T10:34:46.289373: step 749, loss 0.62793, acc 0.71875\n",
      "2017-08-02T10:34:46.847429: step 750, loss 0.5662, acc 0.75\n",
      "2017-08-02T10:34:47.400484: step 751, loss 0.512753, acc 0.765625\n",
      "2017-08-02T10:34:47.957540: step 752, loss 0.597023, acc 0.734375\n",
      "2017-08-02T10:34:48.514595: step 753, loss 0.570967, acc 0.78125\n",
      "2017-08-02T10:34:49.069651: step 754, loss 0.665335, acc 0.65625\n",
      "2017-08-02T10:34:49.627707: step 755, loss 0.564916, acc 0.71875\n",
      "2017-08-02T10:34:50.182762: step 756, loss 0.577048, acc 0.71875\n",
      "2017-08-02T10:34:50.742818: step 757, loss 0.527322, acc 0.75\n",
      "2017-08-02T10:34:51.299874: step 758, loss 0.584581, acc 0.765625\n",
      "2017-08-02T10:34:51.852929: step 759, loss 0.552484, acc 0.78125\n",
      "2017-08-02T10:34:52.430987: step 760, loss 0.659685, acc 0.625\n",
      "2017-08-02T10:34:52.986043: step 761, loss 0.538788, acc 0.765625\n",
      "2017-08-02T10:34:53.539098: step 762, loss 0.646165, acc 0.625\n",
      "2017-08-02T10:34:54.095153: step 763, loss 0.775118, acc 0.515625\n",
      "2017-08-02T10:34:54.656210: step 764, loss 0.698352, acc 0.515625\n",
      "2017-08-02T10:34:55.243268: step 765, loss 0.676626, acc 0.625\n",
      "2017-08-02T10:34:55.832327: step 766, loss 0.566192, acc 0.71875\n",
      "2017-08-02T10:34:56.423386: step 767, loss 0.651595, acc 0.671875\n",
      "2017-08-02T10:34:57.003444: step 768, loss 0.626375, acc 0.6875\n",
      "2017-08-02T10:34:57.577502: step 769, loss 0.754491, acc 0.640625\n",
      "2017-08-02T10:34:58.154559: step 770, loss 0.6016, acc 0.6875\n",
      "2017-08-02T10:34:58.758620: step 771, loss 0.665906, acc 0.703125\n",
      "2017-08-02T10:34:59.317676: step 772, loss 0.63105, acc 0.703125\n",
      "2017-08-02T10:34:59.881732: step 773, loss 0.642541, acc 0.71875\n",
      "2017-08-02T10:35:00.445788: step 774, loss 0.637371, acc 0.671875\n",
      "2017-08-02T10:35:01.030847: step 775, loss 0.623767, acc 0.734375\n",
      "2017-08-02T10:35:01.607905: step 776, loss 0.641547, acc 0.65625\n",
      "2017-08-02T10:35:02.165960: step 777, loss 0.607435, acc 0.703125\n",
      "2017-08-02T10:35:02.722016: step 778, loss 0.537186, acc 0.75\n",
      "2017-08-02T10:35:03.282072: step 779, loss 0.560663, acc 0.734375\n",
      "2017-08-02T10:35:03.839128: step 780, loss 0.603903, acc 0.671875\n",
      "2017-08-02T10:35:04.401184: step 781, loss 0.605618, acc 0.734375\n",
      "2017-08-02T10:35:04.974241: step 782, loss 0.531722, acc 0.78125\n",
      "2017-08-02T10:35:05.529297: step 783, loss 0.525584, acc 0.84375\n",
      "2017-08-02T10:35:06.083352: step 784, loss 0.583888, acc 0.65625\n",
      "2017-08-02T10:35:06.647409: step 785, loss 0.457727, acc 0.78125\n",
      "2017-08-02T10:35:07.204464: step 786, loss 0.547331, acc 0.71875\n",
      "2017-08-02T10:35:07.760520: step 787, loss 0.515697, acc 0.75\n",
      "2017-08-02T10:35:08.318576: step 788, loss 0.540001, acc 0.734375\n",
      "2017-08-02T10:35:08.881632: step 789, loss 0.639018, acc 0.703125\n",
      "2017-08-02T10:35:09.438688: step 790, loss 0.577674, acc 0.71875\n",
      "2017-08-02T10:35:09.997744: step 791, loss 0.613977, acc 0.640625\n",
      "2017-08-02T10:35:10.553799: step 792, loss 0.623969, acc 0.6875\n",
      "2017-08-02T10:35:11.115855: step 793, loss 0.599778, acc 0.734375\n",
      "2017-08-02T10:35:11.674911: step 794, loss 0.688922, acc 0.640625\n",
      "2017-08-02T10:35:12.230967: step 795, loss 0.497505, acc 0.765625\n",
      "2017-08-02T10:35:12.792023: step 796, loss 0.641892, acc 0.703125\n",
      "2017-08-02T10:35:13.350079: step 797, loss 0.663784, acc 0.578125\n",
      "2017-08-02T10:35:13.916135: step 798, loss 0.576139, acc 0.734375\n",
      "2017-08-02T10:35:14.498194: step 799, loss 0.610503, acc 0.6875\n",
      "2017-08-02T10:35:15.053249: step 800, loss 0.532763, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:35:18.752619: step 800, loss 0.566305, acc 0.7408\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-800\n",
      "\n",
      "2017-08-02T10:35:23.708114: step 801, loss 0.600159, acc 0.734375\n",
      "2017-08-02T10:35:24.262170: step 802, loss 0.707302, acc 0.65625\n",
      "2017-08-02T10:35:24.819226: step 803, loss 0.580276, acc 0.75\n",
      "2017-08-02T10:35:25.371281: step 804, loss 0.619786, acc 0.71875\n",
      "2017-08-02T10:35:25.926336: step 805, loss 0.669203, acc 0.625\n",
      "2017-08-02T10:35:26.480392: step 806, loss 0.621626, acc 0.609375\n",
      "2017-08-02T10:35:27.035447: step 807, loss 0.584928, acc 0.71875\n",
      "2017-08-02T10:35:27.587502: step 808, loss 0.649264, acc 0.640625\n",
      "2017-08-02T10:35:28.140558: step 809, loss 0.627289, acc 0.640625\n",
      "2017-08-02T10:35:28.695613: step 810, loss 0.593354, acc 0.75\n",
      "2017-08-02T10:35:29.248668: step 811, loss 0.659715, acc 0.703125\n",
      "2017-08-02T10:35:29.802724: step 812, loss 0.672368, acc 0.671875\n",
      "2017-08-02T10:35:30.357779: step 813, loss 0.646301, acc 0.703125\n",
      "2017-08-02T10:35:30.911835: step 814, loss 0.572796, acc 0.734375\n",
      "2017-08-02T10:35:31.466890: step 815, loss 0.546815, acc 0.75\n",
      "2017-08-02T10:35:32.019946: step 816, loss 0.545397, acc 0.71875\n",
      "2017-08-02T10:35:32.577001: step 817, loss 0.549243, acc 0.71875\n",
      "2017-08-02T10:35:33.132057: step 818, loss 0.690015, acc 0.625\n",
      "2017-08-02T10:35:33.687112: step 819, loss 0.629916, acc 0.703125\n",
      "2017-08-02T10:35:34.244168: step 820, loss 0.529164, acc 0.71875\n",
      "2017-08-02T10:35:34.806224: step 821, loss 0.529431, acc 0.78125\n",
      "2017-08-02T10:35:35.361280: step 822, loss 0.566912, acc 0.734375\n",
      "2017-08-02T10:35:35.922336: step 823, loss 0.630378, acc 0.703125\n",
      "2017-08-02T10:35:36.474391: step 824, loss 0.608714, acc 0.703125\n",
      "2017-08-02T10:35:37.030447: step 825, loss 0.579876, acc 0.625\n",
      "2017-08-02T10:35:37.586502: step 826, loss 0.702946, acc 0.59375\n",
      "2017-08-02T10:35:38.147558: step 827, loss 0.581412, acc 0.671875\n",
      "2017-08-02T10:35:38.706614: step 828, loss 0.554183, acc 0.71875\n",
      "2017-08-02T10:35:39.277671: step 829, loss 0.65618, acc 0.609375\n",
      "2017-08-02T10:35:39.834727: step 830, loss 0.536705, acc 0.734375\n",
      "2017-08-02T10:35:40.394783: step 831, loss 0.520871, acc 0.765625\n",
      "2017-08-02T10:35:40.955839: step 832, loss 0.663112, acc 0.703125\n",
      "2017-08-02T10:35:41.515895: step 833, loss 0.690696, acc 0.625\n",
      "2017-08-02T10:35:42.069950: step 834, loss 0.543786, acc 0.765625\n",
      "2017-08-02T10:35:42.627006: step 835, loss 0.689401, acc 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:35:43.186062: step 836, loss 0.758305, acc 0.59375\n",
      "2017-08-02T10:35:43.745118: step 837, loss 0.61721, acc 0.703125\n",
      "2017-08-02T10:35:44.304174: step 838, loss 0.572368, acc 0.640625\n",
      "2017-08-02T10:35:44.867230: step 839, loss 0.569855, acc 0.6875\n",
      "2017-08-02T10:35:45.418285: step 840, loss 0.563894, acc 0.734375\n",
      "2017-08-02T10:35:45.971341: step 841, loss 0.560292, acc 0.703125\n",
      "2017-08-02T10:35:46.522396: step 842, loss 0.69114, acc 0.609375\n",
      "2017-08-02T10:35:47.073451: step 843, loss 0.603121, acc 0.6875\n",
      "2017-08-02T10:35:47.633507: step 844, loss 0.74205, acc 0.609375\n",
      "2017-08-02T10:35:48.185562: step 845, loss 0.673747, acc 0.609375\n",
      "2017-08-02T10:35:48.738617: step 846, loss 0.476781, acc 0.84375\n",
      "2017-08-02T10:35:49.295673: step 847, loss 0.534995, acc 0.75\n",
      "2017-08-02T10:35:49.849728: step 848, loss 0.506862, acc 0.765625\n",
      "2017-08-02T10:35:50.409784: step 849, loss 0.603516, acc 0.6875\n",
      "2017-08-02T10:35:50.970840: step 850, loss 0.603081, acc 0.703125\n",
      "2017-08-02T10:35:51.527896: step 851, loss 0.487335, acc 0.78125\n",
      "2017-08-02T10:35:52.082952: step 852, loss 0.5055, acc 0.75\n",
      "2017-08-02T10:35:52.640007: step 853, loss 0.692019, acc 0.59375\n",
      "2017-08-02T10:35:53.201063: step 854, loss 0.605649, acc 0.671875\n",
      "2017-08-02T10:35:53.752119: step 855, loss 0.543153, acc 0.75\n",
      "2017-08-02T10:35:54.315175: step 856, loss 0.52927, acc 0.78125\n",
      "2017-08-02T10:35:54.877231: step 857, loss 0.535394, acc 0.796875\n",
      "2017-08-02T10:35:55.453289: step 858, loss 0.557562, acc 0.75\n",
      "2017-08-02T10:35:56.016345: step 859, loss 0.539435, acc 0.71875\n",
      "2017-08-02T10:35:56.571400: step 860, loss 0.53115, acc 0.734375\n",
      "2017-08-02T10:35:57.133457: step 861, loss 0.792686, acc 0.546875\n",
      "2017-08-02T10:35:57.685512: step 862, loss 0.605479, acc 0.671875\n",
      "2017-08-02T10:35:58.240567: step 863, loss 0.561411, acc 0.6875\n",
      "2017-08-02T10:35:58.801623: step 864, loss 0.665333, acc 0.625\n",
      "2017-08-02T10:35:59.362680: step 865, loss 0.64596, acc 0.6875\n",
      "2017-08-02T10:35:59.919735: step 866, loss 0.515669, acc 0.75\n",
      "2017-08-02T10:36:00.477791: step 867, loss 0.581519, acc 0.734375\n",
      "2017-08-02T10:36:01.036847: step 868, loss 0.63134, acc 0.6875\n",
      "2017-08-02T10:36:01.596903: step 869, loss 0.592434, acc 0.734375\n",
      "2017-08-02T10:36:02.156959: step 870, loss 0.637576, acc 0.71875\n",
      "2017-08-02T10:36:02.709014: step 871, loss 0.580151, acc 0.75\n",
      "2017-08-02T10:36:03.266070: step 872, loss 0.523009, acc 0.734375\n",
      "2017-08-02T10:36:03.821125: step 873, loss 0.560869, acc 0.703125\n",
      "2017-08-02T10:36:04.385182: step 874, loss 0.647049, acc 0.609375\n",
      "2017-08-02T10:36:04.939237: step 875, loss 0.548606, acc 0.734375\n",
      "2017-08-02T10:36:05.496293: step 876, loss 0.565752, acc 0.734375\n",
      "2017-08-02T10:36:06.055349: step 877, loss 0.552586, acc 0.71875\n",
      "2017-08-02T10:36:06.610404: step 878, loss 0.606483, acc 0.703125\n",
      "2017-08-02T10:36:07.166460: step 879, loss 0.515555, acc 0.765625\n",
      "2017-08-02T10:36:07.726516: step 880, loss 0.580411, acc 0.75\n",
      "2017-08-02T10:36:08.279571: step 881, loss 0.553364, acc 0.71875\n",
      "2017-08-02T10:36:08.836627: step 882, loss 0.596183, acc 0.71875\n",
      "2017-08-02T10:36:09.398683: step 883, loss 0.594357, acc 0.703125\n",
      "2017-08-02T10:36:09.957739: step 884, loss 0.586413, acc 0.703125\n",
      "2017-08-02T10:36:10.513795: step 885, loss 0.540768, acc 0.703125\n",
      "2017-08-02T10:36:11.073851: step 886, loss 0.682654, acc 0.640625\n",
      "2017-08-02T10:36:11.630906: step 887, loss 0.659686, acc 0.578125\n",
      "2017-08-02T10:36:12.189962: step 888, loss 0.654049, acc 0.65625\n",
      "2017-08-02T10:36:12.748018: step 889, loss 0.701098, acc 0.640625\n",
      "2017-08-02T10:36:13.306074: step 890, loss 0.615845, acc 0.640625\n",
      "2017-08-02T10:36:13.862129: step 891, loss 0.537386, acc 0.78125\n",
      "2017-08-02T10:36:14.431186: step 892, loss 0.600583, acc 0.703125\n",
      "2017-08-02T10:36:15.002243: step 893, loss 0.520068, acc 0.734375\n",
      "2017-08-02T10:36:15.554299: step 894, loss 0.580954, acc 0.671875\n",
      "2017-08-02T10:36:16.111354: step 895, loss 0.61842, acc 0.625\n",
      "2017-08-02T10:36:16.669410: step 896, loss 0.61631, acc 0.734375\n",
      "2017-08-02T10:36:17.230466: step 897, loss 0.602201, acc 0.6875\n",
      "2017-08-02T10:36:17.788522: step 898, loss 0.599609, acc 0.640625\n",
      "2017-08-02T10:36:18.350578: step 899, loss 0.641557, acc 0.703125\n",
      "2017-08-02T10:36:18.917635: step 900, loss 0.535789, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:36:22.602003: step 900, loss 0.55047, acc 0.7548\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-900\n",
      "\n",
      "2017-08-02T10:36:27.261469: step 901, loss 0.565401, acc 0.6875\n",
      "2017-08-02T10:36:27.821525: step 902, loss 0.626947, acc 0.609375\n",
      "2017-08-02T10:36:28.382581: step 903, loss 0.529019, acc 0.734375\n",
      "2017-08-02T10:36:28.935637: step 904, loss 0.665268, acc 0.609375\n",
      "2017-08-02T10:36:29.489692: step 905, loss 0.52222, acc 0.734375\n",
      "2017-08-02T10:36:30.041747: step 906, loss 0.551315, acc 0.71875\n",
      "2017-08-02T10:36:30.596803: step 907, loss 0.613284, acc 0.71875\n",
      "2017-08-02T10:36:31.150858: step 908, loss 0.616379, acc 0.65625\n",
      "2017-08-02T10:36:31.707914: step 909, loss 0.643215, acc 0.671875\n",
      "2017-08-02T10:36:32.267970: step 910, loss 0.611735, acc 0.6875\n",
      "2017-08-02T10:36:32.825025: step 911, loss 0.558603, acc 0.75\n",
      "2017-08-02T10:36:33.383081: step 912, loss 0.560106, acc 0.703125\n",
      "2017-08-02T10:36:33.942137: step 913, loss 0.582961, acc 0.6875\n",
      "2017-08-02T10:36:34.498193: step 914, loss 0.619496, acc 0.640625\n",
      "2017-08-02T10:36:35.055248: step 915, loss 0.519577, acc 0.734375\n",
      "2017-08-02T10:36:35.609304: step 916, loss 0.722064, acc 0.578125\n",
      "2017-08-02T10:36:36.169360: step 917, loss 0.534596, acc 0.8125\n",
      "2017-08-02T10:36:36.724415: step 918, loss 0.536148, acc 0.703125\n",
      "2017-08-02T10:36:37.277471: step 919, loss 0.633682, acc 0.671875\n",
      "2017-08-02T10:36:37.835526: step 920, loss 0.604827, acc 0.625\n",
      "2017-08-02T10:36:38.394582: step 921, loss 0.603149, acc 0.6875\n",
      "2017-08-02T10:36:38.954638: step 922, loss 0.560074, acc 0.6875\n",
      "2017-08-02T10:36:39.512694: step 923, loss 0.560795, acc 0.734375\n",
      "2017-08-02T10:36:40.068750: step 924, loss 0.541916, acc 0.734375\n",
      "2017-08-02T10:36:40.627806: step 925, loss 0.585396, acc 0.703125\n",
      "2017-08-02T10:36:41.191862: step 926, loss 0.540738, acc 0.703125\n",
      "2017-08-02T10:36:41.747918: step 927, loss 0.578181, acc 0.6875\n",
      "2017-08-02T10:36:42.306974: step 928, loss 0.664686, acc 0.6875\n",
      "2017-08-02T10:36:42.868030: step 929, loss 0.542246, acc 0.75\n",
      "2017-08-02T10:36:43.423085: step 930, loss 0.645907, acc 0.625\n",
      "2017-08-02T10:36:43.981141: step 931, loss 0.59284, acc 0.671875\n",
      "2017-08-02T10:36:44.537197: step 932, loss 0.645305, acc 0.75\n",
      "2017-08-02T10:36:45.092252: step 933, loss 0.573986, acc 0.6875\n",
      "2017-08-02T10:36:45.646307: step 934, loss 0.548508, acc 0.734375\n",
      "2017-08-02T10:36:46.203363: step 935, loss 0.57043, acc 0.78125\n",
      "2017-08-02T10:36:46.760419: step 936, loss 0.581392, acc 0.734375\n",
      "2017-08-02T10:36:47.322475: step 937, loss 0.508499, acc 0.796875\n",
      "2017-08-02T10:36:47.881531: step 938, loss 0.631054, acc 0.6875\n",
      "2017-08-02T10:36:48.302573: step 939, loss 0.766573, acc 0.53125\n",
      "2017-08-02T10:36:48.866629: step 940, loss 0.56678, acc 0.734375\n",
      "2017-08-02T10:36:49.428686: step 941, loss 0.536402, acc 0.734375\n",
      "2017-08-02T10:36:49.986741: step 942, loss 0.622399, acc 0.734375\n",
      "2017-08-02T10:36:50.544797: step 943, loss 0.532836, acc 0.71875\n",
      "2017-08-02T10:36:51.104853: step 944, loss 0.738028, acc 0.578125\n",
      "2017-08-02T10:36:51.686911: step 945, loss 0.473727, acc 0.8125\n",
      "2017-08-02T10:36:52.242967: step 946, loss 0.527002, acc 0.796875\n",
      "2017-08-02T10:36:52.802023: step 947, loss 0.528705, acc 0.6875\n",
      "2017-08-02T10:36:53.361079: step 948, loss 0.505786, acc 0.78125\n",
      "2017-08-02T10:36:53.922135: step 949, loss 0.571947, acc 0.6875\n",
      "2017-08-02T10:36:54.484191: step 950, loss 0.616531, acc 0.65625\n",
      "2017-08-02T10:36:55.040247: step 951, loss 0.542817, acc 0.671875\n",
      "2017-08-02T10:36:55.597302: step 952, loss 0.626331, acc 0.609375\n",
      "2017-08-02T10:36:56.153358: step 953, loss 0.570901, acc 0.71875\n",
      "2017-08-02T10:36:56.711414: step 954, loss 0.686014, acc 0.59375\n",
      "2017-08-02T10:36:57.265469: step 955, loss 0.630749, acc 0.65625\n",
      "2017-08-02T10:36:57.820525: step 956, loss 0.561121, acc 0.734375\n",
      "2017-08-02T10:36:58.375580: step 957, loss 0.540252, acc 0.765625\n",
      "2017-08-02T10:36:58.940637: step 958, loss 0.683928, acc 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:36:59.533696: step 959, loss 0.700542, acc 0.671875\n",
      "2017-08-02T10:37:00.129756: step 960, loss 0.528798, acc 0.8125\n",
      "2017-08-02T10:37:00.725815: step 961, loss 0.66108, acc 0.6875\n",
      "2017-08-02T10:37:01.315874: step 962, loss 0.516718, acc 0.78125\n",
      "2017-08-02T10:37:01.899933: step 963, loss 0.506393, acc 0.765625\n",
      "2017-08-02T10:37:02.484991: step 964, loss 0.559605, acc 0.703125\n",
      "2017-08-02T10:37:03.078050: step 965, loss 0.449094, acc 0.859375\n",
      "2017-08-02T10:37:03.672110: step 966, loss 0.643232, acc 0.6875\n",
      "2017-08-02T10:37:04.230166: step 967, loss 0.600904, acc 0.6875\n",
      "2017-08-02T10:37:04.807223: step 968, loss 0.453033, acc 0.796875\n",
      "2017-08-02T10:37:05.369280: step 969, loss 0.581239, acc 0.71875\n",
      "2017-08-02T10:37:05.923335: step 970, loss 0.599413, acc 0.734375\n",
      "2017-08-02T10:37:06.479391: step 971, loss 0.471171, acc 0.8125\n",
      "2017-08-02T10:37:07.044447: step 972, loss 0.515095, acc 0.71875\n",
      "2017-08-02T10:37:07.600503: step 973, loss 0.57216, acc 0.6875\n",
      "2017-08-02T10:37:08.150558: step 974, loss 0.561257, acc 0.703125\n",
      "2017-08-02T10:37:08.709614: step 975, loss 0.547884, acc 0.75\n",
      "2017-08-02T10:37:09.264669: step 976, loss 0.565192, acc 0.703125\n",
      "2017-08-02T10:37:09.822725: step 977, loss 0.561223, acc 0.71875\n",
      "2017-08-02T10:37:10.390782: step 978, loss 0.503144, acc 0.8125\n",
      "2017-08-02T10:37:10.964839: step 979, loss 0.539033, acc 0.75\n",
      "2017-08-02T10:37:11.544897: step 980, loss 0.57355, acc 0.734375\n",
      "2017-08-02T10:37:12.099953: step 981, loss 0.538281, acc 0.734375\n",
      "2017-08-02T10:37:12.659008: step 982, loss 0.63124, acc 0.671875\n",
      "2017-08-02T10:37:13.212064: step 983, loss 0.692597, acc 0.65625\n",
      "2017-08-02T10:37:13.772120: step 984, loss 0.583832, acc 0.640625\n",
      "2017-08-02T10:37:14.330176: step 985, loss 0.545864, acc 0.765625\n",
      "2017-08-02T10:37:14.886231: step 986, loss 0.698694, acc 0.625\n",
      "2017-08-02T10:37:15.458288: step 987, loss 0.650894, acc 0.703125\n",
      "2017-08-02T10:37:16.036346: step 988, loss 0.533469, acc 0.671875\n",
      "2017-08-02T10:37:16.594402: step 989, loss 0.497646, acc 0.796875\n",
      "2017-08-02T10:37:17.151458: step 990, loss 0.63098, acc 0.6875\n",
      "2017-08-02T10:37:17.704513: step 991, loss 0.499223, acc 0.765625\n",
      "2017-08-02T10:37:18.252568: step 992, loss 0.657061, acc 0.671875\n",
      "2017-08-02T10:37:18.813624: step 993, loss 0.511664, acc 0.734375\n",
      "2017-08-02T10:37:19.370680: step 994, loss 0.556611, acc 0.765625\n",
      "2017-08-02T10:37:19.924735: step 995, loss 0.53549, acc 0.71875\n",
      "2017-08-02T10:37:20.479790: step 996, loss 0.612065, acc 0.671875\n",
      "2017-08-02T10:37:21.032846: step 997, loss 0.57153, acc 0.734375\n",
      "2017-08-02T10:37:21.598902: step 998, loss 0.584075, acc 0.6875\n",
      "2017-08-02T10:37:22.155958: step 999, loss 0.570192, acc 0.71875\n",
      "2017-08-02T10:37:22.711014: step 1000, loss 0.523654, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:37:26.359378: step 1000, loss 0.589396, acc 0.696\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1000\n",
      "\n",
      "2017-08-02T10:37:30.159758: step 1001, loss 0.56199, acc 0.71875\n",
      "2017-08-02T10:37:30.713814: step 1002, loss 0.625607, acc 0.75\n",
      "2017-08-02T10:37:31.273870: step 1003, loss 0.619165, acc 0.671875\n",
      "2017-08-02T10:37:31.825925: step 1004, loss 0.763387, acc 0.625\n",
      "2017-08-02T10:37:32.380980: step 1005, loss 0.634849, acc 0.65625\n",
      "2017-08-02T10:37:32.935036: step 1006, loss 0.576681, acc 0.75\n",
      "2017-08-02T10:37:33.493092: step 1007, loss 0.736208, acc 0.625\n",
      "2017-08-02T10:37:34.044147: step 1008, loss 0.712836, acc 0.6875\n",
      "2017-08-02T10:37:34.603203: step 1009, loss 0.651361, acc 0.640625\n",
      "2017-08-02T10:37:35.158258: step 1010, loss 0.541532, acc 0.734375\n",
      "2017-08-02T10:37:35.709313: step 1011, loss 0.600174, acc 0.78125\n",
      "2017-08-02T10:37:36.265369: step 1012, loss 0.559665, acc 0.75\n",
      "2017-08-02T10:37:36.824425: step 1013, loss 0.64805, acc 0.65625\n",
      "2017-08-02T10:37:37.379480: step 1014, loss 0.804923, acc 0.578125\n",
      "2017-08-02T10:37:37.937536: step 1015, loss 0.571825, acc 0.703125\n",
      "2017-08-02T10:37:38.495592: step 1016, loss 0.656551, acc 0.625\n",
      "2017-08-02T10:37:39.053648: step 1017, loss 0.527232, acc 0.765625\n",
      "2017-08-02T10:37:39.610703: step 1018, loss 0.51401, acc 0.734375\n",
      "2017-08-02T10:37:40.168759: step 1019, loss 0.474977, acc 0.765625\n",
      "2017-08-02T10:37:40.727815: step 1020, loss 0.480705, acc 0.765625\n",
      "2017-08-02T10:37:41.287871: step 1021, loss 0.56093, acc 0.765625\n",
      "2017-08-02T10:37:41.840926: step 1022, loss 0.601515, acc 0.65625\n",
      "2017-08-02T10:37:42.396982: step 1023, loss 0.498073, acc 0.890625\n",
      "2017-08-02T10:37:42.951037: step 1024, loss 0.473834, acc 0.78125\n",
      "2017-08-02T10:37:43.509093: step 1025, loss 0.529814, acc 0.875\n",
      "2017-08-02T10:37:44.065149: step 1026, loss 0.53176, acc 0.796875\n",
      "2017-08-02T10:37:44.634206: step 1027, loss 0.548892, acc 0.734375\n",
      "2017-08-02T10:37:45.191261: step 1028, loss 0.551459, acc 0.765625\n",
      "2017-08-02T10:37:45.744317: step 1029, loss 0.459517, acc 0.765625\n",
      "2017-08-02T10:37:46.309373: step 1030, loss 0.547272, acc 0.765625\n",
      "2017-08-02T10:37:46.864429: step 1031, loss 0.527279, acc 0.71875\n",
      "2017-08-02T10:37:47.422484: step 1032, loss 0.483399, acc 0.75\n",
      "2017-08-02T10:37:47.976540: step 1033, loss 0.55489, acc 0.71875\n",
      "2017-08-02T10:37:48.532595: step 1034, loss 0.401351, acc 0.859375\n",
      "2017-08-02T10:37:49.092651: step 1035, loss 0.671717, acc 0.671875\n",
      "2017-08-02T10:37:49.652707: step 1036, loss 0.607111, acc 0.671875\n",
      "2017-08-02T10:37:50.211763: step 1037, loss 0.466572, acc 0.78125\n",
      "2017-08-02T10:37:50.767819: step 1038, loss 0.513738, acc 0.75\n",
      "2017-08-02T10:37:51.318874: step 1039, loss 0.59648, acc 0.703125\n",
      "2017-08-02T10:37:51.875930: step 1040, loss 0.507074, acc 0.765625\n",
      "2017-08-02T10:37:52.430985: step 1041, loss 0.591596, acc 0.71875\n",
      "2017-08-02T10:37:52.986041: step 1042, loss 0.616356, acc 0.640625\n",
      "2017-08-02T10:37:53.541096: step 1043, loss 0.524288, acc 0.734375\n",
      "2017-08-02T10:37:54.095152: step 1044, loss 0.577989, acc 0.734375\n",
      "2017-08-02T10:37:54.654208: step 1045, loss 0.477193, acc 0.75\n",
      "2017-08-02T10:37:55.211263: step 1046, loss 0.605639, acc 0.71875\n",
      "2017-08-02T10:37:55.767319: step 1047, loss 0.477543, acc 0.796875\n",
      "2017-08-02T10:37:56.324375: step 1048, loss 0.728824, acc 0.65625\n",
      "2017-08-02T10:37:56.878430: step 1049, loss 0.586795, acc 0.671875\n",
      "2017-08-02T10:37:57.458488: step 1050, loss 0.473031, acc 0.796875\n",
      "2017-08-02T10:37:58.013543: step 1051, loss 0.584186, acc 0.65625\n",
      "2017-08-02T10:37:58.569599: step 1052, loss 0.542658, acc 0.75\n",
      "2017-08-02T10:37:59.131655: step 1053, loss 0.44864, acc 0.8125\n",
      "2017-08-02T10:37:59.691711: step 1054, loss 0.591513, acc 0.6875\n",
      "2017-08-02T10:38:00.250767: step 1055, loss 0.539649, acc 0.71875\n",
      "2017-08-02T10:38:00.803822: step 1056, loss 0.452501, acc 0.8125\n",
      "2017-08-02T10:38:01.356878: step 1057, loss 0.513652, acc 0.8125\n",
      "2017-08-02T10:38:01.915934: step 1058, loss 0.512608, acc 0.765625\n",
      "2017-08-02T10:38:02.474990: step 1059, loss 0.525615, acc 0.765625\n",
      "2017-08-02T10:38:03.034045: step 1060, loss 0.62223, acc 0.71875\n",
      "2017-08-02T10:38:03.590101: step 1061, loss 0.508768, acc 0.796875\n",
      "2017-08-02T10:38:04.144156: step 1062, loss 0.567396, acc 0.703125\n",
      "2017-08-02T10:38:04.701212: step 1063, loss 0.520606, acc 0.765625\n",
      "2017-08-02T10:38:05.255268: step 1064, loss 0.628748, acc 0.703125\n",
      "2017-08-02T10:38:05.809323: step 1065, loss 0.527607, acc 0.75\n",
      "2017-08-02T10:38:06.370379: step 1066, loss 0.552119, acc 0.75\n",
      "2017-08-02T10:38:06.926435: step 1067, loss 0.62925, acc 0.71875\n",
      "2017-08-02T10:38:07.486491: step 1068, loss 0.617772, acc 0.6875\n",
      "2017-08-02T10:38:08.042546: step 1069, loss 0.482345, acc 0.765625\n",
      "2017-08-02T10:38:08.599602: step 1070, loss 0.540727, acc 0.703125\n",
      "2017-08-02T10:38:09.156658: step 1071, loss 0.590264, acc 0.703125\n",
      "2017-08-02T10:38:09.729715: step 1072, loss 0.539543, acc 0.734375\n",
      "2017-08-02T10:38:10.285771: step 1073, loss 0.638343, acc 0.703125\n",
      "2017-08-02T10:38:10.847827: step 1074, loss 0.561084, acc 0.6875\n",
      "2017-08-02T10:38:11.406883: step 1075, loss 0.513917, acc 0.75\n",
      "2017-08-02T10:38:11.962938: step 1076, loss 0.582001, acc 0.75\n",
      "2017-08-02T10:38:12.523994: step 1077, loss 0.487973, acc 0.75\n",
      "2017-08-02T10:38:13.077050: step 1078, loss 0.586059, acc 0.71875\n",
      "2017-08-02T10:38:13.633105: step 1079, loss 0.473421, acc 0.78125\n",
      "2017-08-02T10:38:14.193161: step 1080, loss 0.592546, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:38:14.753217: step 1081, loss 0.571066, acc 0.65625\n",
      "2017-08-02T10:38:15.306273: step 1082, loss 0.627789, acc 0.671875\n",
      "2017-08-02T10:38:15.860328: step 1083, loss 0.535839, acc 0.75\n",
      "2017-08-02T10:38:16.441386: step 1084, loss 0.580736, acc 0.65625\n",
      "2017-08-02T10:38:17.014443: step 1085, loss 0.502739, acc 0.71875\n",
      "2017-08-02T10:38:17.575499: step 1086, loss 0.622934, acc 0.609375\n",
      "2017-08-02T10:38:18.126555: step 1087, loss 0.467261, acc 0.828125\n",
      "2017-08-02T10:38:18.681610: step 1088, loss 0.543699, acc 0.796875\n",
      "2017-08-02T10:38:19.236666: step 1089, loss 0.459962, acc 0.796875\n",
      "2017-08-02T10:38:19.795721: step 1090, loss 0.536531, acc 0.703125\n",
      "2017-08-02T10:38:20.351777: step 1091, loss 0.537843, acc 0.671875\n",
      "2017-08-02T10:38:20.908833: step 1092, loss 0.567945, acc 0.671875\n",
      "2017-08-02T10:38:21.465888: step 1093, loss 0.580662, acc 0.6875\n",
      "2017-08-02T10:38:22.023944: step 1094, loss 0.514034, acc 0.75\n",
      "2017-08-02T10:38:22.587001: step 1095, loss 0.576566, acc 0.6875\n",
      "2017-08-02T10:38:23.163058: step 1096, loss 0.543275, acc 0.71875\n",
      "2017-08-02T10:38:23.719114: step 1097, loss 0.605429, acc 0.671875\n",
      "2017-08-02T10:38:24.279170: step 1098, loss 0.482154, acc 0.796875\n",
      "2017-08-02T10:38:24.842226: step 1099, loss 0.609989, acc 0.6875\n",
      "2017-08-02T10:38:25.400282: step 1100, loss 0.559104, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:38:29.048647: step 1100, loss 0.541926, acc 0.7464\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1100\n",
      "\n",
      "2017-08-02T10:38:33.136055: step 1101, loss 0.517511, acc 0.734375\n",
      "2017-08-02T10:38:33.688111: step 1102, loss 0.612826, acc 0.671875\n",
      "2017-08-02T10:38:34.243166: step 1103, loss 0.652763, acc 0.65625\n",
      "2017-08-02T10:38:34.802222: step 1104, loss 0.651247, acc 0.703125\n",
      "2017-08-02T10:38:35.360278: step 1105, loss 0.424109, acc 0.828125\n",
      "2017-08-02T10:38:35.915333: step 1106, loss 0.603301, acc 0.75\n",
      "2017-08-02T10:38:36.493391: step 1107, loss 0.539581, acc 0.765625\n",
      "2017-08-02T10:38:37.045446: step 1108, loss 0.656808, acc 0.6875\n",
      "2017-08-02T10:38:37.603502: step 1109, loss 0.55988, acc 0.734375\n",
      "2017-08-02T10:38:38.158558: step 1110, loss 0.532368, acc 0.75\n",
      "2017-08-02T10:38:38.708613: step 1111, loss 0.685226, acc 0.578125\n",
      "2017-08-02T10:38:39.265668: step 1112, loss 0.614016, acc 0.640625\n",
      "2017-08-02T10:38:39.830725: step 1113, loss 0.586363, acc 0.640625\n",
      "2017-08-02T10:38:40.390781: step 1114, loss 0.693542, acc 0.640625\n",
      "2017-08-02T10:38:40.940836: step 1115, loss 0.625751, acc 0.734375\n",
      "2017-08-02T10:38:41.493891: step 1116, loss 0.611462, acc 0.640625\n",
      "2017-08-02T10:38:42.048947: step 1117, loss 0.506282, acc 0.75\n",
      "2017-08-02T10:38:42.602002: step 1118, loss 0.429688, acc 0.84375\n",
      "2017-08-02T10:38:43.158057: step 1119, loss 0.655609, acc 0.703125\n",
      "2017-08-02T10:38:43.712113: step 1120, loss 0.586889, acc 0.703125\n",
      "2017-08-02T10:38:44.266168: step 1121, loss 0.475886, acc 0.828125\n",
      "2017-08-02T10:38:44.823224: step 1122, loss 0.536937, acc 0.75\n",
      "2017-08-02T10:38:45.401282: step 1123, loss 0.622205, acc 0.703125\n",
      "2017-08-02T10:38:45.953337: step 1124, loss 0.553467, acc 0.71875\n",
      "2017-08-02T10:38:46.533395: step 1125, loss 0.538169, acc 0.71875\n",
      "2017-08-02T10:38:47.089451: step 1126, loss 0.495713, acc 0.734375\n",
      "2017-08-02T10:38:47.647506: step 1127, loss 0.57949, acc 0.71875\n",
      "2017-08-02T10:38:48.204562: step 1128, loss 0.570547, acc 0.703125\n",
      "2017-08-02T10:38:48.762618: step 1129, loss 0.584066, acc 0.75\n",
      "2017-08-02T10:38:49.312673: step 1130, loss 0.54524, acc 0.78125\n",
      "2017-08-02T10:38:49.867728: step 1131, loss 0.579026, acc 0.71875\n",
      "2017-08-02T10:38:50.427784: step 1132, loss 0.62017, acc 0.703125\n",
      "2017-08-02T10:38:50.982840: step 1133, loss 0.589322, acc 0.65625\n",
      "2017-08-02T10:38:51.540896: step 1134, loss 0.466573, acc 0.8125\n",
      "2017-08-02T10:38:52.094951: step 1135, loss 0.466877, acc 0.796875\n",
      "2017-08-02T10:38:52.650007: step 1136, loss 0.433755, acc 0.765625\n",
      "2017-08-02T10:38:53.203062: step 1137, loss 0.608056, acc 0.703125\n",
      "2017-08-02T10:38:53.759117: step 1138, loss 0.450394, acc 0.8125\n",
      "2017-08-02T10:38:54.324174: step 1139, loss 0.556134, acc 0.703125\n",
      "2017-08-02T10:38:54.887230: step 1140, loss 0.455057, acc 0.828125\n",
      "2017-08-02T10:38:55.450287: step 1141, loss 0.504166, acc 0.796875\n",
      "2017-08-02T10:38:56.010343: step 1142, loss 0.538998, acc 0.703125\n",
      "2017-08-02T10:38:56.571399: step 1143, loss 0.57804, acc 0.734375\n",
      "2017-08-02T10:38:57.128454: step 1144, loss 0.58399, acc 0.765625\n",
      "2017-08-02T10:38:57.683510: step 1145, loss 0.53269, acc 0.75\n",
      "2017-08-02T10:38:58.241566: step 1146, loss 0.534476, acc 0.765625\n",
      "2017-08-02T10:38:58.802622: step 1147, loss 0.540694, acc 0.75\n",
      "2017-08-02T10:38:59.353677: step 1148, loss 0.480936, acc 0.75\n",
      "2017-08-02T10:38:59.916733: step 1149, loss 0.618758, acc 0.734375\n",
      "2017-08-02T10:39:00.476789: step 1150, loss 0.594216, acc 0.703125\n",
      "2017-08-02T10:39:01.036845: step 1151, loss 0.619888, acc 0.671875\n",
      "2017-08-02T10:39:01.595901: step 1152, loss 0.648523, acc 0.625\n",
      "2017-08-02T10:39:02.152957: step 1153, loss 0.471923, acc 0.78125\n",
      "2017-08-02T10:39:02.706012: step 1154, loss 0.532006, acc 0.71875\n",
      "2017-08-02T10:39:03.268068: step 1155, loss 0.494754, acc 0.75\n",
      "2017-08-02T10:39:03.822124: step 1156, loss 0.560828, acc 0.734375\n",
      "2017-08-02T10:39:04.382180: step 1157, loss 0.600185, acc 0.6875\n",
      "2017-08-02T10:39:04.946236: step 1158, loss 0.593367, acc 0.734375\n",
      "2017-08-02T10:39:05.505292: step 1159, loss 0.464806, acc 0.765625\n",
      "2017-08-02T10:39:06.063348: step 1160, loss 0.47049, acc 0.78125\n",
      "2017-08-02T10:39:06.627404: step 1161, loss 0.508438, acc 0.71875\n",
      "2017-08-02T10:39:07.187460: step 1162, loss 0.448633, acc 0.84375\n",
      "2017-08-02T10:39:07.740515: step 1163, loss 0.526133, acc 0.703125\n",
      "2017-08-02T10:39:08.315573: step 1164, loss 0.500577, acc 0.84375\n",
      "2017-08-02T10:39:08.870628: step 1165, loss 0.506576, acc 0.75\n",
      "2017-08-02T10:39:09.425684: step 1166, loss 0.406804, acc 0.859375\n",
      "2017-08-02T10:39:09.982740: step 1167, loss 0.592387, acc 0.671875\n",
      "2017-08-02T10:39:10.539795: step 1168, loss 0.604274, acc 0.671875\n",
      "2017-08-02T10:39:11.090850: step 1169, loss 0.570763, acc 0.75\n",
      "2017-08-02T10:39:11.653907: step 1170, loss 0.633692, acc 0.640625\n",
      "2017-08-02T10:39:12.212963: step 1171, loss 0.47972, acc 0.765625\n",
      "2017-08-02T10:39:12.773019: step 1172, loss 0.478199, acc 0.765625\n",
      "2017-08-02T10:39:13.335075: step 1173, loss 0.59228, acc 0.6875\n",
      "2017-08-02T10:39:13.891130: step 1174, loss 0.555443, acc 0.71875\n",
      "2017-08-02T10:39:14.453187: step 1175, loss 0.52595, acc 0.78125\n",
      "2017-08-02T10:39:15.011242: step 1176, loss 0.444614, acc 0.8125\n",
      "2017-08-02T10:39:15.566298: step 1177, loss 0.613692, acc 0.671875\n",
      "2017-08-02T10:39:16.122354: step 1178, loss 0.549201, acc 0.75\n",
      "2017-08-02T10:39:16.697411: step 1179, loss 0.689541, acc 0.625\n",
      "2017-08-02T10:39:17.266468: step 1180, loss 0.645783, acc 0.671875\n",
      "2017-08-02T10:39:17.827524: step 1181, loss 0.533214, acc 0.78125\n",
      "2017-08-02T10:39:18.385580: step 1182, loss 0.580508, acc 0.703125\n",
      "2017-08-02T10:39:18.939635: step 1183, loss 0.527342, acc 0.734375\n",
      "2017-08-02T10:39:19.495691: step 1184, loss 0.541874, acc 0.765625\n",
      "2017-08-02T10:39:20.067748: step 1185, loss 0.708232, acc 0.546875\n",
      "2017-08-02T10:39:20.626804: step 1186, loss 0.65015, acc 0.671875\n",
      "2017-08-02T10:39:21.180859: step 1187, loss 0.489059, acc 0.8125\n",
      "2017-08-02T10:39:21.738915: step 1188, loss 0.504353, acc 0.734375\n",
      "2017-08-02T10:39:22.291970: step 1189, loss 0.581632, acc 0.734375\n",
      "2017-08-02T10:39:22.847026: step 1190, loss 0.555925, acc 0.78125\n",
      "2017-08-02T10:39:23.402081: step 1191, loss 0.529978, acc 0.71875\n",
      "2017-08-02T10:39:23.960137: step 1192, loss 0.577395, acc 0.734375\n",
      "2017-08-02T10:39:24.511192: step 1193, loss 0.522347, acc 0.734375\n",
      "2017-08-02T10:39:25.064248: step 1194, loss 0.62249, acc 0.71875\n",
      "2017-08-02T10:39:25.621303: step 1195, loss 0.634431, acc 0.734375\n",
      "2017-08-02T10:39:26.177359: step 1196, loss 0.504233, acc 0.71875\n",
      "2017-08-02T10:39:26.738415: step 1197, loss 0.591445, acc 0.671875\n",
      "2017-08-02T10:39:27.300471: step 1198, loss 0.515178, acc 0.75\n",
      "2017-08-02T10:39:27.855527: step 1199, loss 0.562067, acc 0.71875\n",
      "2017-08-02T10:39:28.412582: step 1200, loss 0.559702, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:39:32.058947: step 1200, loss 0.508776, acc 0.786\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1200\n",
      "\n",
      "2017-08-02T10:39:35.910332: step 1201, loss 0.564797, acc 0.6875\n",
      "2017-08-02T10:39:36.465388: step 1202, loss 0.583718, acc 0.703125\n",
      "2017-08-02T10:39:37.018443: step 1203, loss 0.568483, acc 0.703125\n",
      "2017-08-02T10:39:37.572498: step 1204, loss 0.46282, acc 0.796875\n",
      "2017-08-02T10:39:38.126554: step 1205, loss 0.49119, acc 0.75\n",
      "2017-08-02T10:39:38.677609: step 1206, loss 0.579507, acc 0.71875\n",
      "2017-08-02T10:39:39.235665: step 1207, loss 0.481408, acc 0.796875\n",
      "2017-08-02T10:39:39.792720: step 1208, loss 0.497507, acc 0.765625\n",
      "2017-08-02T10:39:40.352776: step 1209, loss 0.56818, acc 0.75\n",
      "2017-08-02T10:39:40.910832: step 1210, loss 0.510463, acc 0.78125\n",
      "2017-08-02T10:39:41.473888: step 1211, loss 0.447792, acc 0.796875\n",
      "2017-08-02T10:39:42.030944: step 1212, loss 0.447044, acc 0.765625\n",
      "2017-08-02T10:39:42.581999: step 1213, loss 0.516508, acc 0.71875\n",
      "2017-08-02T10:39:43.137055: step 1214, loss 0.602449, acc 0.640625\n",
      "2017-08-02T10:39:43.694110: step 1215, loss 0.537999, acc 0.703125\n",
      "2017-08-02T10:39:44.247166: step 1216, loss 0.517916, acc 0.8125\n",
      "2017-08-02T10:39:44.806222: step 1217, loss 0.456717, acc 0.796875\n",
      "2017-08-02T10:39:45.361277: step 1218, loss 0.594251, acc 0.65625\n",
      "2017-08-02T10:39:45.916333: step 1219, loss 0.484156, acc 0.84375\n",
      "2017-08-02T10:39:46.471388: step 1220, loss 0.48239, acc 0.765625\n",
      "2017-08-02T10:39:47.028444: step 1221, loss 0.567275, acc 0.703125\n",
      "2017-08-02T10:39:47.584499: step 1222, loss 0.541577, acc 0.765625\n",
      "2017-08-02T10:39:48.142555: step 1223, loss 0.642901, acc 0.65625\n",
      "2017-08-02T10:39:48.697611: step 1224, loss 0.573807, acc 0.703125\n",
      "2017-08-02T10:39:49.255667: step 1225, loss 0.533766, acc 0.734375\n",
      "2017-08-02T10:39:49.812722: step 1226, loss 0.599817, acc 0.65625\n",
      "2017-08-02T10:39:50.378779: step 1227, loss 0.537502, acc 0.75\n",
      "2017-08-02T10:39:50.935835: step 1228, loss 0.692381, acc 0.625\n",
      "2017-08-02T10:39:51.490890: step 1229, loss 0.474337, acc 0.765625\n",
      "2017-08-02T10:39:52.043945: step 1230, loss 0.549627, acc 0.671875\n",
      "2017-08-02T10:39:52.598001: step 1231, loss 0.481864, acc 0.796875\n",
      "2017-08-02T10:39:53.155056: step 1232, loss 0.656143, acc 0.578125\n",
      "2017-08-02T10:39:53.712112: step 1233, loss 0.4007, acc 0.890625\n",
      "2017-08-02T10:39:54.263167: step 1234, loss 0.623896, acc 0.6875\n",
      "2017-08-02T10:39:54.821223: step 1235, loss 0.510724, acc 0.75\n",
      "2017-08-02T10:39:55.372278: step 1236, loss 0.440729, acc 0.859375\n",
      "2017-08-02T10:39:55.936335: step 1237, loss 0.575361, acc 0.71875\n",
      "2017-08-02T10:39:56.498391: step 1238, loss 0.499331, acc 0.75\n",
      "2017-08-02T10:39:57.060447: step 1239, loss 0.446109, acc 0.875\n",
      "2017-08-02T10:39:57.629504: step 1240, loss 0.455806, acc 0.78125\n",
      "2017-08-02T10:39:58.189560: step 1241, loss 0.528692, acc 0.765625\n",
      "2017-08-02T10:39:58.754616: step 1242, loss 0.464096, acc 0.734375\n",
      "2017-08-02T10:39:59.321673: step 1243, loss 0.582145, acc 0.671875\n",
      "2017-08-02T10:39:59.922733: step 1244, loss 0.594273, acc 0.703125\n",
      "2017-08-02T10:40:00.512792: step 1245, loss 0.494264, acc 0.828125\n",
      "2017-08-02T10:40:01.104851: step 1246, loss 0.570432, acc 0.671875\n",
      "2017-08-02T10:40:01.685909: step 1247, loss 0.511328, acc 0.75\n",
      "2017-08-02T10:40:02.271968: step 1248, loss 0.560091, acc 0.734375\n",
      "2017-08-02T10:40:02.855026: step 1249, loss 0.535441, acc 0.71875\n",
      "2017-08-02T10:40:03.440085: step 1250, loss 0.593948, acc 0.6875\n",
      "2017-08-02T10:40:04.036144: step 1251, loss 0.519015, acc 0.734375\n",
      "2017-08-02T10:40:04.450186: step 1252, loss 0.453146, acc 0.84375\n",
      "2017-08-02T10:40:05.009242: step 1253, loss 0.503213, acc 0.734375\n",
      "2017-08-02T10:40:05.568298: step 1254, loss 0.554354, acc 0.734375\n",
      "2017-08-02T10:40:06.151356: step 1255, loss 0.591354, acc 0.765625\n",
      "2017-08-02T10:40:06.735414: step 1256, loss 0.446025, acc 0.8125\n",
      "2017-08-02T10:40:07.312472: step 1257, loss 0.418106, acc 0.828125\n",
      "2017-08-02T10:40:07.871528: step 1258, loss 0.542902, acc 0.734375\n",
      "2017-08-02T10:40:08.432584: step 1259, loss 0.567375, acc 0.734375\n",
      "2017-08-02T10:40:09.018643: step 1260, loss 0.638955, acc 0.71875\n",
      "2017-08-02T10:40:09.585699: step 1261, loss 0.584951, acc 0.765625\n",
      "2017-08-02T10:40:10.146755: step 1262, loss 0.481719, acc 0.796875\n",
      "2017-08-02T10:40:10.705811: step 1263, loss 0.544565, acc 0.796875\n",
      "2017-08-02T10:40:11.263867: step 1264, loss 0.555937, acc 0.6875\n",
      "2017-08-02T10:40:11.821923: step 1265, loss 0.551633, acc 0.75\n",
      "2017-08-02T10:40:12.373978: step 1266, loss 0.502162, acc 0.78125\n",
      "2017-08-02T10:40:12.958037: step 1267, loss 0.392671, acc 0.90625\n",
      "2017-08-02T10:40:13.516092: step 1268, loss 0.525787, acc 0.765625\n",
      "2017-08-02T10:40:14.075148: step 1269, loss 0.502712, acc 0.765625\n",
      "2017-08-02T10:40:14.637204: step 1270, loss 0.378145, acc 0.84375\n",
      "2017-08-02T10:40:15.200261: step 1271, loss 0.481083, acc 0.765625\n",
      "2017-08-02T10:40:15.762317: step 1272, loss 0.514012, acc 0.78125\n",
      "2017-08-02T10:40:16.320373: step 1273, loss 0.41412, acc 0.84375\n",
      "2017-08-02T10:40:16.884429: step 1274, loss 0.50366, acc 0.765625\n",
      "2017-08-02T10:40:17.461487: step 1275, loss 0.486389, acc 0.75\n",
      "2017-08-02T10:40:18.014542: step 1276, loss 0.605276, acc 0.71875\n",
      "2017-08-02T10:40:18.579599: step 1277, loss 0.599795, acc 0.625\n",
      "2017-08-02T10:40:19.147655: step 1278, loss 0.498736, acc 0.75\n",
      "2017-08-02T10:40:19.712712: step 1279, loss 0.480477, acc 0.796875\n",
      "2017-08-02T10:40:20.277768: step 1280, loss 0.478805, acc 0.75\n",
      "2017-08-02T10:40:20.845825: step 1281, loss 0.608941, acc 0.71875\n",
      "2017-08-02T10:40:21.411882: step 1282, loss 0.588373, acc 0.71875\n",
      "2017-08-02T10:40:21.978939: step 1283, loss 0.536955, acc 0.765625\n",
      "2017-08-02T10:40:22.554996: step 1284, loss 0.589227, acc 0.71875\n",
      "2017-08-02T10:40:23.122053: step 1285, loss 0.628479, acc 0.75\n",
      "2017-08-02T10:40:23.691110: step 1286, loss 0.52222, acc 0.796875\n",
      "2017-08-02T10:40:24.266167: step 1287, loss 0.537753, acc 0.734375\n",
      "2017-08-02T10:40:24.829224: step 1288, loss 0.608776, acc 0.671875\n",
      "2017-08-02T10:40:25.397280: step 1289, loss 0.427049, acc 0.84375\n",
      "2017-08-02T10:40:25.962337: step 1290, loss 0.53828, acc 0.703125\n",
      "2017-08-02T10:40:26.534394: step 1291, loss 0.582897, acc 0.734375\n",
      "2017-08-02T10:40:27.104451: step 1292, loss 0.491153, acc 0.78125\n",
      "2017-08-02T10:40:27.670508: step 1293, loss 0.472923, acc 0.796875\n",
      "2017-08-02T10:40:28.236564: step 1294, loss 0.470686, acc 0.828125\n",
      "2017-08-02T10:40:28.800621: step 1295, loss 0.529487, acc 0.703125\n",
      "2017-08-02T10:40:29.373678: step 1296, loss 0.588462, acc 0.734375\n",
      "2017-08-02T10:40:29.927733: step 1297, loss 0.595531, acc 0.71875\n",
      "2017-08-02T10:40:30.479789: step 1298, loss 0.507114, acc 0.71875\n",
      "2017-08-02T10:40:31.033844: step 1299, loss 0.428808, acc 0.796875\n",
      "2017-08-02T10:40:31.593900: step 1300, loss 0.602845, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:40:35.260267: step 1300, loss 0.612487, acc 0.6648\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1300\n",
      "\n",
      "2017-08-02T10:40:37.616502: step 1301, loss 0.533326, acc 0.71875\n",
      "2017-08-02T10:40:38.190560: step 1302, loss 0.4938, acc 0.71875\n",
      "2017-08-02T10:40:38.778618: step 1303, loss 0.480092, acc 0.8125\n",
      "2017-08-02T10:40:39.368677: step 1304, loss 0.460499, acc 0.734375\n",
      "2017-08-02T10:40:39.967737: step 1305, loss 0.554178, acc 0.71875\n",
      "2017-08-02T10:40:40.550796: step 1306, loss 0.790613, acc 0.59375\n",
      "2017-08-02T10:40:41.141855: step 1307, loss 0.489445, acc 0.703125\n",
      "2017-08-02T10:40:41.726913: step 1308, loss 0.462847, acc 0.828125\n",
      "2017-08-02T10:40:42.316972: step 1309, loss 0.588432, acc 0.703125\n",
      "2017-08-02T10:40:42.904031: step 1310, loss 0.58994, acc 0.703125\n",
      "2017-08-02T10:40:43.494090: step 1311, loss 0.69455, acc 0.59375\n",
      "2017-08-02T10:40:44.065147: step 1312, loss 0.374293, acc 0.84375\n",
      "2017-08-02T10:40:44.626203: step 1313, loss 0.47507, acc 0.734375\n",
      "2017-08-02T10:40:45.194260: step 1314, loss 0.492102, acc 0.734375\n",
      "2017-08-02T10:40:45.762317: step 1315, loss 0.507292, acc 0.78125\n",
      "2017-08-02T10:40:46.329373: step 1316, loss 0.813695, acc 0.640625\n",
      "2017-08-02T10:40:46.904431: step 1317, loss 0.643646, acc 0.6875\n",
      "2017-08-02T10:40:47.460486: step 1318, loss 0.471942, acc 0.765625\n",
      "2017-08-02T10:40:48.014542: step 1319, loss 0.6117, acc 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:40:48.572598: step 1320, loss 0.642304, acc 0.75\n",
      "2017-08-02T10:40:49.137654: step 1321, loss 0.451015, acc 0.796875\n",
      "2017-08-02T10:40:49.694710: step 1322, loss 0.501504, acc 0.765625\n",
      "2017-08-02T10:40:50.271768: step 1323, loss 0.507744, acc 0.796875\n",
      "2017-08-02T10:40:50.827823: step 1324, loss 0.404811, acc 0.796875\n",
      "2017-08-02T10:40:51.379878: step 1325, loss 0.647895, acc 0.734375\n",
      "2017-08-02T10:40:51.933934: step 1326, loss 0.6004, acc 0.71875\n",
      "2017-08-02T10:40:52.492990: step 1327, loss 0.492425, acc 0.75\n",
      "2017-08-02T10:40:53.050045: step 1328, loss 0.514802, acc 0.75\n",
      "2017-08-02T10:40:53.629103: step 1329, loss 0.527428, acc 0.71875\n",
      "2017-08-02T10:40:54.182159: step 1330, loss 0.439359, acc 0.8125\n",
      "2017-08-02T10:40:54.734214: step 1331, loss 0.49456, acc 0.765625\n",
      "2017-08-02T10:40:55.311271: step 1332, loss 0.560785, acc 0.734375\n",
      "2017-08-02T10:40:55.876328: step 1333, loss 0.779118, acc 0.640625\n",
      "2017-08-02T10:40:56.439384: step 1334, loss 0.445837, acc 0.828125\n",
      "2017-08-02T10:40:56.999440: step 1335, loss 0.52184, acc 0.75\n",
      "2017-08-02T10:40:57.552496: step 1336, loss 0.601355, acc 0.671875\n",
      "2017-08-02T10:40:58.137554: step 1337, loss 0.547875, acc 0.71875\n",
      "2017-08-02T10:40:58.694610: step 1338, loss 0.526189, acc 0.703125\n",
      "2017-08-02T10:40:59.251665: step 1339, loss 0.647107, acc 0.703125\n",
      "2017-08-02T10:40:59.804721: step 1340, loss 0.5147, acc 0.796875\n",
      "2017-08-02T10:41:00.383779: step 1341, loss 0.577861, acc 0.71875\n",
      "2017-08-02T10:41:00.936834: step 1342, loss 0.67505, acc 0.71875\n",
      "2017-08-02T10:41:01.496890: step 1343, loss 0.529437, acc 0.75\n",
      "2017-08-02T10:41:02.053946: step 1344, loss 0.60723, acc 0.703125\n",
      "2017-08-02T10:41:02.607001: step 1345, loss 0.534651, acc 0.796875\n",
      "2017-08-02T10:41:03.163057: step 1346, loss 0.4613, acc 0.859375\n",
      "2017-08-02T10:41:03.710111: step 1347, loss 0.425288, acc 0.796875\n",
      "2017-08-02T10:41:04.265167: step 1348, loss 0.558867, acc 0.71875\n",
      "2017-08-02T10:41:04.835224: step 1349, loss 0.346088, acc 0.796875\n",
      "2017-08-02T10:41:05.426283: step 1350, loss 0.584562, acc 0.6875\n",
      "2017-08-02T10:41:05.983339: step 1351, loss 0.521268, acc 0.734375\n",
      "2017-08-02T10:41:06.537394: step 1352, loss 0.493721, acc 0.75\n",
      "2017-08-02T10:41:07.104451: step 1353, loss 0.561367, acc 0.71875\n",
      "2017-08-02T10:41:07.664507: step 1354, loss 0.545746, acc 0.765625\n",
      "2017-08-02T10:41:08.222562: step 1355, loss 0.572302, acc 0.71875\n",
      "2017-08-02T10:41:08.797620: step 1356, loss 0.586498, acc 0.703125\n",
      "2017-08-02T10:41:09.350675: step 1357, loss 0.616284, acc 0.6875\n",
      "2017-08-02T10:41:09.928733: step 1358, loss 0.515608, acc 0.75\n",
      "2017-08-02T10:41:10.482788: step 1359, loss 0.600234, acc 0.6875\n",
      "2017-08-02T10:41:11.040844: step 1360, loss 0.616706, acc 0.734375\n",
      "2017-08-02T10:41:11.592899: step 1361, loss 0.683591, acc 0.59375\n",
      "2017-08-02T10:41:12.145955: step 1362, loss 0.488674, acc 0.8125\n",
      "2017-08-02T10:41:12.701010: step 1363, loss 0.5265, acc 0.75\n",
      "2017-08-02T10:41:13.258066: step 1364, loss 0.464196, acc 0.796875\n",
      "2017-08-02T10:41:13.812121: step 1365, loss 0.576739, acc 0.71875\n",
      "2017-08-02T10:41:14.371177: step 1366, loss 0.515684, acc 0.796875\n",
      "2017-08-02T10:41:14.929233: step 1367, loss 0.414929, acc 0.796875\n",
      "2017-08-02T10:41:15.489289: step 1368, loss 0.4796, acc 0.703125\n",
      "2017-08-02T10:41:16.046345: step 1369, loss 0.622535, acc 0.671875\n",
      "2017-08-02T10:41:16.600400: step 1370, loss 0.604181, acc 0.71875\n",
      "2017-08-02T10:41:17.154456: step 1371, loss 0.427152, acc 0.859375\n",
      "2017-08-02T10:41:17.730513: step 1372, loss 0.531103, acc 0.78125\n",
      "2017-08-02T10:41:18.306571: step 1373, loss 0.503452, acc 0.75\n",
      "2017-08-02T10:41:18.861626: step 1374, loss 0.428595, acc 0.828125\n",
      "2017-08-02T10:41:19.421682: step 1375, loss 0.427385, acc 0.796875\n",
      "2017-08-02T10:41:19.983738: step 1376, loss 0.499011, acc 0.78125\n",
      "2017-08-02T10:41:20.537794: step 1377, loss 0.580038, acc 0.75\n",
      "2017-08-02T10:41:21.094850: step 1378, loss 0.493274, acc 0.71875\n",
      "2017-08-02T10:41:21.657906: step 1379, loss 0.518369, acc 0.734375\n",
      "2017-08-02T10:41:22.212961: step 1380, loss 0.50977, acc 0.796875\n",
      "2017-08-02T10:41:22.769017: step 1381, loss 0.493102, acc 0.765625\n",
      "2017-08-02T10:41:23.324072: step 1382, loss 0.433832, acc 0.84375\n",
      "2017-08-02T10:41:23.875128: step 1383, loss 0.549214, acc 0.6875\n",
      "2017-08-02T10:41:24.435184: step 1384, loss 0.429547, acc 0.828125\n",
      "2017-08-02T10:41:24.995240: step 1385, loss 0.609204, acc 0.703125\n",
      "2017-08-02T10:41:25.551295: step 1386, loss 0.387337, acc 0.859375\n",
      "2017-08-02T10:41:26.108351: step 1387, loss 0.525452, acc 0.75\n",
      "2017-08-02T10:41:26.689409: step 1388, loss 0.419087, acc 0.859375\n",
      "2017-08-02T10:41:27.245465: step 1389, loss 0.474426, acc 0.78125\n",
      "2017-08-02T10:41:27.798520: step 1390, loss 0.453771, acc 0.8125\n",
      "2017-08-02T10:41:28.353575: step 1391, loss 0.427193, acc 0.828125\n",
      "2017-08-02T10:41:28.905631: step 1392, loss 0.472404, acc 0.8125\n",
      "2017-08-02T10:41:29.481688: step 1393, loss 0.506649, acc 0.765625\n",
      "2017-08-02T10:41:30.056746: step 1394, loss 0.491022, acc 0.78125\n",
      "2017-08-02T10:41:30.615802: step 1395, loss 0.509971, acc 0.765625\n",
      "2017-08-02T10:41:31.179858: step 1396, loss 0.44849, acc 0.796875\n",
      "2017-08-02T10:41:31.746915: step 1397, loss 0.48571, acc 0.796875\n",
      "2017-08-02T10:41:32.301970: step 1398, loss 0.466208, acc 0.8125\n",
      "2017-08-02T10:41:32.858026: step 1399, loss 0.446958, acc 0.765625\n",
      "2017-08-02T10:41:33.412081: step 1400, loss 0.559485, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:41:37.062446: step 1400, loss 0.4828, acc 0.7832\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1400\n",
      "\n",
      "2017-08-02T10:41:39.365676: step 1401, loss 0.375686, acc 0.875\n",
      "2017-08-02T10:41:39.937734: step 1402, loss 0.505263, acc 0.78125\n",
      "2017-08-02T10:41:40.521792: step 1403, loss 0.456367, acc 0.75\n",
      "2017-08-02T10:41:41.092849: step 1404, loss 0.452812, acc 0.78125\n",
      "2017-08-02T10:41:41.683908: step 1405, loss 0.603743, acc 0.65625\n",
      "2017-08-02T10:41:42.252965: step 1406, loss 0.572116, acc 0.703125\n",
      "2017-08-02T10:41:42.816021: step 1407, loss 0.484533, acc 0.75\n",
      "2017-08-02T10:41:43.379078: step 1408, loss 0.470258, acc 0.78125\n",
      "2017-08-02T10:41:43.962136: step 1409, loss 0.566541, acc 0.703125\n",
      "2017-08-02T10:41:44.547195: step 1410, loss 0.486817, acc 0.734375\n",
      "2017-08-02T10:41:45.138254: step 1411, loss 0.463206, acc 0.796875\n",
      "2017-08-02T10:41:45.707311: step 1412, loss 0.45466, acc 0.796875\n",
      "2017-08-02T10:41:46.267367: step 1413, loss 0.449592, acc 0.84375\n",
      "2017-08-02T10:41:46.853425: step 1414, loss 0.504754, acc 0.75\n",
      "2017-08-02T10:41:47.409481: step 1415, loss 0.541207, acc 0.765625\n",
      "2017-08-02T10:41:47.961536: step 1416, loss 0.502309, acc 0.734375\n",
      "2017-08-02T10:41:48.517592: step 1417, loss 0.474542, acc 0.734375\n",
      "2017-08-02T10:41:49.074647: step 1418, loss 0.511918, acc 0.734375\n",
      "2017-08-02T10:41:49.629703: step 1419, loss 0.550837, acc 0.71875\n",
      "2017-08-02T10:41:50.183758: step 1420, loss 0.608171, acc 0.734375\n",
      "2017-08-02T10:41:50.744814: step 1421, loss 0.538, acc 0.71875\n",
      "2017-08-02T10:41:51.309871: step 1422, loss 0.560775, acc 0.703125\n",
      "2017-08-02T10:41:51.862926: step 1423, loss 0.605319, acc 0.65625\n",
      "2017-08-02T10:41:52.425982: step 1424, loss 0.364852, acc 0.875\n",
      "2017-08-02T10:41:52.986038: step 1425, loss 0.565513, acc 0.765625\n",
      "2017-08-02T10:41:53.540094: step 1426, loss 0.631462, acc 0.6875\n",
      "2017-08-02T10:41:54.097149: step 1427, loss 0.591726, acc 0.703125\n",
      "2017-08-02T10:41:54.651205: step 1428, loss 0.568515, acc 0.765625\n",
      "2017-08-02T10:41:55.208261: step 1429, loss 0.599215, acc 0.71875\n",
      "2017-08-02T10:41:55.765316: step 1430, loss 0.452198, acc 0.78125\n",
      "2017-08-02T10:41:56.323372: step 1431, loss 0.482869, acc 0.796875\n",
      "2017-08-02T10:41:56.881428: step 1432, loss 0.513723, acc 0.765625\n",
      "2017-08-02T10:41:57.435483: step 1433, loss 0.584321, acc 0.6875\n",
      "2017-08-02T10:41:57.988539: step 1434, loss 0.569774, acc 0.75\n",
      "2017-08-02T10:41:58.546594: step 1435, loss 0.494205, acc 0.78125\n",
      "2017-08-02T10:41:59.098650: step 1436, loss 0.469756, acc 0.765625\n",
      "2017-08-02T10:41:59.652705: step 1437, loss 0.505214, acc 0.796875\n",
      "2017-08-02T10:42:00.209761: step 1438, loss 0.608944, acc 0.6875\n",
      "2017-08-02T10:42:00.767816: step 1439, loss 0.437047, acc 0.78125\n",
      "2017-08-02T10:42:01.322872: step 1440, loss 0.428019, acc 0.828125\n",
      "2017-08-02T10:42:01.884928: step 1441, loss 0.643125, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:42:02.436983: step 1442, loss 0.480641, acc 0.765625\n",
      "2017-08-02T10:42:02.994039: step 1443, loss 0.588439, acc 0.703125\n",
      "2017-08-02T10:42:03.550095: step 1444, loss 0.468829, acc 0.78125\n",
      "2017-08-02T10:42:04.104150: step 1445, loss 0.401634, acc 0.828125\n",
      "2017-08-02T10:42:04.659206: step 1446, loss 0.540893, acc 0.78125\n",
      "2017-08-02T10:42:05.221262: step 1447, loss 0.505017, acc 0.8125\n",
      "2017-08-02T10:42:05.776317: step 1448, loss 0.50892, acc 0.734375\n",
      "2017-08-02T10:42:06.355375: step 1449, loss 0.462364, acc 0.765625\n",
      "2017-08-02T10:42:06.908430: step 1450, loss 0.527568, acc 0.75\n",
      "2017-08-02T10:42:07.462486: step 1451, loss 0.475672, acc 0.796875\n",
      "2017-08-02T10:42:08.014541: step 1452, loss 0.538298, acc 0.734375\n",
      "2017-08-02T10:42:08.569597: step 1453, loss 0.4426, acc 0.828125\n",
      "2017-08-02T10:42:09.124652: step 1454, loss 0.43614, acc 0.8125\n",
      "2017-08-02T10:42:09.678707: step 1455, loss 0.416916, acc 0.796875\n",
      "2017-08-02T10:42:10.235763: step 1456, loss 0.527341, acc 0.75\n",
      "2017-08-02T10:42:10.793819: step 1457, loss 0.458609, acc 0.859375\n",
      "2017-08-02T10:42:11.348874: step 1458, loss 0.492052, acc 0.78125\n",
      "2017-08-02T10:42:11.901930: step 1459, loss 0.466881, acc 0.78125\n",
      "2017-08-02T10:42:12.478987: step 1460, loss 0.459251, acc 0.78125\n",
      "2017-08-02T10:42:13.036043: step 1461, loss 0.444044, acc 0.78125\n",
      "2017-08-02T10:42:13.589098: step 1462, loss 0.406853, acc 0.828125\n",
      "2017-08-02T10:42:14.143154: step 1463, loss 0.494629, acc 0.78125\n",
      "2017-08-02T10:42:14.700210: step 1464, loss 0.486285, acc 0.78125\n",
      "2017-08-02T10:42:15.258265: step 1465, loss 0.521023, acc 0.71875\n",
      "2017-08-02T10:42:15.830323: step 1466, loss 0.506603, acc 0.734375\n",
      "2017-08-02T10:42:16.387378: step 1467, loss 0.553571, acc 0.796875\n",
      "2017-08-02T10:42:16.942434: step 1468, loss 0.478906, acc 0.78125\n",
      "2017-08-02T10:42:17.499489: step 1469, loss 0.522119, acc 0.828125\n",
      "2017-08-02T10:42:18.051545: step 1470, loss 0.512198, acc 0.828125\n",
      "2017-08-02T10:42:18.629602: step 1471, loss 0.635178, acc 0.6875\n",
      "2017-08-02T10:42:19.215661: step 1472, loss 0.447556, acc 0.78125\n",
      "2017-08-02T10:42:19.764716: step 1473, loss 0.727387, acc 0.671875\n",
      "2017-08-02T10:42:20.340774: step 1474, loss 0.428194, acc 0.8125\n",
      "2017-08-02T10:42:20.897829: step 1475, loss 0.614582, acc 0.671875\n",
      "2017-08-02T10:42:21.456885: step 1476, loss 0.556624, acc 0.734375\n",
      "2017-08-02T10:42:22.015941: step 1477, loss 0.47906, acc 0.75\n",
      "2017-08-02T10:42:22.568996: step 1478, loss 0.49548, acc 0.78125\n",
      "2017-08-02T10:42:23.126052: step 1479, loss 0.554989, acc 0.734375\n",
      "2017-08-02T10:42:23.682108: step 1480, loss 0.562565, acc 0.734375\n",
      "2017-08-02T10:42:24.238163: step 1481, loss 0.539813, acc 0.78125\n",
      "2017-08-02T10:42:24.795219: step 1482, loss 0.407424, acc 0.828125\n",
      "2017-08-02T10:42:25.349274: step 1483, loss 0.495086, acc 0.796875\n",
      "2017-08-02T10:42:25.903330: step 1484, loss 0.488552, acc 0.78125\n",
      "2017-08-02T10:42:26.463386: step 1485, loss 0.561435, acc 0.765625\n",
      "2017-08-02T10:42:27.017441: step 1486, loss 0.749388, acc 0.59375\n",
      "2017-08-02T10:42:27.567496: step 1487, loss 0.424586, acc 0.859375\n",
      "2017-08-02T10:42:28.121552: step 1488, loss 0.41358, acc 0.78125\n",
      "2017-08-02T10:42:28.685608: step 1489, loss 0.586237, acc 0.734375\n",
      "2017-08-02T10:42:29.245664: step 1490, loss 0.566489, acc 0.75\n",
      "2017-08-02T10:42:29.802720: step 1491, loss 0.378399, acc 0.84375\n",
      "2017-08-02T10:42:30.359775: step 1492, loss 0.557204, acc 0.6875\n",
      "2017-08-02T10:42:30.914831: step 1493, loss 0.511444, acc 0.8125\n",
      "2017-08-02T10:42:31.475887: step 1494, loss 0.668409, acc 0.65625\n",
      "2017-08-02T10:42:32.027942: step 1495, loss 0.609683, acc 0.640625\n",
      "2017-08-02T10:42:32.581998: step 1496, loss 0.559323, acc 0.703125\n",
      "2017-08-02T10:42:33.141053: step 1497, loss 0.492879, acc 0.78125\n",
      "2017-08-02T10:42:33.719111: step 1498, loss 0.548525, acc 0.734375\n",
      "2017-08-02T10:42:34.296169: step 1499, loss 0.567984, acc 0.734375\n",
      "2017-08-02T10:42:34.871226: step 1500, loss 0.654749, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:42:38.620601: step 1500, loss 0.522697, acc 0.7668\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1500\n",
      "\n",
      "2017-08-02T10:42:41.238863: step 1501, loss 0.615274, acc 0.75\n",
      "2017-08-02T10:42:41.820921: step 1502, loss 0.404444, acc 0.859375\n",
      "2017-08-02T10:42:42.413981: step 1503, loss 0.660741, acc 0.625\n",
      "2017-08-02T10:42:42.992038: step 1504, loss 0.609582, acc 0.6875\n",
      "2017-08-02T10:42:43.573097: step 1505, loss 0.692022, acc 0.65625\n",
      "2017-08-02T10:42:44.156155: step 1506, loss 0.513412, acc 0.71875\n",
      "2017-08-02T10:42:44.745214: step 1507, loss 0.502492, acc 0.75\n",
      "2017-08-02T10:42:45.338273: step 1508, loss 0.428644, acc 0.8125\n",
      "2017-08-02T10:42:45.930332: step 1509, loss 0.530058, acc 0.765625\n",
      "2017-08-02T10:42:46.497389: step 1510, loss 0.524294, acc 0.75\n",
      "2017-08-02T10:42:47.067446: step 1511, loss 0.463128, acc 0.796875\n",
      "2017-08-02T10:42:47.639503: step 1512, loss 0.480749, acc 0.78125\n",
      "2017-08-02T10:42:48.211560: step 1513, loss 0.510015, acc 0.75\n",
      "2017-08-02T10:42:48.790618: step 1514, loss 0.439819, acc 0.84375\n",
      "2017-08-02T10:42:49.360675: step 1515, loss 0.474925, acc 0.796875\n",
      "2017-08-02T10:42:49.944734: step 1516, loss 0.735749, acc 0.65625\n",
      "2017-08-02T10:42:50.531792: step 1517, loss 0.485448, acc 0.734375\n",
      "2017-08-02T10:42:51.114851: step 1518, loss 0.630191, acc 0.671875\n",
      "2017-08-02T10:42:51.688908: step 1519, loss 0.584091, acc 0.71875\n",
      "2017-08-02T10:42:52.260965: step 1520, loss 0.472525, acc 0.828125\n",
      "2017-08-02T10:42:52.860025: step 1521, loss 0.547005, acc 0.78125\n",
      "2017-08-02T10:42:53.442083: step 1522, loss 0.485074, acc 0.828125\n",
      "2017-08-02T10:42:54.009140: step 1523, loss 0.520424, acc 0.78125\n",
      "2017-08-02T10:42:54.570196: step 1524, loss 0.512321, acc 0.765625\n",
      "2017-08-02T10:42:55.139253: step 1525, loss 0.337269, acc 0.875\n",
      "2017-08-02T10:42:55.694309: step 1526, loss 0.523254, acc 0.78125\n",
      "2017-08-02T10:42:56.251364: step 1527, loss 0.530552, acc 0.734375\n",
      "2017-08-02T10:42:56.814421: step 1528, loss 0.514884, acc 0.78125\n",
      "2017-08-02T10:42:57.370476: step 1529, loss 0.473575, acc 0.8125\n",
      "2017-08-02T10:42:57.929532: step 1530, loss 0.542551, acc 0.734375\n",
      "2017-08-02T10:42:58.487588: step 1531, loss 0.539663, acc 0.703125\n",
      "2017-08-02T10:42:59.046644: step 1532, loss 0.508932, acc 0.71875\n",
      "2017-08-02T10:42:59.605700: step 1533, loss 0.451456, acc 0.78125\n",
      "2017-08-02T10:43:00.168756: step 1534, loss 0.402437, acc 0.84375\n",
      "2017-08-02T10:43:00.759815: step 1535, loss 0.468048, acc 0.8125\n",
      "2017-08-02T10:43:01.354875: step 1536, loss 0.507057, acc 0.796875\n",
      "2017-08-02T10:43:01.955935: step 1537, loss 0.499606, acc 0.765625\n",
      "2017-08-02T10:43:02.548994: step 1538, loss 0.483341, acc 0.828125\n",
      "2017-08-02T10:43:03.144053: step 1539, loss 0.54729, acc 0.71875\n",
      "2017-08-02T10:43:03.735113: step 1540, loss 0.46325, acc 0.796875\n",
      "2017-08-02T10:43:04.332172: step 1541, loss 0.436044, acc 0.78125\n",
      "2017-08-02T10:43:04.917231: step 1542, loss 0.480031, acc 0.78125\n",
      "2017-08-02T10:43:05.474286: step 1543, loss 0.646468, acc 0.71875\n",
      "2017-08-02T10:43:06.030342: step 1544, loss 0.601423, acc 0.671875\n",
      "2017-08-02T10:43:06.603399: step 1545, loss 0.522052, acc 0.765625\n",
      "2017-08-02T10:43:07.162455: step 1546, loss 0.514826, acc 0.75\n",
      "2017-08-02T10:43:07.718511: step 1547, loss 0.450662, acc 0.78125\n",
      "2017-08-02T10:43:08.277567: step 1548, loss 0.409198, acc 0.828125\n",
      "2017-08-02T10:43:08.837623: step 1549, loss 0.381237, acc 0.84375\n",
      "2017-08-02T10:43:09.394678: step 1550, loss 0.541531, acc 0.75\n",
      "2017-08-02T10:43:09.952734: step 1551, loss 0.502808, acc 0.8125\n",
      "2017-08-02T10:43:10.510790: step 1552, loss 0.49326, acc 0.78125\n",
      "2017-08-02T10:43:11.067846: step 1553, loss 0.526426, acc 0.765625\n",
      "2017-08-02T10:43:11.633902: step 1554, loss 0.455048, acc 0.796875\n",
      "2017-08-02T10:43:12.191958: step 1555, loss 0.489779, acc 0.734375\n",
      "2017-08-02T10:43:12.748014: step 1556, loss 0.633564, acc 0.703125\n",
      "2017-08-02T10:43:13.303069: step 1557, loss 0.507442, acc 0.828125\n",
      "2017-08-02T10:43:13.861125: step 1558, loss 0.512038, acc 0.71875\n",
      "2017-08-02T10:43:14.424181: step 1559, loss 0.495704, acc 0.765625\n",
      "2017-08-02T10:43:15.003239: step 1560, loss 0.48705, acc 0.765625\n",
      "2017-08-02T10:43:15.560295: step 1561, loss 0.526629, acc 0.71875\n",
      "2017-08-02T10:43:16.116351: step 1562, loss 0.559873, acc 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:43:16.676407: step 1563, loss 0.498126, acc 0.78125\n",
      "2017-08-02T10:43:17.233462: step 1564, loss 0.652372, acc 0.71875\n",
      "2017-08-02T10:43:17.643503: step 1565, loss 0.531079, acc 0.75\n",
      "2017-08-02T10:43:18.204559: step 1566, loss 0.601464, acc 0.78125\n",
      "2017-08-02T10:43:18.756615: step 1567, loss 0.416128, acc 0.8125\n",
      "2017-08-02T10:43:19.318671: step 1568, loss 0.587481, acc 0.75\n",
      "2017-08-02T10:43:19.893728: step 1569, loss 0.406801, acc 0.84375\n",
      "2017-08-02T10:43:20.447784: step 1570, loss 0.40634, acc 0.8125\n",
      "2017-08-02T10:43:21.002839: step 1571, loss 0.383736, acc 0.796875\n",
      "2017-08-02T10:43:21.556895: step 1572, loss 0.631922, acc 0.671875\n",
      "2017-08-02T10:43:22.109950: step 1573, loss 0.437886, acc 0.75\n",
      "2017-08-02T10:43:22.670006: step 1574, loss 0.472852, acc 0.78125\n",
      "2017-08-02T10:43:23.227062: step 1575, loss 0.348024, acc 0.875\n",
      "2017-08-02T10:43:23.782117: step 1576, loss 0.465041, acc 0.796875\n",
      "2017-08-02T10:43:24.339173: step 1577, loss 0.474045, acc 0.78125\n",
      "2017-08-02T10:43:24.895228: step 1578, loss 0.550684, acc 0.71875\n",
      "2017-08-02T10:43:25.450284: step 1579, loss 0.460156, acc 0.828125\n",
      "2017-08-02T10:43:26.008340: step 1580, loss 0.396337, acc 0.84375\n",
      "2017-08-02T10:43:26.572396: step 1581, loss 0.434384, acc 0.78125\n",
      "2017-08-02T10:43:27.128452: step 1582, loss 0.444031, acc 0.828125\n",
      "2017-08-02T10:43:27.680507: step 1583, loss 0.441364, acc 0.765625\n",
      "2017-08-02T10:43:28.235562: step 1584, loss 0.428693, acc 0.828125\n",
      "2017-08-02T10:43:28.795618: step 1585, loss 0.424307, acc 0.828125\n",
      "2017-08-02T10:43:29.354674: step 1586, loss 0.442106, acc 0.765625\n",
      "2017-08-02T10:43:29.908730: step 1587, loss 0.52708, acc 0.75\n",
      "2017-08-02T10:43:30.466785: step 1588, loss 0.521076, acc 0.8125\n",
      "2017-08-02T10:43:31.017841: step 1589, loss 0.469636, acc 0.796875\n",
      "2017-08-02T10:43:31.579897: step 1590, loss 0.445018, acc 0.828125\n",
      "2017-08-02T10:43:32.156954: step 1591, loss 0.499613, acc 0.796875\n",
      "2017-08-02T10:43:32.717010: step 1592, loss 0.358244, acc 0.84375\n",
      "2017-08-02T10:43:33.274066: step 1593, loss 0.563209, acc 0.734375\n",
      "2017-08-02T10:43:33.835122: step 1594, loss 0.61628, acc 0.671875\n",
      "2017-08-02T10:43:34.393178: step 1595, loss 0.401957, acc 0.828125\n",
      "2017-08-02T10:43:34.951234: step 1596, loss 0.509163, acc 0.734375\n",
      "2017-08-02T10:43:35.509290: step 1597, loss 0.419454, acc 0.796875\n",
      "2017-08-02T10:43:36.067345: step 1598, loss 0.512855, acc 0.734375\n",
      "2017-08-02T10:43:36.621401: step 1599, loss 0.3729, acc 0.828125\n",
      "2017-08-02T10:43:37.178457: step 1600, loss 0.484827, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:43:40.875826: step 1600, loss 0.515379, acc 0.7656\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1600\n",
      "\n",
      "2017-08-02T10:43:43.510090: step 1601, loss 0.44956, acc 0.71875\n",
      "2017-08-02T10:43:44.066145: step 1602, loss 0.501348, acc 0.6875\n",
      "2017-08-02T10:43:44.625201: step 1603, loss 0.601433, acc 0.75\n",
      "2017-08-02T10:43:45.181257: step 1604, loss 0.440972, acc 0.859375\n",
      "2017-08-02T10:43:45.739313: step 1605, loss 0.407614, acc 0.8125\n",
      "2017-08-02T10:43:46.298368: step 1606, loss 0.492822, acc 0.765625\n",
      "2017-08-02T10:43:46.856424: step 1607, loss 0.529976, acc 0.765625\n",
      "2017-08-02T10:43:47.413480: step 1608, loss 0.466725, acc 0.828125\n",
      "2017-08-02T10:43:47.964535: step 1609, loss 0.453128, acc 0.78125\n",
      "2017-08-02T10:43:48.524591: step 1610, loss 0.459663, acc 0.75\n",
      "2017-08-02T10:43:49.086647: step 1611, loss 0.511932, acc 0.75\n",
      "2017-08-02T10:43:49.640703: step 1612, loss 0.330537, acc 0.84375\n",
      "2017-08-02T10:43:50.195758: step 1613, loss 0.458587, acc 0.828125\n",
      "2017-08-02T10:43:50.748813: step 1614, loss 0.405266, acc 0.859375\n",
      "2017-08-02T10:43:51.303869: step 1615, loss 0.408547, acc 0.84375\n",
      "2017-08-02T10:43:51.858924: step 1616, loss 0.419206, acc 0.828125\n",
      "2017-08-02T10:43:52.419981: step 1617, loss 0.441694, acc 0.765625\n",
      "2017-08-02T10:43:52.977036: step 1618, loss 0.503205, acc 0.734375\n",
      "2017-08-02T10:43:53.530092: step 1619, loss 0.44563, acc 0.84375\n",
      "2017-08-02T10:43:54.091148: step 1620, loss 0.527785, acc 0.703125\n",
      "2017-08-02T10:43:54.674206: step 1621, loss 0.52944, acc 0.765625\n",
      "2017-08-02T10:43:55.230262: step 1622, loss 0.494025, acc 0.796875\n",
      "2017-08-02T10:43:55.786317: step 1623, loss 0.586242, acc 0.65625\n",
      "2017-08-02T10:43:56.361375: step 1624, loss 0.55698, acc 0.75\n",
      "2017-08-02T10:43:56.936432: step 1625, loss 0.464207, acc 0.828125\n",
      "2017-08-02T10:43:57.494488: step 1626, loss 0.541639, acc 0.765625\n",
      "2017-08-02T10:43:58.046543: step 1627, loss 0.508519, acc 0.78125\n",
      "2017-08-02T10:43:58.601599: step 1628, loss 0.503586, acc 0.765625\n",
      "2017-08-02T10:43:59.176656: step 1629, loss 0.406473, acc 0.875\n",
      "2017-08-02T10:43:59.731712: step 1630, loss 0.460001, acc 0.765625\n",
      "2017-08-02T10:44:00.287767: step 1631, loss 0.481823, acc 0.734375\n",
      "2017-08-02T10:44:00.847823: step 1632, loss 0.53823, acc 0.734375\n",
      "2017-08-02T10:44:01.403879: step 1633, loss 0.453429, acc 0.78125\n",
      "2017-08-02T10:44:01.952934: step 1634, loss 0.492808, acc 0.8125\n",
      "2017-08-02T10:44:02.511990: step 1635, loss 0.469362, acc 0.796875\n",
      "2017-08-02T10:44:03.074046: step 1636, loss 0.433069, acc 0.8125\n",
      "2017-08-02T10:44:03.625101: step 1637, loss 0.42847, acc 0.8125\n",
      "2017-08-02T10:44:04.181157: step 1638, loss 0.469719, acc 0.796875\n",
      "2017-08-02T10:44:04.741213: step 1639, loss 0.405646, acc 0.828125\n",
      "2017-08-02T10:44:05.298268: step 1640, loss 0.535946, acc 0.734375\n",
      "2017-08-02T10:44:05.855324: step 1641, loss 0.422137, acc 0.8125\n",
      "2017-08-02T10:44:06.414380: step 1642, loss 0.459511, acc 0.75\n",
      "2017-08-02T10:44:06.971436: step 1643, loss 0.454228, acc 0.8125\n",
      "2017-08-02T10:44:07.547493: step 1644, loss 0.414165, acc 0.859375\n",
      "2017-08-02T10:44:08.098548: step 1645, loss 0.444002, acc 0.796875\n",
      "2017-08-02T10:44:08.651604: step 1646, loss 0.440195, acc 0.75\n",
      "2017-08-02T10:44:09.209659: step 1647, loss 0.552579, acc 0.703125\n",
      "2017-08-02T10:44:09.767715: step 1648, loss 0.552169, acc 0.703125\n",
      "2017-08-02T10:44:10.326771: step 1649, loss 0.580681, acc 0.765625\n",
      "2017-08-02T10:44:10.885827: step 1650, loss 0.355957, acc 0.828125\n",
      "2017-08-02T10:44:11.441883: step 1651, loss 0.383563, acc 0.875\n",
      "2017-08-02T10:44:11.998938: step 1652, loss 0.47994, acc 0.796875\n",
      "2017-08-02T10:44:12.551994: step 1653, loss 0.397347, acc 0.921875\n",
      "2017-08-02T10:44:13.102049: step 1654, loss 0.474291, acc 0.796875\n",
      "2017-08-02T10:44:13.658104: step 1655, loss 0.413949, acc 0.796875\n",
      "2017-08-02T10:44:14.208159: step 1656, loss 0.323323, acc 0.890625\n",
      "2017-08-02T10:44:14.770215: step 1657, loss 0.437527, acc 0.78125\n",
      "2017-08-02T10:44:15.332272: step 1658, loss 0.52087, acc 0.75\n",
      "2017-08-02T10:44:15.887327: step 1659, loss 0.368503, acc 0.890625\n",
      "2017-08-02T10:44:16.441382: step 1660, loss 0.434618, acc 0.8125\n",
      "2017-08-02T10:44:16.993438: step 1661, loss 0.553393, acc 0.734375\n",
      "2017-08-02T10:44:17.550493: step 1662, loss 0.424082, acc 0.828125\n",
      "2017-08-02T10:44:18.102549: step 1663, loss 0.420263, acc 0.84375\n",
      "2017-08-02T10:44:18.660604: step 1664, loss 0.404046, acc 0.890625\n",
      "2017-08-02T10:44:19.214660: step 1665, loss 0.345458, acc 0.859375\n",
      "2017-08-02T10:44:19.769715: step 1666, loss 0.389864, acc 0.84375\n",
      "2017-08-02T10:44:20.349773: step 1667, loss 0.549044, acc 0.765625\n",
      "2017-08-02T10:44:20.909829: step 1668, loss 0.367034, acc 0.859375\n",
      "2017-08-02T10:44:21.465885: step 1669, loss 0.567003, acc 0.703125\n",
      "2017-08-02T10:44:22.021940: step 1670, loss 0.419649, acc 0.78125\n",
      "2017-08-02T10:44:22.573996: step 1671, loss 0.48375, acc 0.828125\n",
      "2017-08-02T10:44:23.135052: step 1672, loss 0.388482, acc 0.890625\n",
      "2017-08-02T10:44:23.695108: step 1673, loss 0.432556, acc 0.8125\n",
      "2017-08-02T10:44:24.248163: step 1674, loss 0.462993, acc 0.78125\n",
      "2017-08-02T10:44:24.804219: step 1675, loss 0.356861, acc 0.890625\n",
      "2017-08-02T10:44:25.366275: step 1676, loss 0.468985, acc 0.8125\n",
      "2017-08-02T10:44:25.922330: step 1677, loss 0.429953, acc 0.796875\n",
      "2017-08-02T10:44:26.481386: step 1678, loss 0.421424, acc 0.8125\n",
      "2017-08-02T10:44:27.033442: step 1679, loss 0.395045, acc 0.84375\n",
      "2017-08-02T10:44:27.591497: step 1680, loss 0.455287, acc 0.78125\n",
      "2017-08-02T10:44:28.169555: step 1681, loss 0.508018, acc 0.796875\n",
      "2017-08-02T10:44:28.725611: step 1682, loss 0.497465, acc 0.75\n",
      "2017-08-02T10:44:29.292667: step 1683, loss 0.455717, acc 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:44:29.851723: step 1684, loss 0.468618, acc 0.78125\n",
      "2017-08-02T10:44:30.402778: step 1685, loss 0.696069, acc 0.640625\n",
      "2017-08-02T10:44:30.956834: step 1686, loss 0.531898, acc 0.78125\n",
      "2017-08-02T10:44:31.504889: step 1687, loss 0.475655, acc 0.78125\n",
      "2017-08-02T10:44:32.056944: step 1688, loss 0.373732, acc 0.859375\n",
      "2017-08-02T10:44:32.609999: step 1689, loss 0.495784, acc 0.796875\n",
      "2017-08-02T10:44:33.165055: step 1690, loss 0.389561, acc 0.828125\n",
      "2017-08-02T10:44:33.718110: step 1691, loss 0.576986, acc 0.6875\n",
      "2017-08-02T10:44:34.268165: step 1692, loss 0.489863, acc 0.75\n",
      "2017-08-02T10:44:34.831221: step 1693, loss 0.378937, acc 0.875\n",
      "2017-08-02T10:44:35.391277: step 1694, loss 0.386128, acc 0.828125\n",
      "2017-08-02T10:44:35.947333: step 1695, loss 0.408875, acc 0.796875\n",
      "2017-08-02T10:44:36.501388: step 1696, loss 0.502129, acc 0.734375\n",
      "2017-08-02T10:44:37.059444: step 1697, loss 0.32089, acc 0.875\n",
      "2017-08-02T10:44:37.615500: step 1698, loss 0.336674, acc 0.90625\n",
      "2017-08-02T10:44:38.170555: step 1699, loss 0.585488, acc 0.703125\n",
      "2017-08-02T10:44:38.729611: step 1700, loss 0.402422, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:44:42.462984: step 1700, loss 0.46713, acc 0.8008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1700\n",
      "\n",
      "2017-08-02T10:44:44.997238: step 1701, loss 0.484149, acc 0.765625\n",
      "2017-08-02T10:44:45.572295: step 1702, loss 0.377754, acc 0.828125\n",
      "2017-08-02T10:44:46.136352: step 1703, loss 0.412564, acc 0.875\n",
      "2017-08-02T10:44:46.697408: step 1704, loss 0.456889, acc 0.8125\n",
      "2017-08-02T10:44:47.268465: step 1705, loss 0.53393, acc 0.75\n",
      "2017-08-02T10:44:47.846523: step 1706, loss 0.390216, acc 0.796875\n",
      "2017-08-02T10:44:48.422580: step 1707, loss 0.501508, acc 0.796875\n",
      "2017-08-02T10:44:49.012639: step 1708, loss 0.393605, acc 0.84375\n",
      "2017-08-02T10:44:49.575696: step 1709, loss 0.467645, acc 0.796875\n",
      "2017-08-02T10:44:50.129751: step 1710, loss 0.372028, acc 0.859375\n",
      "2017-08-02T10:44:50.690807: step 1711, loss 0.480896, acc 0.84375\n",
      "2017-08-02T10:44:51.247863: step 1712, loss 0.417839, acc 0.78125\n",
      "2017-08-02T10:44:51.801918: step 1713, loss 0.381441, acc 0.90625\n",
      "2017-08-02T10:44:52.361974: step 1714, loss 0.471094, acc 0.78125\n",
      "2017-08-02T10:44:52.928031: step 1715, loss 0.532779, acc 0.734375\n",
      "2017-08-02T10:44:53.484086: step 1716, loss 0.513262, acc 0.75\n",
      "2017-08-02T10:44:54.037142: step 1717, loss 0.559112, acc 0.765625\n",
      "2017-08-02T10:44:54.603198: step 1718, loss 0.396426, acc 0.796875\n",
      "2017-08-02T10:44:55.168255: step 1719, loss 0.588055, acc 0.734375\n",
      "2017-08-02T10:44:55.736312: step 1720, loss 0.405922, acc 0.796875\n",
      "2017-08-02T10:44:56.304368: step 1721, loss 0.608962, acc 0.671875\n",
      "2017-08-02T10:44:56.871425: step 1722, loss 0.410568, acc 0.828125\n",
      "2017-08-02T10:44:57.432481: step 1723, loss 0.4968, acc 0.8125\n",
      "2017-08-02T10:44:57.991537: step 1724, loss 0.394384, acc 0.84375\n",
      "2017-08-02T10:44:58.558594: step 1725, loss 0.459301, acc 0.765625\n",
      "2017-08-02T10:44:59.129651: step 1726, loss 0.485178, acc 0.859375\n",
      "2017-08-02T10:44:59.713709: step 1727, loss 0.47235, acc 0.796875\n",
      "2017-08-02T10:45:00.290767: step 1728, loss 0.386101, acc 0.859375\n",
      "2017-08-02T10:45:00.858824: step 1729, loss 0.394466, acc 0.8125\n",
      "2017-08-02T10:45:01.428881: step 1730, loss 0.492855, acc 0.78125\n",
      "2017-08-02T10:45:02.013939: step 1731, loss 0.41363, acc 0.8125\n",
      "2017-08-02T10:45:02.592997: step 1732, loss 0.440954, acc 0.796875\n",
      "2017-08-02T10:45:03.158054: step 1733, loss 0.482688, acc 0.703125\n",
      "2017-08-02T10:45:03.723110: step 1734, loss 0.486572, acc 0.8125\n",
      "2017-08-02T10:45:04.283166: step 1735, loss 0.380595, acc 0.875\n",
      "2017-08-02T10:45:04.859224: step 1736, loss 0.446333, acc 0.796875\n",
      "2017-08-02T10:45:05.418280: step 1737, loss 0.364127, acc 0.875\n",
      "2017-08-02T10:45:05.976335: step 1738, loss 0.337603, acc 0.890625\n",
      "2017-08-02T10:45:06.536391: step 1739, loss 0.441814, acc 0.828125\n",
      "2017-08-02T10:45:07.099448: step 1740, loss 0.469083, acc 0.75\n",
      "2017-08-02T10:45:07.657504: step 1741, loss 0.48851, acc 0.8125\n",
      "2017-08-02T10:45:08.217560: step 1742, loss 0.471855, acc 0.765625\n",
      "2017-08-02T10:45:08.773615: step 1743, loss 0.479919, acc 0.75\n",
      "2017-08-02T10:45:09.325670: step 1744, loss 0.53595, acc 0.796875\n",
      "2017-08-02T10:45:09.880726: step 1745, loss 0.436875, acc 0.765625\n",
      "2017-08-02T10:45:10.438782: step 1746, loss 0.496482, acc 0.796875\n",
      "2017-08-02T10:45:10.991837: step 1747, loss 0.564224, acc 0.71875\n",
      "2017-08-02T10:45:11.550893: step 1748, loss 0.636792, acc 0.734375\n",
      "2017-08-02T10:45:12.109949: step 1749, loss 0.535602, acc 0.75\n",
      "2017-08-02T10:45:12.670005: step 1750, loss 0.371719, acc 0.828125\n",
      "2017-08-02T10:45:13.225060: step 1751, loss 0.448418, acc 0.828125\n",
      "2017-08-02T10:45:13.777115: step 1752, loss 0.409088, acc 0.796875\n",
      "2017-08-02T10:45:14.331171: step 1753, loss 0.338183, acc 0.875\n",
      "2017-08-02T10:45:14.892227: step 1754, loss 0.399079, acc 0.859375\n",
      "2017-08-02T10:45:15.446282: step 1755, loss 0.424693, acc 0.859375\n",
      "2017-08-02T10:45:16.006338: step 1756, loss 0.356626, acc 0.875\n",
      "2017-08-02T10:45:16.561394: step 1757, loss 0.396648, acc 0.8125\n",
      "2017-08-02T10:45:17.143452: step 1758, loss 0.392273, acc 0.8125\n",
      "2017-08-02T10:45:17.701508: step 1759, loss 0.491457, acc 0.796875\n",
      "2017-08-02T10:45:18.254563: step 1760, loss 0.510545, acc 0.75\n",
      "2017-08-02T10:45:18.815619: step 1761, loss 0.469731, acc 0.828125\n",
      "2017-08-02T10:45:19.376675: step 1762, loss 0.599013, acc 0.8125\n",
      "2017-08-02T10:45:19.930731: step 1763, loss 0.453667, acc 0.890625\n",
      "2017-08-02T10:45:20.490787: step 1764, loss 0.398489, acc 0.828125\n",
      "2017-08-02T10:45:21.070845: step 1765, loss 0.377624, acc 0.828125\n",
      "2017-08-02T10:45:21.635901: step 1766, loss 0.439404, acc 0.84375\n",
      "2017-08-02T10:45:22.183956: step 1767, loss 0.456759, acc 0.828125\n",
      "2017-08-02T10:45:22.744012: step 1768, loss 0.476439, acc 0.8125\n",
      "2017-08-02T10:45:23.295067: step 1769, loss 0.378171, acc 0.859375\n",
      "2017-08-02T10:45:23.856123: step 1770, loss 0.436729, acc 0.796875\n",
      "2017-08-02T10:45:24.413179: step 1771, loss 0.536659, acc 0.765625\n",
      "2017-08-02T10:45:24.968234: step 1772, loss 0.380904, acc 0.796875\n",
      "2017-08-02T10:45:25.524290: step 1773, loss 0.487472, acc 0.75\n",
      "2017-08-02T10:45:26.078345: step 1774, loss 0.412607, acc 0.84375\n",
      "2017-08-02T10:45:26.638401: step 1775, loss 0.451872, acc 0.75\n",
      "2017-08-02T10:45:27.197457: step 1776, loss 0.503333, acc 0.75\n",
      "2017-08-02T10:45:27.756513: step 1777, loss 0.388869, acc 0.828125\n",
      "2017-08-02T10:45:28.312569: step 1778, loss 0.434861, acc 0.859375\n",
      "2017-08-02T10:45:28.869625: step 1779, loss 0.46587, acc 0.8125\n",
      "2017-08-02T10:45:29.429681: step 1780, loss 0.393033, acc 0.84375\n",
      "2017-08-02T10:45:29.993737: step 1781, loss 0.368105, acc 0.84375\n",
      "2017-08-02T10:45:30.547792: step 1782, loss 0.380934, acc 0.84375\n",
      "2017-08-02T10:45:31.100848: step 1783, loss 0.500509, acc 0.75\n",
      "2017-08-02T10:45:31.655903: step 1784, loss 0.396419, acc 0.84375\n",
      "2017-08-02T10:45:32.213959: step 1785, loss 0.534201, acc 0.71875\n",
      "2017-08-02T10:45:32.773015: step 1786, loss 0.427498, acc 0.84375\n",
      "2017-08-02T10:45:33.328070: step 1787, loss 0.452734, acc 0.796875\n",
      "2017-08-02T10:45:33.882126: step 1788, loss 0.405681, acc 0.890625\n",
      "2017-08-02T10:45:34.439181: step 1789, loss 0.45194, acc 0.78125\n",
      "2017-08-02T10:45:34.998237: step 1790, loss 0.527582, acc 0.78125\n",
      "2017-08-02T10:45:35.567294: step 1791, loss 0.465241, acc 0.734375\n",
      "2017-08-02T10:45:36.149352: step 1792, loss 0.422294, acc 0.796875\n",
      "2017-08-02T10:45:36.709408: step 1793, loss 0.451792, acc 0.796875\n",
      "2017-08-02T10:45:37.265464: step 1794, loss 0.446603, acc 0.828125\n",
      "2017-08-02T10:45:37.822520: step 1795, loss 0.498014, acc 0.796875\n",
      "2017-08-02T10:45:38.380576: step 1796, loss 0.529182, acc 0.78125\n",
      "2017-08-02T10:45:38.936631: step 1797, loss 0.375663, acc 0.828125\n",
      "2017-08-02T10:45:39.496687: step 1798, loss 0.398971, acc 0.828125\n",
      "2017-08-02T10:45:40.059743: step 1799, loss 0.465624, acc 0.765625\n",
      "2017-08-02T10:45:40.614799: step 1800, loss 0.468162, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:45:44.292167: step 1800, loss 0.454125, acc 0.81\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1800\n",
      "\n",
      "2017-08-02T10:45:46.903428: step 1801, loss 0.518803, acc 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:45:47.459483: step 1802, loss 0.421197, acc 0.84375\n",
      "2017-08-02T10:45:48.019539: step 1803, loss 0.439614, acc 0.796875\n",
      "2017-08-02T10:45:48.570594: step 1804, loss 0.421788, acc 0.828125\n",
      "2017-08-02T10:45:49.120649: step 1805, loss 0.414804, acc 0.8125\n",
      "2017-08-02T10:45:49.676705: step 1806, loss 0.462844, acc 0.796875\n",
      "2017-08-02T10:45:50.228760: step 1807, loss 0.431809, acc 0.78125\n",
      "2017-08-02T10:45:50.782816: step 1808, loss 0.344461, acc 0.828125\n",
      "2017-08-02T10:45:51.336871: step 1809, loss 0.498943, acc 0.765625\n",
      "2017-08-02T10:45:51.894927: step 1810, loss 0.456986, acc 0.765625\n",
      "2017-08-02T10:45:52.448982: step 1811, loss 0.38859, acc 0.8125\n",
      "2017-08-02T10:45:52.999037: step 1812, loss 0.385788, acc 0.84375\n",
      "2017-08-02T10:45:53.551092: step 1813, loss 0.412289, acc 0.796875\n",
      "2017-08-02T10:45:54.106148: step 1814, loss 0.456842, acc 0.8125\n",
      "2017-08-02T10:45:54.663204: step 1815, loss 0.375014, acc 0.84375\n",
      "2017-08-02T10:45:55.223260: step 1816, loss 0.465631, acc 0.8125\n",
      "2017-08-02T10:45:55.778315: step 1817, loss 0.390765, acc 0.890625\n",
      "2017-08-02T10:45:56.332371: step 1818, loss 0.414078, acc 0.859375\n",
      "2017-08-02T10:45:56.895427: step 1819, loss 0.38668, acc 0.78125\n",
      "2017-08-02T10:45:57.450482: step 1820, loss 0.424705, acc 0.828125\n",
      "2017-08-02T10:45:58.006538: step 1821, loss 0.369143, acc 0.8125\n",
      "2017-08-02T10:45:58.559593: step 1822, loss 0.422423, acc 0.8125\n",
      "2017-08-02T10:45:59.114649: step 1823, loss 0.447939, acc 0.765625\n",
      "2017-08-02T10:45:59.667704: step 1824, loss 0.399131, acc 0.859375\n",
      "2017-08-02T10:46:00.225760: step 1825, loss 0.425536, acc 0.8125\n",
      "2017-08-02T10:46:00.799817: step 1826, loss 0.506814, acc 0.765625\n",
      "2017-08-02T10:46:01.415879: step 1827, loss 0.466497, acc 0.796875\n",
      "2017-08-02T10:46:02.010938: step 1828, loss 0.472911, acc 0.765625\n",
      "2017-08-02T10:46:02.599997: step 1829, loss 0.494632, acc 0.75\n",
      "2017-08-02T10:46:03.187056: step 1830, loss 0.531406, acc 0.75\n",
      "2017-08-02T10:46:03.789116: step 1831, loss 0.436586, acc 0.78125\n",
      "2017-08-02T10:46:04.378175: step 1832, loss 0.338235, acc 0.921875\n",
      "2017-08-02T10:46:04.963234: step 1833, loss 0.405818, acc 0.84375\n",
      "2017-08-02T10:46:05.519289: step 1834, loss 0.482844, acc 0.8125\n",
      "2017-08-02T10:46:06.068344: step 1835, loss 0.39308, acc 0.859375\n",
      "2017-08-02T10:46:06.621399: step 1836, loss 0.331372, acc 0.859375\n",
      "2017-08-02T10:46:07.179455: step 1837, loss 0.348608, acc 0.875\n",
      "2017-08-02T10:46:07.742511: step 1838, loss 0.423693, acc 0.828125\n",
      "2017-08-02T10:46:08.297567: step 1839, loss 0.398416, acc 0.796875\n",
      "2017-08-02T10:46:08.851622: step 1840, loss 0.439645, acc 0.84375\n",
      "2017-08-02T10:46:09.404678: step 1841, loss 0.389594, acc 0.8125\n",
      "2017-08-02T10:46:09.959733: step 1842, loss 0.471175, acc 0.78125\n",
      "2017-08-02T10:46:10.517789: step 1843, loss 0.345183, acc 0.859375\n",
      "2017-08-02T10:46:11.072844: step 1844, loss 0.49806, acc 0.78125\n",
      "2017-08-02T10:46:11.634901: step 1845, loss 0.394106, acc 0.875\n",
      "2017-08-02T10:46:12.193957: step 1846, loss 0.380072, acc 0.84375\n",
      "2017-08-02T10:46:12.757013: step 1847, loss 0.451812, acc 0.78125\n",
      "2017-08-02T10:46:13.305068: step 1848, loss 0.360797, acc 0.890625\n",
      "2017-08-02T10:46:13.857123: step 1849, loss 0.463502, acc 0.78125\n",
      "2017-08-02T10:46:14.421179: step 1850, loss 0.510995, acc 0.734375\n",
      "2017-08-02T10:46:14.974235: step 1851, loss 0.373488, acc 0.84375\n",
      "2017-08-02T10:46:15.530290: step 1852, loss 0.363697, acc 0.8125\n",
      "2017-08-02T10:46:16.088346: step 1853, loss 0.414368, acc 0.828125\n",
      "2017-08-02T10:46:16.646402: step 1854, loss 0.491612, acc 0.765625\n",
      "2017-08-02T10:46:17.206458: step 1855, loss 0.49079, acc 0.75\n",
      "2017-08-02T10:46:17.765514: step 1856, loss 0.482437, acc 0.75\n",
      "2017-08-02T10:46:18.323569: step 1857, loss 0.445627, acc 0.828125\n",
      "2017-08-02T10:46:18.881625: step 1858, loss 0.520692, acc 0.796875\n",
      "2017-08-02T10:46:19.437681: step 1859, loss 0.411653, acc 0.84375\n",
      "2017-08-02T10:46:19.996737: step 1860, loss 0.452659, acc 0.765625\n",
      "2017-08-02T10:46:20.552792: step 1861, loss 0.358688, acc 0.875\n",
      "2017-08-02T10:46:21.107848: step 1862, loss 0.580648, acc 0.6875\n",
      "2017-08-02T10:46:21.680905: step 1863, loss 0.550808, acc 0.703125\n",
      "2017-08-02T10:46:22.238961: step 1864, loss 0.34854, acc 0.90625\n",
      "2017-08-02T10:46:22.799017: step 1865, loss 0.405303, acc 0.78125\n",
      "2017-08-02T10:46:23.355073: step 1866, loss 0.419483, acc 0.828125\n",
      "2017-08-02T10:46:23.914128: step 1867, loss 0.432385, acc 0.828125\n",
      "2017-08-02T10:46:24.479185: step 1868, loss 0.475029, acc 0.78125\n",
      "2017-08-02T10:46:25.034240: step 1869, loss 0.395722, acc 0.828125\n",
      "2017-08-02T10:46:25.595297: step 1870, loss 0.536546, acc 0.703125\n",
      "2017-08-02T10:46:26.147352: step 1871, loss 0.46817, acc 0.765625\n",
      "2017-08-02T10:46:26.715409: step 1872, loss 0.527725, acc 0.71875\n",
      "2017-08-02T10:46:27.266464: step 1873, loss 0.368967, acc 0.859375\n",
      "2017-08-02T10:46:27.823519: step 1874, loss 0.613871, acc 0.8125\n",
      "2017-08-02T10:46:28.379575: step 1875, loss 0.493929, acc 0.75\n",
      "2017-08-02T10:46:28.936631: step 1876, loss 0.489193, acc 0.765625\n",
      "2017-08-02T10:46:29.494686: step 1877, loss 0.417784, acc 0.8125\n",
      "2017-08-02T10:46:29.909728: step 1878, loss 0.521996, acc 0.75\n",
      "2017-08-02T10:46:30.472784: step 1879, loss 0.433568, acc 0.828125\n",
      "2017-08-02T10:46:31.027840: step 1880, loss 0.56705, acc 0.75\n",
      "2017-08-02T10:46:31.580895: step 1881, loss 0.368649, acc 0.875\n",
      "2017-08-02T10:46:32.133950: step 1882, loss 0.387961, acc 0.90625\n",
      "2017-08-02T10:46:32.688006: step 1883, loss 0.426112, acc 0.796875\n",
      "2017-08-02T10:46:33.244061: step 1884, loss 0.36291, acc 0.84375\n",
      "2017-08-02T10:46:33.800117: step 1885, loss 0.46371, acc 0.84375\n",
      "2017-08-02T10:46:34.374174: step 1886, loss 0.436946, acc 0.796875\n",
      "2017-08-02T10:46:34.931230: step 1887, loss 0.454969, acc 0.8125\n",
      "2017-08-02T10:46:35.493286: step 1888, loss 0.428924, acc 0.78125\n",
      "2017-08-02T10:46:36.047342: step 1889, loss 0.445873, acc 0.796875\n",
      "2017-08-02T10:46:36.602397: step 1890, loss 0.41683, acc 0.84375\n",
      "2017-08-02T10:46:37.162453: step 1891, loss 0.536535, acc 0.734375\n",
      "2017-08-02T10:46:37.720509: step 1892, loss 0.410821, acc 0.765625\n",
      "2017-08-02T10:46:38.274564: step 1893, loss 0.391582, acc 0.859375\n",
      "2017-08-02T10:46:38.831620: step 1894, loss 0.420586, acc 0.796875\n",
      "2017-08-02T10:46:39.387676: step 1895, loss 0.413121, acc 0.828125\n",
      "2017-08-02T10:46:39.940731: step 1896, loss 0.453453, acc 0.84375\n",
      "2017-08-02T10:46:40.497787: step 1897, loss 0.39187, acc 0.84375\n",
      "2017-08-02T10:46:41.054842: step 1898, loss 0.479374, acc 0.828125\n",
      "2017-08-02T10:46:41.613898: step 1899, loss 0.448467, acc 0.8125\n",
      "2017-08-02T10:46:42.169954: step 1900, loss 0.35669, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:46:45.813318: step 1900, loss 0.447796, acc 0.814\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-1900\n",
      "\n",
      "2017-08-02T10:46:48.367574: step 1901, loss 0.507227, acc 0.765625\n",
      "2017-08-02T10:46:48.925629: step 1902, loss 0.365548, acc 0.890625\n",
      "2017-08-02T10:46:49.482685: step 1903, loss 0.30957, acc 0.90625\n",
      "2017-08-02T10:46:50.045741: step 1904, loss 0.30121, acc 0.84375\n",
      "2017-08-02T10:46:50.604797: step 1905, loss 0.353046, acc 0.890625\n",
      "2017-08-02T10:46:51.158853: step 1906, loss 0.402283, acc 0.890625\n",
      "2017-08-02T10:46:51.713908: step 1907, loss 0.421807, acc 0.78125\n",
      "2017-08-02T10:46:52.272964: step 1908, loss 0.429918, acc 0.796875\n",
      "2017-08-02T10:46:52.830020: step 1909, loss 0.338444, acc 0.875\n",
      "2017-08-02T10:46:53.388076: step 1910, loss 0.445407, acc 0.796875\n",
      "2017-08-02T10:46:53.941131: step 1911, loss 0.308414, acc 0.875\n",
      "2017-08-02T10:46:54.497186: step 1912, loss 0.308641, acc 0.9375\n",
      "2017-08-02T10:46:55.054242: step 1913, loss 0.459302, acc 0.8125\n",
      "2017-08-02T10:46:55.611298: step 1914, loss 0.363602, acc 0.859375\n",
      "2017-08-02T10:46:56.164353: step 1915, loss 0.384269, acc 0.859375\n",
      "2017-08-02T10:46:56.722409: step 1916, loss 0.359696, acc 0.84375\n",
      "2017-08-02T10:46:57.283465: step 1917, loss 0.464459, acc 0.78125\n",
      "2017-08-02T10:46:57.844521: step 1918, loss 0.335693, acc 0.859375\n",
      "2017-08-02T10:46:58.399577: step 1919, loss 0.428097, acc 0.875\n",
      "2017-08-02T10:46:58.953632: step 1920, loss 0.327166, acc 0.859375\n",
      "2017-08-02T10:46:59.513688: step 1921, loss 0.474576, acc 0.796875\n",
      "2017-08-02T10:47:00.068744: step 1922, loss 0.406055, acc 0.8125\n",
      "2017-08-02T10:47:00.626799: step 1923, loss 0.369641, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:47:01.180855: step 1924, loss 0.412709, acc 0.828125\n",
      "2017-08-02T10:47:01.733910: step 1925, loss 0.386918, acc 0.859375\n",
      "2017-08-02T10:47:02.286965: step 1926, loss 0.529326, acc 0.734375\n",
      "2017-08-02T10:47:02.839021: step 1927, loss 0.386563, acc 0.84375\n",
      "2017-08-02T10:47:03.397076: step 1928, loss 0.450833, acc 0.8125\n",
      "2017-08-02T10:47:03.951132: step 1929, loss 0.431626, acc 0.84375\n",
      "2017-08-02T10:47:04.514188: step 1930, loss 0.355339, acc 0.859375\n",
      "2017-08-02T10:47:05.076244: step 1931, loss 0.432939, acc 0.796875\n",
      "2017-08-02T10:47:05.640301: step 1932, loss 0.449592, acc 0.78125\n",
      "2017-08-02T10:47:06.201357: step 1933, loss 0.412103, acc 0.84375\n",
      "2017-08-02T10:47:06.763413: step 1934, loss 0.494371, acc 0.765625\n",
      "2017-08-02T10:47:07.320469: step 1935, loss 0.502128, acc 0.71875\n",
      "2017-08-02T10:47:07.876524: step 1936, loss 0.34637, acc 0.84375\n",
      "2017-08-02T10:47:08.434580: step 1937, loss 0.400733, acc 0.828125\n",
      "2017-08-02T10:47:08.992636: step 1938, loss 0.340244, acc 0.84375\n",
      "2017-08-02T10:47:09.547691: step 1939, loss 0.311294, acc 0.859375\n",
      "2017-08-02T10:47:10.104747: step 1940, loss 0.411106, acc 0.796875\n",
      "2017-08-02T10:47:10.665803: step 1941, loss 0.483197, acc 0.75\n",
      "2017-08-02T10:47:11.228859: step 1942, loss 0.547097, acc 0.765625\n",
      "2017-08-02T10:47:11.790916: step 1943, loss 0.385635, acc 0.859375\n",
      "2017-08-02T10:47:12.355972: step 1944, loss 0.499304, acc 0.78125\n",
      "2017-08-02T10:47:12.945031: step 1945, loss 0.364829, acc 0.84375\n",
      "2017-08-02T10:47:13.538090: step 1946, loss 0.489691, acc 0.796875\n",
      "2017-08-02T10:47:14.092146: step 1947, loss 0.588238, acc 0.71875\n",
      "2017-08-02T10:47:14.650202: step 1948, loss 0.469605, acc 0.78125\n",
      "2017-08-02T10:47:15.229259: step 1949, loss 0.369598, acc 0.84375\n",
      "2017-08-02T10:47:15.794316: step 1950, loss 0.440604, acc 0.796875\n",
      "2017-08-02T10:47:16.351372: step 1951, loss 0.475901, acc 0.75\n",
      "2017-08-02T10:47:16.914428: step 1952, loss 0.5759, acc 0.734375\n",
      "2017-08-02T10:47:17.473484: step 1953, loss 0.413579, acc 0.890625\n",
      "2017-08-02T10:47:18.026539: step 1954, loss 0.359139, acc 0.84375\n",
      "2017-08-02T10:47:18.578594: step 1955, loss 0.369582, acc 0.8125\n",
      "2017-08-02T10:47:19.128649: step 1956, loss 0.441299, acc 0.78125\n",
      "2017-08-02T10:47:19.684705: step 1957, loss 0.514963, acc 0.71875\n",
      "2017-08-02T10:47:20.239760: step 1958, loss 0.466669, acc 0.828125\n",
      "2017-08-02T10:47:20.795816: step 1959, loss 0.45449, acc 0.875\n",
      "2017-08-02T10:47:21.350872: step 1960, loss 0.466086, acc 0.765625\n",
      "2017-08-02T10:47:21.922929: step 1961, loss 0.615387, acc 0.734375\n",
      "2017-08-02T10:47:22.493986: step 1962, loss 0.473367, acc 0.828125\n",
      "2017-08-02T10:47:23.049041: step 1963, loss 0.463634, acc 0.796875\n",
      "2017-08-02T10:47:23.605097: step 1964, loss 0.43659, acc 0.8125\n",
      "2017-08-02T10:47:24.167153: step 1965, loss 0.323854, acc 0.875\n",
      "2017-08-02T10:47:24.721209: step 1966, loss 0.373364, acc 0.84375\n",
      "2017-08-02T10:47:25.272264: step 1967, loss 0.357926, acc 0.859375\n",
      "2017-08-02T10:47:25.833320: step 1968, loss 0.387994, acc 0.875\n",
      "2017-08-02T10:47:26.390375: step 1969, loss 0.488342, acc 0.75\n",
      "2017-08-02T10:47:26.944431: step 1970, loss 0.367195, acc 0.828125\n",
      "2017-08-02T10:47:27.498486: step 1971, loss 0.41029, acc 0.828125\n",
      "2017-08-02T10:47:28.055542: step 1972, loss 0.376537, acc 0.84375\n",
      "2017-08-02T10:47:28.615598: step 1973, loss 0.369632, acc 0.84375\n",
      "2017-08-02T10:47:29.170653: step 1974, loss 0.45646, acc 0.796875\n",
      "2017-08-02T10:47:29.725709: step 1975, loss 0.463059, acc 0.828125\n",
      "2017-08-02T10:47:30.281765: step 1976, loss 0.501163, acc 0.78125\n",
      "2017-08-02T10:47:30.834820: step 1977, loss 0.402717, acc 0.828125\n",
      "2017-08-02T10:47:31.383875: step 1978, loss 0.448063, acc 0.78125\n",
      "2017-08-02T10:47:31.934930: step 1979, loss 0.382258, acc 0.84375\n",
      "2017-08-02T10:47:32.489985: step 1980, loss 0.483309, acc 0.796875\n",
      "2017-08-02T10:47:33.046041: step 1981, loss 0.42987, acc 0.8125\n",
      "2017-08-02T10:47:33.607097: step 1982, loss 0.411882, acc 0.8125\n",
      "2017-08-02T10:47:34.165153: step 1983, loss 0.479533, acc 0.8125\n",
      "2017-08-02T10:47:34.719208: step 1984, loss 0.376548, acc 0.828125\n",
      "2017-08-02T10:47:35.275264: step 1985, loss 0.396615, acc 0.8125\n",
      "2017-08-02T10:47:35.830319: step 1986, loss 0.366331, acc 0.859375\n",
      "2017-08-02T10:47:36.380374: step 1987, loss 0.317857, acc 0.890625\n",
      "2017-08-02T10:47:36.936430: step 1988, loss 0.509372, acc 0.75\n",
      "2017-08-02T10:47:37.495486: step 1989, loss 0.36597, acc 0.875\n",
      "2017-08-02T10:47:38.045541: step 1990, loss 0.430024, acc 0.765625\n",
      "2017-08-02T10:47:38.600596: step 1991, loss 0.44642, acc 0.890625\n",
      "2017-08-02T10:47:39.160652: step 1992, loss 0.59427, acc 0.71875\n",
      "2017-08-02T10:47:39.712708: step 1993, loss 0.289294, acc 0.890625\n",
      "2017-08-02T10:47:40.267763: step 1994, loss 0.393236, acc 0.84375\n",
      "2017-08-02T10:47:40.824819: step 1995, loss 0.323474, acc 0.890625\n",
      "2017-08-02T10:47:41.380874: step 1996, loss 0.479461, acc 0.796875\n",
      "2017-08-02T10:47:41.938930: step 1997, loss 0.504631, acc 0.75\n",
      "2017-08-02T10:47:42.499986: step 1998, loss 0.305708, acc 0.921875\n",
      "2017-08-02T10:47:43.049041: step 1999, loss 0.411357, acc 0.859375\n",
      "2017-08-02T10:47:43.606097: step 2000, loss 0.306693, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:47:47.245461: step 2000, loss 0.442873, acc 0.8176\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2000\n",
      "\n",
      "2017-08-02T10:47:49.989735: step 2001, loss 0.518338, acc 0.78125\n",
      "2017-08-02T10:47:50.540790: step 2002, loss 0.437234, acc 0.796875\n",
      "2017-08-02T10:47:51.105847: step 2003, loss 0.325096, acc 0.890625\n",
      "2017-08-02T10:47:51.662902: step 2004, loss 0.387295, acc 0.828125\n",
      "2017-08-02T10:47:52.216958: step 2005, loss 0.496988, acc 0.828125\n",
      "2017-08-02T10:47:52.775014: step 2006, loss 0.428082, acc 0.828125\n",
      "2017-08-02T10:47:53.331069: step 2007, loss 0.421416, acc 0.828125\n",
      "2017-08-02T10:47:53.890125: step 2008, loss 0.446367, acc 0.765625\n",
      "2017-08-02T10:47:54.466183: step 2009, loss 0.391116, acc 0.875\n",
      "2017-08-02T10:47:55.023238: step 2010, loss 0.429636, acc 0.796875\n",
      "2017-08-02T10:47:55.579294: step 2011, loss 0.335864, acc 0.890625\n",
      "2017-08-02T10:47:56.133349: step 2012, loss 0.405817, acc 0.84375\n",
      "2017-08-02T10:47:56.696406: step 2013, loss 0.412145, acc 0.828125\n",
      "2017-08-02T10:47:57.248461: step 2014, loss 0.476808, acc 0.75\n",
      "2017-08-02T10:47:57.806517: step 2015, loss 0.394385, acc 0.84375\n",
      "2017-08-02T10:47:58.370573: step 2016, loss 0.386045, acc 0.875\n",
      "2017-08-02T10:47:58.924629: step 2017, loss 0.35807, acc 0.828125\n",
      "2017-08-02T10:47:59.477684: step 2018, loss 0.546334, acc 0.75\n",
      "2017-08-02T10:48:00.029739: step 2019, loss 0.393531, acc 0.890625\n",
      "2017-08-02T10:48:00.588795: step 2020, loss 0.374217, acc 0.828125\n",
      "2017-08-02T10:48:01.147851: step 2021, loss 0.50839, acc 0.75\n",
      "2017-08-02T10:48:01.705907: step 2022, loss 0.43492, acc 0.84375\n",
      "2017-08-02T10:48:02.264963: step 2023, loss 0.401707, acc 0.796875\n",
      "2017-08-02T10:48:02.821018: step 2024, loss 0.366305, acc 0.84375\n",
      "2017-08-02T10:48:03.377074: step 2025, loss 0.325893, acc 0.890625\n",
      "2017-08-02T10:48:03.931129: step 2026, loss 0.402489, acc 0.78125\n",
      "2017-08-02T10:48:04.489185: step 2027, loss 0.451937, acc 0.796875\n",
      "2017-08-02T10:48:05.049241: step 2028, loss 0.390543, acc 0.828125\n",
      "2017-08-02T10:48:05.603296: step 2029, loss 0.400065, acc 0.8125\n",
      "2017-08-02T10:48:06.161352: step 2030, loss 0.456527, acc 0.78125\n",
      "2017-08-02T10:48:06.732409: step 2031, loss 0.503277, acc 0.8125\n",
      "2017-08-02T10:48:07.294465: step 2032, loss 0.399864, acc 0.84375\n",
      "2017-08-02T10:48:07.852521: step 2033, loss 0.499157, acc 0.828125\n",
      "2017-08-02T10:48:08.411577: step 2034, loss 0.41552, acc 0.828125\n",
      "2017-08-02T10:48:08.971633: step 2035, loss 0.317235, acc 0.84375\n",
      "2017-08-02T10:48:09.531689: step 2036, loss 0.374808, acc 0.8125\n",
      "2017-08-02T10:48:10.084744: step 2037, loss 0.406602, acc 0.859375\n",
      "2017-08-02T10:48:10.644800: step 2038, loss 0.385297, acc 0.859375\n",
      "2017-08-02T10:48:11.201856: step 2039, loss 0.36016, acc 0.859375\n",
      "2017-08-02T10:48:11.761912: step 2040, loss 0.416284, acc 0.84375\n",
      "2017-08-02T10:48:12.314967: step 2041, loss 0.502515, acc 0.75\n",
      "2017-08-02T10:48:12.872023: step 2042, loss 0.557138, acc 0.765625\n",
      "2017-08-02T10:48:13.429079: step 2043, loss 0.396206, acc 0.875\n",
      "2017-08-02T10:48:13.984134: step 2044, loss 0.39282, acc 0.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:48:14.539190: step 2045, loss 0.370189, acc 0.859375\n",
      "2017-08-02T10:48:15.096246: step 2046, loss 0.369932, acc 0.875\n",
      "2017-08-02T10:48:15.651301: step 2047, loss 0.431331, acc 0.828125\n",
      "2017-08-02T10:48:16.207357: step 2048, loss 0.339764, acc 0.875\n",
      "2017-08-02T10:48:16.767413: step 2049, loss 0.379229, acc 0.828125\n",
      "2017-08-02T10:48:17.320468: step 2050, loss 0.375095, acc 0.8125\n",
      "2017-08-02T10:48:17.874523: step 2051, loss 0.359994, acc 0.890625\n",
      "2017-08-02T10:48:18.428579: step 2052, loss 0.410379, acc 0.796875\n",
      "2017-08-02T10:48:18.978634: step 2053, loss 0.449798, acc 0.8125\n",
      "2017-08-02T10:48:19.536690: step 2054, loss 0.29238, acc 0.90625\n",
      "2017-08-02T10:48:20.090745: step 2055, loss 0.574271, acc 0.71875\n",
      "2017-08-02T10:48:20.649801: step 2056, loss 0.307206, acc 0.890625\n",
      "2017-08-02T10:48:21.203856: step 2057, loss 0.402068, acc 0.828125\n",
      "2017-08-02T10:48:21.754911: step 2058, loss 0.312352, acc 0.890625\n",
      "2017-08-02T10:48:22.313967: step 2059, loss 0.452491, acc 0.8125\n",
      "2017-08-02T10:48:22.893025: step 2060, loss 0.499451, acc 0.78125\n",
      "2017-08-02T10:48:23.465082: step 2061, loss 0.490293, acc 0.734375\n",
      "2017-08-02T10:48:24.021138: step 2062, loss 0.383156, acc 0.828125\n",
      "2017-08-02T10:48:24.573193: step 2063, loss 0.440998, acc 0.796875\n",
      "2017-08-02T10:48:25.126248: step 2064, loss 0.43802, acc 0.828125\n",
      "2017-08-02T10:48:25.683304: step 2065, loss 0.441388, acc 0.84375\n",
      "2017-08-02T10:48:26.242360: step 2066, loss 0.560691, acc 0.8125\n",
      "2017-08-02T10:48:26.805416: step 2067, loss 0.620856, acc 0.71875\n",
      "2017-08-02T10:48:27.359472: step 2068, loss 0.366502, acc 0.890625\n",
      "2017-08-02T10:48:27.934529: step 2069, loss 0.403825, acc 0.859375\n",
      "2017-08-02T10:48:28.491585: step 2070, loss 0.395914, acc 0.796875\n",
      "2017-08-02T10:48:29.047641: step 2071, loss 0.463213, acc 0.765625\n",
      "2017-08-02T10:48:29.606696: step 2072, loss 0.60439, acc 0.703125\n",
      "2017-08-02T10:48:30.160752: step 2073, loss 0.448885, acc 0.765625\n",
      "2017-08-02T10:48:30.716807: step 2074, loss 0.438506, acc 0.765625\n",
      "2017-08-02T10:48:31.270863: step 2075, loss 0.432294, acc 0.8125\n",
      "2017-08-02T10:48:31.820918: step 2076, loss 0.480603, acc 0.8125\n",
      "2017-08-02T10:48:32.373973: step 2077, loss 0.576023, acc 0.765625\n",
      "2017-08-02T10:48:32.929029: step 2078, loss 0.447709, acc 0.8125\n",
      "2017-08-02T10:48:33.483084: step 2079, loss 0.361106, acc 0.859375\n",
      "2017-08-02T10:48:34.037139: step 2080, loss 0.431425, acc 0.796875\n",
      "2017-08-02T10:48:34.590195: step 2081, loss 0.363677, acc 0.859375\n",
      "2017-08-02T10:48:35.147250: step 2082, loss 0.426928, acc 0.796875\n",
      "2017-08-02T10:48:35.698306: step 2083, loss 0.432172, acc 0.8125\n",
      "2017-08-02T10:48:36.253361: step 2084, loss 0.394964, acc 0.8125\n",
      "2017-08-02T10:48:36.813417: step 2085, loss 0.405318, acc 0.8125\n",
      "2017-08-02T10:48:37.366472: step 2086, loss 0.395459, acc 0.8125\n",
      "2017-08-02T10:48:37.917527: step 2087, loss 0.353993, acc 0.859375\n",
      "2017-08-02T10:48:38.466582: step 2088, loss 0.422635, acc 0.828125\n",
      "2017-08-02T10:48:39.019638: step 2089, loss 0.616983, acc 0.8125\n",
      "2017-08-02T10:48:39.567692: step 2090, loss 0.455324, acc 0.828125\n",
      "2017-08-02T10:48:40.121748: step 2091, loss 0.313535, acc 0.9375\n",
      "2017-08-02T10:48:40.678804: step 2092, loss 0.395636, acc 0.828125\n",
      "2017-08-02T10:48:41.234859: step 2093, loss 0.391909, acc 0.796875\n",
      "2017-08-02T10:48:41.792915: step 2094, loss 0.370996, acc 0.890625\n",
      "2017-08-02T10:48:42.349971: step 2095, loss 0.383299, acc 0.84375\n",
      "2017-08-02T10:48:42.903026: step 2096, loss 0.438549, acc 0.84375\n",
      "2017-08-02T10:48:43.457081: step 2097, loss 0.505702, acc 0.765625\n",
      "2017-08-02T10:48:44.009137: step 2098, loss 0.421186, acc 0.828125\n",
      "2017-08-02T10:48:44.559192: step 2099, loss 0.374879, acc 0.84375\n",
      "2017-08-02T10:48:45.115247: step 2100, loss 0.477081, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:48:48.740610: step 2100, loss 0.505552, acc 0.7728\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2100\n",
      "\n",
      "2017-08-02T10:48:51.330869: step 2101, loss 0.637817, acc 0.796875\n",
      "2017-08-02T10:48:51.892925: step 2102, loss 0.399196, acc 0.796875\n",
      "2017-08-02T10:48:52.443980: step 2103, loss 0.377057, acc 0.8125\n",
      "2017-08-02T10:48:52.997035: step 2104, loss 0.395144, acc 0.828125\n",
      "2017-08-02T10:48:53.551091: step 2105, loss 0.41006, acc 0.828125\n",
      "2017-08-02T10:48:54.116147: step 2106, loss 0.390002, acc 0.8125\n",
      "2017-08-02T10:48:54.708206: step 2107, loss 0.555361, acc 0.796875\n",
      "2017-08-02T10:48:55.266262: step 2108, loss 0.352458, acc 0.875\n",
      "2017-08-02T10:48:55.823318: step 2109, loss 0.435921, acc 0.84375\n",
      "2017-08-02T10:48:56.385374: step 2110, loss 0.374201, acc 0.90625\n",
      "2017-08-02T10:48:56.946430: step 2111, loss 0.471421, acc 0.8125\n",
      "2017-08-02T10:48:57.523488: step 2112, loss 0.357346, acc 0.921875\n",
      "2017-08-02T10:48:58.077543: step 2113, loss 0.428332, acc 0.796875\n",
      "2017-08-02T10:48:58.636599: step 2114, loss 0.53411, acc 0.8125\n",
      "2017-08-02T10:48:59.197655: step 2115, loss 0.490748, acc 0.78125\n",
      "2017-08-02T10:48:59.759711: step 2116, loss 0.381506, acc 0.875\n",
      "2017-08-02T10:49:00.318767: step 2117, loss 0.323119, acc 0.859375\n",
      "2017-08-02T10:49:00.878823: step 2118, loss 0.508737, acc 0.765625\n",
      "2017-08-02T10:49:01.436879: step 2119, loss 0.416386, acc 0.78125\n",
      "2017-08-02T10:49:01.994935: step 2120, loss 0.477071, acc 0.828125\n",
      "2017-08-02T10:49:02.549990: step 2121, loss 0.314872, acc 0.890625\n",
      "2017-08-02T10:49:03.105046: step 2122, loss 0.33595, acc 0.84375\n",
      "2017-08-02T10:49:03.659101: step 2123, loss 0.525833, acc 0.75\n",
      "2017-08-02T10:49:04.215157: step 2124, loss 0.390413, acc 0.859375\n",
      "2017-08-02T10:49:04.785214: step 2125, loss 0.393948, acc 0.84375\n",
      "2017-08-02T10:49:05.345270: step 2126, loss 0.301989, acc 0.890625\n",
      "2017-08-02T10:49:05.905326: step 2127, loss 0.41581, acc 0.828125\n",
      "2017-08-02T10:49:06.487384: step 2128, loss 0.393234, acc 0.828125\n",
      "2017-08-02T10:49:07.053441: step 2129, loss 0.317895, acc 0.859375\n",
      "2017-08-02T10:49:07.618497: step 2130, loss 0.455247, acc 0.78125\n",
      "2017-08-02T10:49:08.182554: step 2131, loss 0.379126, acc 0.84375\n",
      "2017-08-02T10:49:08.741610: step 2132, loss 0.405474, acc 0.84375\n",
      "2017-08-02T10:49:09.303666: step 2133, loss 0.345625, acc 0.90625\n",
      "2017-08-02T10:49:09.877723: step 2134, loss 0.383835, acc 0.84375\n",
      "2017-08-02T10:49:10.439779: step 2135, loss 0.347943, acc 0.828125\n",
      "2017-08-02T10:49:11.000835: step 2136, loss 0.45061, acc 0.828125\n",
      "2017-08-02T10:49:11.576893: step 2137, loss 0.390837, acc 0.8125\n",
      "2017-08-02T10:49:12.131949: step 2138, loss 0.356713, acc 0.84375\n",
      "2017-08-02T10:49:12.690004: step 2139, loss 0.469826, acc 0.78125\n",
      "2017-08-02T10:49:13.250060: step 2140, loss 0.507431, acc 0.71875\n",
      "2017-08-02T10:49:13.811116: step 2141, loss 0.394333, acc 0.828125\n",
      "2017-08-02T10:49:14.371172: step 2142, loss 0.415926, acc 0.78125\n",
      "2017-08-02T10:49:14.932229: step 2143, loss 0.3585, acc 0.84375\n",
      "2017-08-02T10:49:15.485284: step 2144, loss 0.530698, acc 0.78125\n",
      "2017-08-02T10:49:16.044340: step 2145, loss 0.429873, acc 0.78125\n",
      "2017-08-02T10:49:16.612397: step 2146, loss 0.417045, acc 0.796875\n",
      "2017-08-02T10:49:17.166452: step 2147, loss 0.428009, acc 0.875\n",
      "2017-08-02T10:49:17.723508: step 2148, loss 0.364742, acc 0.859375\n",
      "2017-08-02T10:49:18.277563: step 2149, loss 0.491297, acc 0.765625\n",
      "2017-08-02T10:49:18.842620: step 2150, loss 0.442585, acc 0.796875\n",
      "2017-08-02T10:49:19.404676: step 2151, loss 0.455461, acc 0.796875\n",
      "2017-08-02T10:49:19.963732: step 2152, loss 0.514456, acc 0.78125\n",
      "2017-08-02T10:49:20.524788: step 2153, loss 0.511537, acc 0.75\n",
      "2017-08-02T10:49:21.083844: step 2154, loss 0.372089, acc 0.859375\n",
      "2017-08-02T10:49:21.642900: step 2155, loss 0.354441, acc 0.84375\n",
      "2017-08-02T10:49:22.200955: step 2156, loss 0.243883, acc 0.96875\n",
      "2017-08-02T10:49:22.759011: step 2157, loss 0.475977, acc 0.78125\n",
      "2017-08-02T10:49:23.314067: step 2158, loss 0.43518, acc 0.828125\n",
      "2017-08-02T10:49:23.905126: step 2159, loss 0.40776, acc 0.84375\n",
      "2017-08-02T10:49:24.467182: step 2160, loss 0.508878, acc 0.8125\n",
      "2017-08-02T10:49:25.022237: step 2161, loss 0.632513, acc 0.734375\n",
      "2017-08-02T10:49:25.578293: step 2162, loss 0.378278, acc 0.84375\n",
      "2017-08-02T10:49:26.137349: step 2163, loss 0.446604, acc 0.765625\n",
      "2017-08-02T10:49:26.700405: step 2164, loss 0.383455, acc 0.828125\n",
      "2017-08-02T10:49:27.264462: step 2165, loss 0.386915, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:49:27.827518: step 2166, loss 0.347544, acc 0.84375\n",
      "2017-08-02T10:49:28.386574: step 2167, loss 0.415521, acc 0.875\n",
      "2017-08-02T10:49:28.944630: step 2168, loss 0.395123, acc 0.828125\n",
      "2017-08-02T10:49:29.501685: step 2169, loss 0.541159, acc 0.796875\n",
      "2017-08-02T10:49:30.057741: step 2170, loss 0.396753, acc 0.8125\n",
      "2017-08-02T10:49:30.613797: step 2171, loss 0.538189, acc 0.828125\n",
      "2017-08-02T10:49:31.169852: step 2172, loss 0.281936, acc 0.890625\n",
      "2017-08-02T10:49:31.724908: step 2173, loss 0.389749, acc 0.78125\n",
      "2017-08-02T10:49:32.280963: step 2174, loss 0.330228, acc 0.859375\n",
      "2017-08-02T10:49:32.840019: step 2175, loss 0.335485, acc 0.875\n",
      "2017-08-02T10:49:33.413076: step 2176, loss 0.424596, acc 0.8125\n",
      "2017-08-02T10:49:33.972132: step 2177, loss 0.384424, acc 0.828125\n",
      "2017-08-02T10:49:34.549190: step 2178, loss 0.241705, acc 0.9375\n",
      "2017-08-02T10:49:35.107246: step 2179, loss 0.421467, acc 0.828125\n",
      "2017-08-02T10:49:35.662301: step 2180, loss 0.408262, acc 0.859375\n",
      "2017-08-02T10:49:36.220357: step 2181, loss 0.472151, acc 0.78125\n",
      "2017-08-02T10:49:36.774413: step 2182, loss 0.266947, acc 0.921875\n",
      "2017-08-02T10:49:37.331468: step 2183, loss 0.378833, acc 0.859375\n",
      "2017-08-02T10:49:37.891524: step 2184, loss 0.475559, acc 0.828125\n",
      "2017-08-02T10:49:38.443579: step 2185, loss 0.29299, acc 0.890625\n",
      "2017-08-02T10:49:39.012636: step 2186, loss 0.379171, acc 0.828125\n",
      "2017-08-02T10:49:39.566692: step 2187, loss 0.497141, acc 0.796875\n",
      "2017-08-02T10:49:40.123747: step 2188, loss 0.28016, acc 0.890625\n",
      "2017-08-02T10:49:40.675803: step 2189, loss 0.351213, acc 0.859375\n",
      "2017-08-02T10:49:41.234859: step 2190, loss 0.464493, acc 0.8125\n",
      "2017-08-02T10:49:41.648900: step 2191, loss 0.218771, acc 0.96875\n",
      "2017-08-02T10:49:42.205956: step 2192, loss 0.351187, acc 0.890625\n",
      "2017-08-02T10:49:42.760011: step 2193, loss 0.308153, acc 0.875\n",
      "2017-08-02T10:49:43.313066: step 2194, loss 0.40409, acc 0.8125\n",
      "2017-08-02T10:49:43.869122: step 2195, loss 0.438909, acc 0.84375\n",
      "2017-08-02T10:49:44.433178: step 2196, loss 0.275983, acc 0.890625\n",
      "2017-08-02T10:49:44.993234: step 2197, loss 0.34152, acc 0.90625\n",
      "2017-08-02T10:49:45.543289: step 2198, loss 0.406004, acc 0.828125\n",
      "2017-08-02T10:49:46.099345: step 2199, loss 0.296832, acc 0.859375\n",
      "2017-08-02T10:49:46.663401: step 2200, loss 0.379546, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:49:50.365772: step 2200, loss 0.419031, acc 0.822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2200\n",
      "\n",
      "2017-08-02T10:49:53.248060: step 2201, loss 0.40199, acc 0.875\n",
      "2017-08-02T10:49:53.814116: step 2202, loss 0.407131, acc 0.8125\n",
      "2017-08-02T10:49:54.380173: step 2203, loss 0.443919, acc 0.75\n",
      "2017-08-02T10:49:54.965231: step 2204, loss 0.349699, acc 0.84375\n",
      "2017-08-02T10:49:55.542289: step 2205, loss 0.432717, acc 0.828125\n",
      "2017-08-02T10:49:56.130348: step 2206, loss 0.442942, acc 0.75\n",
      "2017-08-02T10:49:56.721407: step 2207, loss 0.301656, acc 0.890625\n",
      "2017-08-02T10:49:57.301465: step 2208, loss 0.337179, acc 0.875\n",
      "2017-08-02T10:49:57.859521: step 2209, loss 0.388208, acc 0.890625\n",
      "2017-08-02T10:49:58.418577: step 2210, loss 0.346841, acc 0.859375\n",
      "2017-08-02T10:49:58.981633: step 2211, loss 0.366032, acc 0.875\n",
      "2017-08-02T10:49:59.546690: step 2212, loss 0.441851, acc 0.796875\n",
      "2017-08-02T10:50:00.101745: step 2213, loss 0.380125, acc 0.828125\n",
      "2017-08-02T10:50:00.657801: step 2214, loss 0.352619, acc 0.890625\n",
      "2017-08-02T10:50:01.222857: step 2215, loss 0.330982, acc 0.84375\n",
      "2017-08-02T10:50:01.786914: step 2216, loss 0.453507, acc 0.8125\n",
      "2017-08-02T10:50:02.355970: step 2217, loss 0.250208, acc 0.9375\n",
      "2017-08-02T10:50:02.923027: step 2218, loss 0.302607, acc 0.875\n",
      "2017-08-02T10:50:03.493084: step 2219, loss 0.565807, acc 0.734375\n",
      "2017-08-02T10:50:04.061141: step 2220, loss 0.44256, acc 0.84375\n",
      "2017-08-02T10:50:04.628198: step 2221, loss 0.324572, acc 0.9375\n",
      "2017-08-02T10:50:05.201255: step 2222, loss 0.281475, acc 0.90625\n",
      "2017-08-02T10:50:05.770312: step 2223, loss 0.295974, acc 0.90625\n",
      "2017-08-02T10:50:06.336368: step 2224, loss 0.351794, acc 0.84375\n",
      "2017-08-02T10:50:06.927428: step 2225, loss 0.367539, acc 0.84375\n",
      "2017-08-02T10:50:07.497485: step 2226, loss 0.621293, acc 0.765625\n",
      "2017-08-02T10:50:08.064541: step 2227, loss 0.368158, acc 0.859375\n",
      "2017-08-02T10:50:08.636598: step 2228, loss 0.345106, acc 0.875\n",
      "2017-08-02T10:50:09.209656: step 2229, loss 0.361349, acc 0.84375\n",
      "2017-08-02T10:50:09.763711: step 2230, loss 0.402736, acc 0.875\n",
      "2017-08-02T10:50:10.330768: step 2231, loss 0.370812, acc 0.8125\n",
      "2017-08-02T10:50:10.889824: step 2232, loss 0.421958, acc 0.875\n",
      "2017-08-02T10:50:11.473882: step 2233, loss 0.322799, acc 0.875\n",
      "2017-08-02T10:50:12.032938: step 2234, loss 0.291515, acc 0.890625\n",
      "2017-08-02T10:50:12.596994: step 2235, loss 0.350049, acc 0.859375\n",
      "2017-08-02T10:50:13.153050: step 2236, loss 0.274792, acc 0.921875\n",
      "2017-08-02T10:50:13.709106: step 2237, loss 0.420431, acc 0.84375\n",
      "2017-08-02T10:50:14.275162: step 2238, loss 0.356904, acc 0.875\n",
      "2017-08-02T10:50:14.836218: step 2239, loss 0.324041, acc 0.90625\n",
      "2017-08-02T10:50:15.389274: step 2240, loss 0.431736, acc 0.859375\n",
      "2017-08-02T10:50:15.945329: step 2241, loss 0.280139, acc 0.9375\n",
      "2017-08-02T10:50:16.501385: step 2242, loss 0.397245, acc 0.875\n",
      "2017-08-02T10:50:17.058441: step 2243, loss 0.413054, acc 0.84375\n",
      "2017-08-02T10:50:17.617496: step 2244, loss 0.459915, acc 0.78125\n",
      "2017-08-02T10:50:18.203555: step 2245, loss 0.310456, acc 0.875\n",
      "2017-08-02T10:50:18.799615: step 2246, loss 0.364802, acc 0.875\n",
      "2017-08-02T10:50:19.384673: step 2247, loss 0.291445, acc 0.90625\n",
      "2017-08-02T10:50:19.956730: step 2248, loss 0.393954, acc 0.875\n",
      "2017-08-02T10:50:20.531788: step 2249, loss 0.28281, acc 0.859375\n",
      "2017-08-02T10:50:21.130848: step 2250, loss 0.303835, acc 0.90625\n",
      "2017-08-02T10:50:21.719907: step 2251, loss 0.430102, acc 0.8125\n",
      "2017-08-02T10:50:22.310966: step 2252, loss 0.35985, acc 0.875\n",
      "2017-08-02T10:50:22.877022: step 2253, loss 0.380915, acc 0.84375\n",
      "2017-08-02T10:50:23.445079: step 2254, loss 0.364089, acc 0.875\n",
      "2017-08-02T10:50:24.016136: step 2255, loss 0.358165, acc 0.828125\n",
      "2017-08-02T10:50:24.597194: step 2256, loss 0.408785, acc 0.859375\n",
      "2017-08-02T10:50:25.171252: step 2257, loss 0.371594, acc 0.859375\n",
      "2017-08-02T10:50:25.744309: step 2258, loss 0.388871, acc 0.859375\n",
      "2017-08-02T10:50:26.310366: step 2259, loss 0.34483, acc 0.890625\n",
      "2017-08-02T10:50:26.876422: step 2260, loss 0.325626, acc 0.90625\n",
      "2017-08-02T10:50:27.436478: step 2261, loss 0.364434, acc 0.84375\n",
      "2017-08-02T10:50:28.008535: step 2262, loss 0.381005, acc 0.84375\n",
      "2017-08-02T10:50:28.580593: step 2263, loss 0.249521, acc 0.921875\n",
      "2017-08-02T10:50:29.155650: step 2264, loss 0.338417, acc 0.890625\n",
      "2017-08-02T10:50:29.729708: step 2265, loss 0.370404, acc 0.84375\n",
      "2017-08-02T10:50:30.299765: step 2266, loss 0.407886, acc 0.84375\n",
      "2017-08-02T10:50:30.875822: step 2267, loss 0.278521, acc 0.890625\n",
      "2017-08-02T10:50:31.439879: step 2268, loss 0.263566, acc 0.953125\n",
      "2017-08-02T10:50:31.996934: step 2269, loss 0.343944, acc 0.921875\n",
      "2017-08-02T10:50:32.554990: step 2270, loss 0.355669, acc 0.8125\n",
      "2017-08-02T10:50:33.113046: step 2271, loss 0.314421, acc 0.875\n",
      "2017-08-02T10:50:33.677102: step 2272, loss 0.397444, acc 0.828125\n",
      "2017-08-02T10:50:34.238158: step 2273, loss 0.383663, acc 0.875\n",
      "2017-08-02T10:50:34.816216: step 2274, loss 0.388061, acc 0.828125\n",
      "2017-08-02T10:50:35.374272: step 2275, loss 0.460449, acc 0.78125\n",
      "2017-08-02T10:50:35.933328: step 2276, loss 0.301968, acc 0.9375\n",
      "2017-08-02T10:50:36.490384: step 2277, loss 0.272022, acc 0.9375\n",
      "2017-08-02T10:50:37.051440: step 2278, loss 0.375548, acc 0.84375\n",
      "2017-08-02T10:50:37.611496: step 2279, loss 0.39792, acc 0.8125\n",
      "2017-08-02T10:50:38.170552: step 2280, loss 0.335432, acc 0.90625\n",
      "2017-08-02T10:50:38.731608: step 2281, loss 0.427647, acc 0.765625\n",
      "2017-08-02T10:50:39.295664: step 2282, loss 0.361687, acc 0.890625\n",
      "2017-08-02T10:50:39.852720: step 2283, loss 0.278436, acc 0.921875\n",
      "2017-08-02T10:50:40.412776: step 2284, loss 0.512002, acc 0.78125\n",
      "2017-08-02T10:50:40.970832: step 2285, loss 0.376742, acc 0.859375\n",
      "2017-08-02T10:50:41.530888: step 2286, loss 0.475347, acc 0.765625\n",
      "2017-08-02T10:50:42.093944: step 2287, loss 0.277083, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:50:42.666001: step 2288, loss 0.360497, acc 0.890625\n",
      "2017-08-02T10:50:43.226057: step 2289, loss 0.39993, acc 0.75\n",
      "2017-08-02T10:50:43.782113: step 2290, loss 0.344191, acc 0.84375\n",
      "2017-08-02T10:50:44.335168: step 2291, loss 0.329743, acc 0.90625\n",
      "2017-08-02T10:50:44.893224: step 2292, loss 0.372939, acc 0.84375\n",
      "2017-08-02T10:50:45.458280: step 2293, loss 0.391307, acc 0.828125\n",
      "2017-08-02T10:50:46.014336: step 2294, loss 0.287438, acc 0.90625\n",
      "2017-08-02T10:50:46.570391: step 2295, loss 0.397978, acc 0.828125\n",
      "2017-08-02T10:50:47.126447: step 2296, loss 0.275683, acc 0.953125\n",
      "2017-08-02T10:50:47.686503: step 2297, loss 0.258114, acc 0.921875\n",
      "2017-08-02T10:50:48.242559: step 2298, loss 0.313454, acc 0.90625\n",
      "2017-08-02T10:50:48.798614: step 2299, loss 0.261566, acc 0.9375\n",
      "2017-08-02T10:50:49.360670: step 2300, loss 0.268833, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:50:53.115046: step 2300, loss 0.423544, acc 0.8168\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2300\n",
      "\n",
      "2017-08-02T10:50:55.381272: step 2301, loss 0.384419, acc 0.8125\n",
      "2017-08-02T10:50:55.966331: step 2302, loss 0.288415, acc 0.875\n",
      "2017-08-02T10:50:56.542389: step 2303, loss 0.342035, acc 0.859375\n",
      "2017-08-02T10:50:57.113446: step 2304, loss 0.315985, acc 0.859375\n",
      "2017-08-02T10:50:57.698504: step 2305, loss 0.456273, acc 0.75\n",
      "2017-08-02T10:50:58.271561: step 2306, loss 0.297908, acc 0.90625\n",
      "2017-08-02T10:50:58.855620: step 2307, loss 0.292751, acc 0.84375\n",
      "2017-08-02T10:50:59.452680: step 2308, loss 0.27972, acc 0.921875\n",
      "2017-08-02T10:51:00.026737: step 2309, loss 0.261697, acc 0.90625\n",
      "2017-08-02T10:51:00.589793: step 2310, loss 0.360148, acc 0.859375\n",
      "2017-08-02T10:51:01.148849: step 2311, loss 0.376938, acc 0.8125\n",
      "2017-08-02T10:51:01.707905: step 2312, loss 0.437446, acc 0.828125\n",
      "2017-08-02T10:51:02.264961: step 2313, loss 0.24922, acc 0.90625\n",
      "2017-08-02T10:51:02.836018: step 2314, loss 0.260542, acc 0.921875\n",
      "2017-08-02T10:51:03.411075: step 2315, loss 0.360832, acc 0.890625\n",
      "2017-08-02T10:51:03.979132: step 2316, loss 0.38865, acc 0.8125\n",
      "2017-08-02T10:51:04.547189: step 2317, loss 0.348369, acc 0.875\n",
      "2017-08-02T10:51:05.110245: step 2318, loss 0.394103, acc 0.828125\n",
      "2017-08-02T10:51:05.670301: step 2319, loss 0.283123, acc 0.921875\n",
      "2017-08-02T10:51:06.233358: step 2320, loss 0.386968, acc 0.875\n",
      "2017-08-02T10:51:06.798414: step 2321, loss 0.410323, acc 0.84375\n",
      "2017-08-02T10:51:07.361470: step 2322, loss 0.450281, acc 0.859375\n",
      "2017-08-02T10:51:07.916526: step 2323, loss 0.422501, acc 0.8125\n",
      "2017-08-02T10:51:08.492583: step 2324, loss 0.398791, acc 0.84375\n",
      "2017-08-02T10:51:09.049639: step 2325, loss 0.425404, acc 0.828125\n",
      "2017-08-02T10:51:09.609695: step 2326, loss 0.290214, acc 0.890625\n",
      "2017-08-02T10:51:10.183753: step 2327, loss 0.408387, acc 0.875\n",
      "2017-08-02T10:51:10.741808: step 2328, loss 0.294236, acc 0.859375\n",
      "2017-08-02T10:51:11.302864: step 2329, loss 0.37678, acc 0.84375\n",
      "2017-08-02T10:51:11.865921: step 2330, loss 0.335941, acc 0.890625\n",
      "2017-08-02T10:51:12.435978: step 2331, loss 0.330413, acc 0.84375\n",
      "2017-08-02T10:51:12.992033: step 2332, loss 0.3537, acc 0.84375\n",
      "2017-08-02T10:51:13.551089: step 2333, loss 0.252486, acc 0.90625\n",
      "2017-08-02T10:51:14.114146: step 2334, loss 0.333957, acc 0.84375\n",
      "2017-08-02T10:51:14.675202: step 2335, loss 0.296183, acc 0.90625\n",
      "2017-08-02T10:51:15.233257: step 2336, loss 0.315901, acc 0.890625\n",
      "2017-08-02T10:51:15.788313: step 2337, loss 0.362418, acc 0.84375\n",
      "2017-08-02T10:51:16.341368: step 2338, loss 0.343503, acc 0.828125\n",
      "2017-08-02T10:51:16.905425: step 2339, loss 0.379246, acc 0.875\n",
      "2017-08-02T10:51:17.467481: step 2340, loss 0.508957, acc 0.75\n",
      "2017-08-02T10:51:18.028537: step 2341, loss 0.261813, acc 0.890625\n",
      "2017-08-02T10:51:18.591593: step 2342, loss 0.370748, acc 0.890625\n",
      "2017-08-02T10:51:19.155650: step 2343, loss 0.436672, acc 0.84375\n",
      "2017-08-02T10:51:19.718706: step 2344, loss 0.363121, acc 0.875\n",
      "2017-08-02T10:51:20.278762: step 2345, loss 0.511386, acc 0.78125\n",
      "2017-08-02T10:51:20.843818: step 2346, loss 0.377174, acc 0.84375\n",
      "2017-08-02T10:51:21.400874: step 2347, loss 0.454013, acc 0.78125\n",
      "2017-08-02T10:51:21.957930: step 2348, loss 0.440058, acc 0.8125\n",
      "2017-08-02T10:51:22.516986: step 2349, loss 0.344132, acc 0.828125\n",
      "2017-08-02T10:51:23.076042: step 2350, loss 0.373034, acc 0.890625\n",
      "2017-08-02T10:51:23.633097: step 2351, loss 0.30473, acc 0.921875\n",
      "2017-08-02T10:51:24.194153: step 2352, loss 0.415828, acc 0.796875\n",
      "2017-08-02T10:51:24.777212: step 2353, loss 0.326491, acc 0.890625\n",
      "2017-08-02T10:51:25.359270: step 2354, loss 0.438176, acc 0.796875\n",
      "2017-08-02T10:51:25.913325: step 2355, loss 0.236869, acc 0.953125\n",
      "2017-08-02T10:51:26.472381: step 2356, loss 0.392034, acc 0.828125\n",
      "2017-08-02T10:51:27.034437: step 2357, loss 0.408915, acc 0.796875\n",
      "2017-08-02T10:51:27.590493: step 2358, loss 0.289654, acc 0.9375\n",
      "2017-08-02T10:51:28.152549: step 2359, loss 0.36836, acc 0.84375\n",
      "2017-08-02T10:51:28.714605: step 2360, loss 0.417379, acc 0.78125\n",
      "2017-08-02T10:51:29.274661: step 2361, loss 0.339557, acc 0.859375\n",
      "2017-08-02T10:51:29.834717: step 2362, loss 0.421485, acc 0.828125\n",
      "2017-08-02T10:51:30.410775: step 2363, loss 0.327581, acc 0.859375\n",
      "2017-08-02T10:51:30.972831: step 2364, loss 0.346424, acc 0.90625\n",
      "2017-08-02T10:51:31.529887: step 2365, loss 0.286393, acc 0.90625\n",
      "2017-08-02T10:51:32.089943: step 2366, loss 0.322236, acc 0.859375\n",
      "2017-08-02T10:51:32.652999: step 2367, loss 0.347423, acc 0.90625\n",
      "2017-08-02T10:51:33.212055: step 2368, loss 0.343567, acc 0.828125\n",
      "2017-08-02T10:51:33.769111: step 2369, loss 0.288918, acc 0.921875\n",
      "2017-08-02T10:51:34.325166: step 2370, loss 0.288453, acc 0.953125\n",
      "2017-08-02T10:51:34.888223: step 2371, loss 0.314595, acc 0.90625\n",
      "2017-08-02T10:51:35.449279: step 2372, loss 0.324631, acc 0.90625\n",
      "2017-08-02T10:51:36.008335: step 2373, loss 0.351271, acc 0.875\n",
      "2017-08-02T10:51:36.563390: step 2374, loss 0.415729, acc 0.765625\n",
      "2017-08-02T10:51:37.126447: step 2375, loss 0.39166, acc 0.90625\n",
      "2017-08-02T10:51:37.684502: step 2376, loss 0.384542, acc 0.828125\n",
      "2017-08-02T10:51:38.241558: step 2377, loss 0.39633, acc 0.828125\n",
      "2017-08-02T10:51:38.805614: step 2378, loss 0.272736, acc 0.890625\n",
      "2017-08-02T10:51:39.373671: step 2379, loss 0.32882, acc 0.921875\n",
      "2017-08-02T10:51:39.936728: step 2380, loss 0.256116, acc 0.9375\n",
      "2017-08-02T10:51:40.498784: step 2381, loss 0.426786, acc 0.84375\n",
      "2017-08-02T10:51:41.061840: step 2382, loss 0.310718, acc 0.859375\n",
      "2017-08-02T10:51:41.620896: step 2383, loss 0.324029, acc 0.875\n",
      "2017-08-02T10:51:42.187953: step 2384, loss 0.407649, acc 0.84375\n",
      "2017-08-02T10:51:42.755009: step 2385, loss 0.231654, acc 0.9375\n",
      "2017-08-02T10:51:43.315065: step 2386, loss 0.30394, acc 0.875\n",
      "2017-08-02T10:51:43.875121: step 2387, loss 0.311867, acc 0.859375\n",
      "2017-08-02T10:51:44.434177: step 2388, loss 0.394698, acc 0.875\n",
      "2017-08-02T10:51:45.016235: step 2389, loss 0.335052, acc 0.859375\n",
      "2017-08-02T10:51:45.572291: step 2390, loss 0.490075, acc 0.796875\n",
      "2017-08-02T10:51:46.132347: step 2391, loss 0.539235, acc 0.75\n",
      "2017-08-02T10:51:46.697404: step 2392, loss 0.236469, acc 0.953125\n",
      "2017-08-02T10:51:47.259460: step 2393, loss 0.332509, acc 0.828125\n",
      "2017-08-02T10:51:47.814515: step 2394, loss 0.334901, acc 0.875\n",
      "2017-08-02T10:51:48.370571: step 2395, loss 0.585261, acc 0.796875\n",
      "2017-08-02T10:51:48.929627: step 2396, loss 0.398187, acc 0.8125\n",
      "2017-08-02T10:51:49.491683: step 2397, loss 0.38475, acc 0.84375\n",
      "2017-08-02T10:51:50.072741: step 2398, loss 0.299992, acc 0.890625\n",
      "2017-08-02T10:51:50.634797: step 2399, loss 0.366987, acc 0.828125\n",
      "2017-08-02T10:51:51.193853: step 2400, loss 0.284479, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:51:54.972231: step 2400, loss 0.413902, acc 0.8256\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2400\n",
      "\n",
      "2017-08-02T10:51:57.762510: step 2401, loss 0.466865, acc 0.765625\n",
      "2017-08-02T10:51:58.348569: step 2402, loss 0.459662, acc 0.796875\n",
      "2017-08-02T10:51:58.919626: step 2403, loss 0.272881, acc 0.921875\n",
      "2017-08-02T10:51:59.484682: step 2404, loss 0.390201, acc 0.84375\n",
      "2017-08-02T10:52:00.082742: step 2405, loss 0.291315, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:52:00.667800: step 2406, loss 0.371847, acc 0.859375\n",
      "2017-08-02T10:52:01.248859: step 2407, loss 0.378956, acc 0.859375\n",
      "2017-08-02T10:52:01.830917: step 2408, loss 0.342579, acc 0.875\n",
      "2017-08-02T10:52:02.399974: step 2409, loss 0.395, acc 0.765625\n",
      "2017-08-02T10:52:02.955029: step 2410, loss 0.241825, acc 0.9375\n",
      "2017-08-02T10:52:03.514085: step 2411, loss 0.282728, acc 0.890625\n",
      "2017-08-02T10:52:04.071141: step 2412, loss 0.281655, acc 0.90625\n",
      "2017-08-02T10:52:04.648198: step 2413, loss 0.377555, acc 0.84375\n",
      "2017-08-02T10:52:05.208254: step 2414, loss 0.345246, acc 0.875\n",
      "2017-08-02T10:52:05.789313: step 2415, loss 0.345601, acc 0.875\n",
      "2017-08-02T10:52:06.346368: step 2416, loss 0.340368, acc 0.875\n",
      "2017-08-02T10:52:06.905424: step 2417, loss 0.236501, acc 0.953125\n",
      "2017-08-02T10:52:07.469481: step 2418, loss 0.339712, acc 0.875\n",
      "2017-08-02T10:52:08.052539: step 2419, loss 0.278, acc 0.90625\n",
      "2017-08-02T10:52:08.618595: step 2420, loss 0.229865, acc 0.96875\n",
      "2017-08-02T10:52:09.183652: step 2421, loss 0.381758, acc 0.859375\n",
      "2017-08-02T10:52:09.745708: step 2422, loss 0.457138, acc 0.84375\n",
      "2017-08-02T10:52:10.311765: step 2423, loss 0.438972, acc 0.796875\n",
      "2017-08-02T10:52:10.874821: step 2424, loss 0.314171, acc 0.921875\n",
      "2017-08-02T10:52:11.444878: step 2425, loss 0.340461, acc 0.921875\n",
      "2017-08-02T10:52:12.002934: step 2426, loss 0.501277, acc 0.796875\n",
      "2017-08-02T10:52:12.562990: step 2427, loss 0.40944, acc 0.796875\n",
      "2017-08-02T10:52:13.122046: step 2428, loss 0.327454, acc 0.875\n",
      "2017-08-02T10:52:13.681102: step 2429, loss 0.269514, acc 0.90625\n",
      "2017-08-02T10:52:14.244158: step 2430, loss 0.345543, acc 0.859375\n",
      "2017-08-02T10:52:14.816215: step 2431, loss 0.314759, acc 0.875\n",
      "2017-08-02T10:52:15.373271: step 2432, loss 0.543646, acc 0.8125\n",
      "2017-08-02T10:52:15.926326: step 2433, loss 0.392398, acc 0.875\n",
      "2017-08-02T10:52:16.480382: step 2434, loss 0.337346, acc 0.875\n",
      "2017-08-02T10:52:17.039437: step 2435, loss 0.417067, acc 0.8125\n",
      "2017-08-02T10:52:17.597493: step 2436, loss 0.404877, acc 0.8125\n",
      "2017-08-02T10:52:18.152549: step 2437, loss 0.255841, acc 0.9375\n",
      "2017-08-02T10:52:18.715605: step 2438, loss 0.336192, acc 0.84375\n",
      "2017-08-02T10:52:19.271661: step 2439, loss 0.354746, acc 0.8125\n",
      "2017-08-02T10:52:19.830717: step 2440, loss 0.436767, acc 0.828125\n",
      "2017-08-02T10:52:20.387772: step 2441, loss 0.237633, acc 0.921875\n",
      "2017-08-02T10:52:20.945828: step 2442, loss 0.385994, acc 0.796875\n",
      "2017-08-02T10:52:21.503884: step 2443, loss 0.34143, acc 0.859375\n",
      "2017-08-02T10:52:22.060940: step 2444, loss 0.243092, acc 0.953125\n",
      "2017-08-02T10:52:22.616995: step 2445, loss 0.425432, acc 0.84375\n",
      "2017-08-02T10:52:23.175051: step 2446, loss 0.397617, acc 0.828125\n",
      "2017-08-02T10:52:23.737107: step 2447, loss 0.333353, acc 0.90625\n",
      "2017-08-02T10:52:24.294163: step 2448, loss 0.265614, acc 0.90625\n",
      "2017-08-02T10:52:24.852219: step 2449, loss 0.349038, acc 0.90625\n",
      "2017-08-02T10:52:25.416275: step 2450, loss 0.353879, acc 0.875\n",
      "2017-08-02T10:52:26.007334: step 2451, loss 0.334742, acc 0.84375\n",
      "2017-08-02T10:52:26.601394: step 2452, loss 0.390522, acc 0.84375\n",
      "2017-08-02T10:52:27.205454: step 2453, loss 0.341558, acc 0.859375\n",
      "2017-08-02T10:52:27.811515: step 2454, loss 0.351711, acc 0.828125\n",
      "2017-08-02T10:52:28.401574: step 2455, loss 0.344813, acc 0.859375\n",
      "2017-08-02T10:52:29.000633: step 2456, loss 0.444001, acc 0.84375\n",
      "2017-08-02T10:52:29.596693: step 2457, loss 0.364607, acc 0.859375\n",
      "2017-08-02T10:52:30.183752: step 2458, loss 0.310459, acc 0.890625\n",
      "2017-08-02T10:52:30.776811: step 2459, loss 0.417695, acc 0.796875\n",
      "2017-08-02T10:52:31.341868: step 2460, loss 0.254103, acc 0.921875\n",
      "2017-08-02T10:52:31.900923: step 2461, loss 0.38338, acc 0.828125\n",
      "2017-08-02T10:52:32.464980: step 2462, loss 0.340599, acc 0.890625\n",
      "2017-08-02T10:52:33.026036: step 2463, loss 0.329757, acc 0.90625\n",
      "2017-08-02T10:52:33.584092: step 2464, loss 0.378753, acc 0.828125\n",
      "2017-08-02T10:52:34.141147: step 2465, loss 0.406474, acc 0.8125\n",
      "2017-08-02T10:52:34.710204: step 2466, loss 0.520282, acc 0.75\n",
      "2017-08-02T10:52:35.280261: step 2467, loss 0.396882, acc 0.859375\n",
      "2017-08-02T10:52:35.837317: step 2468, loss 0.333299, acc 0.90625\n",
      "2017-08-02T10:52:36.398373: step 2469, loss 0.375215, acc 0.84375\n",
      "2017-08-02T10:52:36.953429: step 2470, loss 0.379733, acc 0.8125\n",
      "2017-08-02T10:52:37.519485: step 2471, loss 0.392321, acc 0.828125\n",
      "2017-08-02T10:52:38.077541: step 2472, loss 0.415548, acc 0.796875\n",
      "2017-08-02T10:52:38.635597: step 2473, loss 0.270285, acc 0.921875\n",
      "2017-08-02T10:52:39.193653: step 2474, loss 0.321667, acc 0.84375\n",
      "2017-08-02T10:52:39.746708: step 2475, loss 0.246345, acc 0.921875\n",
      "2017-08-02T10:52:40.306764: step 2476, loss 0.441925, acc 0.84375\n",
      "2017-08-02T10:52:40.864820: step 2477, loss 0.370124, acc 0.828125\n",
      "2017-08-02T10:52:41.417875: step 2478, loss 0.286253, acc 0.90625\n",
      "2017-08-02T10:52:41.977931: step 2479, loss 0.403761, acc 0.828125\n",
      "2017-08-02T10:52:42.537987: step 2480, loss 0.310389, acc 0.875\n",
      "2017-08-02T10:52:43.103044: step 2481, loss 0.311846, acc 0.890625\n",
      "2017-08-02T10:52:43.663100: step 2482, loss 0.349149, acc 0.84375\n",
      "2017-08-02T10:52:44.228156: step 2483, loss 0.429491, acc 0.828125\n",
      "2017-08-02T10:52:44.788212: step 2484, loss 0.365884, acc 0.828125\n",
      "2017-08-02T10:52:45.363270: step 2485, loss 0.348209, acc 0.875\n",
      "2017-08-02T10:52:45.922325: step 2486, loss 0.282328, acc 0.890625\n",
      "2017-08-02T10:52:46.485382: step 2487, loss 0.348313, acc 0.890625\n",
      "2017-08-02T10:52:47.046438: step 2488, loss 0.507996, acc 0.796875\n",
      "2017-08-02T10:52:47.607494: step 2489, loss 0.333803, acc 0.890625\n",
      "2017-08-02T10:52:48.166550: step 2490, loss 0.325766, acc 0.859375\n",
      "2017-08-02T10:52:48.725606: step 2491, loss 0.383747, acc 0.8125\n",
      "2017-08-02T10:52:49.287662: step 2492, loss 0.362721, acc 0.84375\n",
      "2017-08-02T10:52:49.856719: step 2493, loss 0.357959, acc 0.90625\n",
      "2017-08-02T10:52:50.415775: step 2494, loss 0.363758, acc 0.875\n",
      "2017-08-02T10:52:50.974831: step 2495, loss 0.339528, acc 0.84375\n",
      "2017-08-02T10:52:51.530886: step 2496, loss 0.396488, acc 0.8125\n",
      "2017-08-02T10:52:52.087942: step 2497, loss 0.317729, acc 0.875\n",
      "2017-08-02T10:52:52.658999: step 2498, loss 0.402305, acc 0.8125\n",
      "2017-08-02T10:52:53.219055: step 2499, loss 0.348033, acc 0.859375\n",
      "2017-08-02T10:52:53.774111: step 2500, loss 0.345523, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:52:57.495483: step 2500, loss 0.429386, acc 0.8248\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2500\n",
      "\n",
      "2017-08-02T10:53:00.068740: step 2501, loss 0.357341, acc 0.828125\n",
      "2017-08-02T10:53:00.630796: step 2502, loss 0.416125, acc 0.859375\n",
      "2017-08-02T10:53:01.189852: step 2503, loss 0.379682, acc 0.828125\n",
      "2017-08-02T10:53:01.606894: step 2504, loss 0.236527, acc 0.90625\n",
      "2017-08-02T10:53:02.164950: step 2505, loss 0.366605, acc 0.8125\n",
      "2017-08-02T10:53:02.735007: step 2506, loss 0.328315, acc 0.859375\n",
      "2017-08-02T10:53:03.323065: step 2507, loss 0.294309, acc 0.921875\n",
      "2017-08-02T10:53:03.887122: step 2508, loss 0.306703, acc 0.859375\n",
      "2017-08-02T10:53:04.456179: step 2509, loss 0.241654, acc 0.96875\n",
      "2017-08-02T10:53:05.041237: step 2510, loss 0.297199, acc 0.921875\n",
      "2017-08-02T10:53:05.633296: step 2511, loss 0.223141, acc 0.984375\n",
      "2017-08-02T10:53:06.192352: step 2512, loss 0.330623, acc 0.890625\n",
      "2017-08-02T10:53:06.762409: step 2513, loss 0.323791, acc 0.890625\n",
      "2017-08-02T10:53:07.330466: step 2514, loss 0.350685, acc 0.890625\n",
      "2017-08-02T10:53:07.899523: step 2515, loss 0.318807, acc 0.859375\n",
      "2017-08-02T10:53:08.458579: step 2516, loss 0.303392, acc 0.890625\n",
      "2017-08-02T10:53:09.017635: step 2517, loss 0.277881, acc 0.90625\n",
      "2017-08-02T10:53:09.589692: step 2518, loss 0.305911, acc 0.90625\n",
      "2017-08-02T10:53:10.145748: step 2519, loss 0.333929, acc 0.921875\n",
      "2017-08-02T10:53:10.703803: step 2520, loss 0.232707, acc 0.921875\n",
      "2017-08-02T10:53:11.268860: step 2521, loss 0.325249, acc 0.875\n",
      "2017-08-02T10:53:11.830916: step 2522, loss 0.320538, acc 0.890625\n",
      "2017-08-02T10:53:12.391972: step 2523, loss 0.457233, acc 0.796875\n",
      "2017-08-02T10:53:12.964029: step 2524, loss 0.242632, acc 0.921875\n",
      "2017-08-02T10:53:13.518085: step 2525, loss 0.316189, acc 0.875\n",
      "2017-08-02T10:53:14.072140: step 2526, loss 0.326377, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:53:14.634196: step 2527, loss 0.312317, acc 0.859375\n",
      "2017-08-02T10:53:15.195252: step 2528, loss 0.274615, acc 0.890625\n",
      "2017-08-02T10:53:15.757309: step 2529, loss 0.327993, acc 0.875\n",
      "2017-08-02T10:53:16.313364: step 2530, loss 0.277817, acc 0.90625\n",
      "2017-08-02T10:53:16.895422: step 2531, loss 0.293983, acc 0.90625\n",
      "2017-08-02T10:53:17.459479: step 2532, loss 0.443955, acc 0.8125\n",
      "2017-08-02T10:53:18.021535: step 2533, loss 0.339462, acc 0.890625\n",
      "2017-08-02T10:53:18.577591: step 2534, loss 0.338701, acc 0.859375\n",
      "2017-08-02T10:53:19.131646: step 2535, loss 0.369924, acc 0.859375\n",
      "2017-08-02T10:53:19.693702: step 2536, loss 0.330918, acc 0.859375\n",
      "2017-08-02T10:53:20.254758: step 2537, loss 0.428569, acc 0.828125\n",
      "2017-08-02T10:53:20.811814: step 2538, loss 0.291815, acc 0.875\n",
      "2017-08-02T10:53:21.363869: step 2539, loss 0.346119, acc 0.859375\n",
      "2017-08-02T10:53:21.916925: step 2540, loss 0.287638, acc 0.90625\n",
      "2017-08-02T10:53:22.478981: step 2541, loss 0.373085, acc 0.859375\n",
      "2017-08-02T10:53:23.034036: step 2542, loss 0.391879, acc 0.828125\n",
      "2017-08-02T10:53:23.587092: step 2543, loss 0.36376, acc 0.859375\n",
      "2017-08-02T10:53:24.142147: step 2544, loss 0.327756, acc 0.875\n",
      "2017-08-02T10:53:24.697203: step 2545, loss 0.288973, acc 0.890625\n",
      "2017-08-02T10:53:25.253258: step 2546, loss 0.252437, acc 0.90625\n",
      "2017-08-02T10:53:25.807314: step 2547, loss 0.312272, acc 0.921875\n",
      "2017-08-02T10:53:26.385371: step 2548, loss 0.261531, acc 0.9375\n",
      "2017-08-02T10:53:26.965429: step 2549, loss 0.259292, acc 0.9375\n",
      "2017-08-02T10:53:27.526485: step 2550, loss 0.250292, acc 0.921875\n",
      "2017-08-02T10:53:28.086541: step 2551, loss 0.289388, acc 0.9375\n",
      "2017-08-02T10:53:28.644597: step 2552, loss 0.329782, acc 0.875\n",
      "2017-08-02T10:53:29.204653: step 2553, loss 0.264679, acc 0.890625\n",
      "2017-08-02T10:53:29.759709: step 2554, loss 0.23469, acc 0.921875\n",
      "2017-08-02T10:53:30.322765: step 2555, loss 0.460569, acc 0.796875\n",
      "2017-08-02T10:53:30.879821: step 2556, loss 0.307547, acc 0.890625\n",
      "2017-08-02T10:53:31.437877: step 2557, loss 0.407342, acc 0.890625\n",
      "2017-08-02T10:53:31.995932: step 2558, loss 0.336631, acc 0.84375\n",
      "2017-08-02T10:53:32.552988: step 2559, loss 0.258612, acc 0.921875\n",
      "2017-08-02T10:53:33.106043: step 2560, loss 0.293796, acc 0.84375\n",
      "2017-08-02T10:53:33.667099: step 2561, loss 0.312068, acc 0.890625\n",
      "2017-08-02T10:53:34.228156: step 2562, loss 0.273135, acc 0.9375\n",
      "2017-08-02T10:53:34.799213: step 2563, loss 0.215403, acc 0.953125\n",
      "2017-08-02T10:53:35.358269: step 2564, loss 0.315997, acc 0.84375\n",
      "2017-08-02T10:53:35.915324: step 2565, loss 0.30677, acc 0.90625\n",
      "2017-08-02T10:53:36.471380: step 2566, loss 0.265139, acc 0.9375\n",
      "2017-08-02T10:53:37.027435: step 2567, loss 0.341989, acc 0.84375\n",
      "2017-08-02T10:53:37.582491: step 2568, loss 0.316609, acc 0.90625\n",
      "2017-08-02T10:53:38.144547: step 2569, loss 0.394188, acc 0.875\n",
      "2017-08-02T10:53:38.706603: step 2570, loss 0.23805, acc 0.921875\n",
      "2017-08-02T10:53:39.278661: step 2571, loss 0.441516, acc 0.84375\n",
      "2017-08-02T10:53:39.836716: step 2572, loss 0.283054, acc 0.90625\n",
      "2017-08-02T10:53:40.411774: step 2573, loss 0.396556, acc 0.796875\n",
      "2017-08-02T10:53:40.966829: step 2574, loss 0.332435, acc 0.84375\n",
      "2017-08-02T10:53:41.525885: step 2575, loss 0.280876, acc 0.90625\n",
      "2017-08-02T10:53:42.080941: step 2576, loss 0.417542, acc 0.828125\n",
      "2017-08-02T10:53:42.639997: step 2577, loss 0.305327, acc 0.921875\n",
      "2017-08-02T10:53:43.196052: step 2578, loss 0.2658, acc 0.890625\n",
      "2017-08-02T10:53:43.754108: step 2579, loss 0.35579, acc 0.890625\n",
      "2017-08-02T10:53:44.313164: step 2580, loss 0.260617, acc 0.921875\n",
      "2017-08-02T10:53:44.875220: step 2581, loss 0.258076, acc 0.90625\n",
      "2017-08-02T10:53:45.439277: step 2582, loss 0.269968, acc 0.921875\n",
      "2017-08-02T10:53:45.993332: step 2583, loss 0.232821, acc 0.9375\n",
      "2017-08-02T10:53:46.550388: step 2584, loss 0.350279, acc 0.859375\n",
      "2017-08-02T10:53:47.106443: step 2585, loss 0.311364, acc 0.90625\n",
      "2017-08-02T10:53:47.662499: step 2586, loss 0.298096, acc 0.875\n",
      "2017-08-02T10:53:48.214554: step 2587, loss 0.331591, acc 0.875\n",
      "2017-08-02T10:53:48.768609: step 2588, loss 0.312146, acc 0.9375\n",
      "2017-08-02T10:53:49.326665: step 2589, loss 0.352391, acc 0.859375\n",
      "2017-08-02T10:53:49.882721: step 2590, loss 0.39446, acc 0.78125\n",
      "2017-08-02T10:53:50.442777: step 2591, loss 0.303357, acc 0.90625\n",
      "2017-08-02T10:53:51.006833: step 2592, loss 0.416733, acc 0.828125\n",
      "2017-08-02T10:53:51.564889: step 2593, loss 0.354725, acc 0.84375\n",
      "2017-08-02T10:53:52.122945: step 2594, loss 0.31998, acc 0.921875\n",
      "2017-08-02T10:53:52.679000: step 2595, loss 0.341483, acc 0.796875\n",
      "2017-08-02T10:53:53.238056: step 2596, loss 0.286161, acc 0.90625\n",
      "2017-08-02T10:53:53.797112: step 2597, loss 0.257548, acc 0.984375\n",
      "2017-08-02T10:53:54.355168: step 2598, loss 0.238162, acc 0.90625\n",
      "2017-08-02T10:53:54.914224: step 2599, loss 0.362133, acc 0.84375\n",
      "2017-08-02T10:53:55.472280: step 2600, loss 0.232264, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:53:59.246657: step 2600, loss 0.410267, acc 0.8276\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2600\n",
      "\n",
      "2017-08-02T10:54:01.842917: step 2601, loss 0.316827, acc 0.921875\n",
      "2017-08-02T10:54:02.401973: step 2602, loss 0.286869, acc 0.875\n",
      "2017-08-02T10:54:02.963029: step 2603, loss 0.186777, acc 0.96875\n",
      "2017-08-02T10:54:03.554088: step 2604, loss 0.316624, acc 0.828125\n",
      "2017-08-02T10:54:04.111144: step 2605, loss 0.285653, acc 0.890625\n",
      "2017-08-02T10:54:04.666199: step 2606, loss 0.168272, acc 0.96875\n",
      "2017-08-02T10:54:05.225255: step 2607, loss 0.37646, acc 0.84375\n",
      "2017-08-02T10:54:05.789311: step 2608, loss 0.405514, acc 0.890625\n",
      "2017-08-02T10:54:06.345367: step 2609, loss 0.356683, acc 0.921875\n",
      "2017-08-02T10:54:06.904423: step 2610, loss 0.280018, acc 0.90625\n",
      "2017-08-02T10:54:07.462479: step 2611, loss 0.358513, acc 0.8125\n",
      "2017-08-02T10:54:08.030535: step 2612, loss 0.279069, acc 0.90625\n",
      "2017-08-02T10:54:08.596592: step 2613, loss 0.322075, acc 0.875\n",
      "2017-08-02T10:54:09.153648: step 2614, loss 0.382469, acc 0.8125\n",
      "2017-08-02T10:54:09.718704: step 2615, loss 0.38018, acc 0.8125\n",
      "2017-08-02T10:54:10.280760: step 2616, loss 0.26679, acc 0.890625\n",
      "2017-08-02T10:54:10.851818: step 2617, loss 0.196265, acc 0.953125\n",
      "2017-08-02T10:54:11.410873: step 2618, loss 0.386202, acc 0.890625\n",
      "2017-08-02T10:54:11.976930: step 2619, loss 0.3161, acc 0.890625\n",
      "2017-08-02T10:54:12.533986: step 2620, loss 0.246955, acc 0.921875\n",
      "2017-08-02T10:54:13.093042: step 2621, loss 0.223864, acc 0.9375\n",
      "2017-08-02T10:54:13.684101: step 2622, loss 0.332625, acc 0.890625\n",
      "2017-08-02T10:54:14.238156: step 2623, loss 0.256903, acc 0.921875\n",
      "2017-08-02T10:54:14.802213: step 2624, loss 0.427907, acc 0.8125\n",
      "2017-08-02T10:54:15.361268: step 2625, loss 0.376537, acc 0.859375\n",
      "2017-08-02T10:54:15.918324: step 2626, loss 0.312332, acc 0.875\n",
      "2017-08-02T10:54:16.477380: step 2627, loss 0.35467, acc 0.890625\n",
      "2017-08-02T10:54:17.036436: step 2628, loss 0.275097, acc 0.90625\n",
      "2017-08-02T10:54:17.588491: step 2629, loss 0.32072, acc 0.875\n",
      "2017-08-02T10:54:18.148547: step 2630, loss 0.344802, acc 0.8125\n",
      "2017-08-02T10:54:18.709603: step 2631, loss 0.32538, acc 0.890625\n",
      "2017-08-02T10:54:19.276660: step 2632, loss 0.238343, acc 0.9375\n",
      "2017-08-02T10:54:19.834716: step 2633, loss 0.21212, acc 0.9375\n",
      "2017-08-02T10:54:20.394772: step 2634, loss 0.349058, acc 0.859375\n",
      "2017-08-02T10:54:20.971829: step 2635, loss 0.291536, acc 0.953125\n",
      "2017-08-02T10:54:21.528885: step 2636, loss 0.293913, acc 0.90625\n",
      "2017-08-02T10:54:22.086941: step 2637, loss 0.369058, acc 0.859375\n",
      "2017-08-02T10:54:22.644997: step 2638, loss 0.405572, acc 0.84375\n",
      "2017-08-02T10:54:23.209053: step 2639, loss 0.391576, acc 0.8125\n",
      "2017-08-02T10:54:23.765109: step 2640, loss 0.309332, acc 0.90625\n",
      "2017-08-02T10:54:24.318164: step 2641, loss 0.218725, acc 0.9375\n",
      "2017-08-02T10:54:24.877220: step 2642, loss 0.368093, acc 0.875\n",
      "2017-08-02T10:54:25.437276: step 2643, loss 0.335408, acc 0.84375\n",
      "2017-08-02T10:54:26.001332: step 2644, loss 0.357047, acc 0.890625\n",
      "2017-08-02T10:54:26.563389: step 2645, loss 0.381456, acc 0.84375\n",
      "2017-08-02T10:54:27.141446: step 2646, loss 0.342107, acc 0.890625\n",
      "2017-08-02T10:54:27.699502: step 2647, loss 0.292143, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:54:28.259558: step 2648, loss 0.448813, acc 0.8125\n",
      "2017-08-02T10:54:28.815614: step 2649, loss 0.235606, acc 0.953125\n",
      "2017-08-02T10:54:29.367669: step 2650, loss 0.422457, acc 0.84375\n",
      "2017-08-02T10:54:29.934726: step 2651, loss 0.27401, acc 0.90625\n",
      "2017-08-02T10:54:30.497782: step 2652, loss 0.324158, acc 0.890625\n",
      "2017-08-02T10:54:31.049837: step 2653, loss 0.280046, acc 0.875\n",
      "2017-08-02T10:54:31.608893: step 2654, loss 0.327412, acc 0.859375\n",
      "2017-08-02T10:54:32.172949: step 2655, loss 0.326316, acc 0.90625\n",
      "2017-08-02T10:54:32.731005: step 2656, loss 0.392328, acc 0.84375\n",
      "2017-08-02T10:54:33.294062: step 2657, loss 0.258204, acc 0.90625\n",
      "2017-08-02T10:54:33.860118: step 2658, loss 0.263745, acc 0.90625\n",
      "2017-08-02T10:54:34.418174: step 2659, loss 0.245363, acc 0.890625\n",
      "2017-08-02T10:54:34.981230: step 2660, loss 0.211496, acc 0.9375\n",
      "2017-08-02T10:54:35.550287: step 2661, loss 0.305966, acc 0.890625\n",
      "2017-08-02T10:54:36.108343: step 2662, loss 0.196211, acc 0.9375\n",
      "2017-08-02T10:54:36.662398: step 2663, loss 0.22813, acc 0.953125\n",
      "2017-08-02T10:54:37.218454: step 2664, loss 0.346117, acc 0.84375\n",
      "2017-08-02T10:54:37.772509: step 2665, loss 0.216219, acc 0.953125\n",
      "2017-08-02T10:54:38.332565: step 2666, loss 0.330591, acc 0.859375\n",
      "2017-08-02T10:54:38.893621: step 2667, loss 0.240499, acc 0.90625\n",
      "2017-08-02T10:54:39.461678: step 2668, loss 0.407419, acc 0.90625\n",
      "2017-08-02T10:54:40.018734: step 2669, loss 0.32261, acc 0.890625\n",
      "2017-08-02T10:54:40.571789: step 2670, loss 0.379989, acc 0.859375\n",
      "2017-08-02T10:54:41.131845: step 2671, loss 0.226489, acc 0.90625\n",
      "2017-08-02T10:54:41.684901: step 2672, loss 0.347552, acc 0.828125\n",
      "2017-08-02T10:54:42.245957: step 2673, loss 0.365356, acc 0.90625\n",
      "2017-08-02T10:54:42.800012: step 2674, loss 0.330522, acc 0.875\n",
      "2017-08-02T10:54:43.360068: step 2675, loss 0.207761, acc 0.953125\n",
      "2017-08-02T10:54:43.919124: step 2676, loss 0.380111, acc 0.875\n",
      "2017-08-02T10:54:44.474179: step 2677, loss 0.288725, acc 0.9375\n",
      "2017-08-02T10:54:45.033235: step 2678, loss 0.258962, acc 0.90625\n",
      "2017-08-02T10:54:45.592291: step 2679, loss 0.209252, acc 0.96875\n",
      "2017-08-02T10:54:46.145347: step 2680, loss 0.278443, acc 0.890625\n",
      "2017-08-02T10:54:46.724404: step 2681, loss 0.211711, acc 0.96875\n",
      "2017-08-02T10:54:47.286461: step 2682, loss 0.253601, acc 0.921875\n",
      "2017-08-02T10:54:47.841516: step 2683, loss 0.314725, acc 0.890625\n",
      "2017-08-02T10:54:48.405573: step 2684, loss 0.295661, acc 0.90625\n",
      "2017-08-02T10:54:48.964628: step 2685, loss 0.228219, acc 0.90625\n",
      "2017-08-02T10:54:49.518684: step 2686, loss 0.240207, acc 0.9375\n",
      "2017-08-02T10:54:50.073739: step 2687, loss 0.267125, acc 0.9375\n",
      "2017-08-02T10:54:50.629795: step 2688, loss 0.255814, acc 0.9375\n",
      "2017-08-02T10:54:51.186851: step 2689, loss 0.29021, acc 0.875\n",
      "2017-08-02T10:54:51.743906: step 2690, loss 0.305005, acc 0.90625\n",
      "2017-08-02T10:54:52.318964: step 2691, loss 0.229461, acc 0.953125\n",
      "2017-08-02T10:54:52.882020: step 2692, loss 0.29192, acc 0.875\n",
      "2017-08-02T10:54:53.441076: step 2693, loss 0.185033, acc 0.96875\n",
      "2017-08-02T10:54:53.996132: step 2694, loss 0.2516, acc 0.953125\n",
      "2017-08-02T10:54:54.556188: step 2695, loss 0.369894, acc 0.828125\n",
      "2017-08-02T10:54:55.114243: step 2696, loss 0.294837, acc 0.859375\n",
      "2017-08-02T10:54:55.673299: step 2697, loss 0.312264, acc 0.90625\n",
      "2017-08-02T10:54:56.226355: step 2698, loss 0.343421, acc 0.828125\n",
      "2017-08-02T10:54:56.784410: step 2699, loss 0.287646, acc 0.875\n",
      "2017-08-02T10:54:57.339466: step 2700, loss 0.240344, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:55:01.008833: step 2700, loss 0.415524, acc 0.8292\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2700\n",
      "\n",
      "2017-08-02T10:55:03.677100: step 2701, loss 0.23998, acc 0.921875\n",
      "2017-08-02T10:55:04.259158: step 2702, loss 0.297495, acc 0.875\n",
      "2017-08-02T10:55:04.857218: step 2703, loss 0.244775, acc 0.90625\n",
      "2017-08-02T10:55:05.437276: step 2704, loss 0.25091, acc 0.921875\n",
      "2017-08-02T10:55:06.007333: step 2705, loss 0.310669, acc 0.90625\n",
      "2017-08-02T10:55:06.577390: step 2706, loss 0.265577, acc 0.9375\n",
      "2017-08-02T10:55:07.162448: step 2707, loss 0.218346, acc 0.953125\n",
      "2017-08-02T10:55:07.735505: step 2708, loss 0.289944, acc 0.9375\n",
      "2017-08-02T10:55:08.315563: step 2709, loss 0.312647, acc 0.90625\n",
      "2017-08-02T10:55:08.892621: step 2710, loss 0.331601, acc 0.859375\n",
      "2017-08-02T10:55:09.452677: step 2711, loss 0.290958, acc 0.890625\n",
      "2017-08-02T10:55:10.005732: step 2712, loss 0.344974, acc 0.859375\n",
      "2017-08-02T10:55:10.561788: step 2713, loss 0.212015, acc 0.9375\n",
      "2017-08-02T10:55:11.126844: step 2714, loss 0.271722, acc 0.890625\n",
      "2017-08-02T10:55:11.679900: step 2715, loss 0.323613, acc 0.890625\n",
      "2017-08-02T10:55:12.248957: step 2716, loss 0.327131, acc 0.921875\n",
      "2017-08-02T10:55:12.804012: step 2717, loss 0.331538, acc 0.875\n",
      "2017-08-02T10:55:13.359068: step 2718, loss 0.274615, acc 0.890625\n",
      "2017-08-02T10:55:13.932125: step 2719, loss 0.258695, acc 0.9375\n",
      "2017-08-02T10:55:14.502182: step 2720, loss 0.275215, acc 0.890625\n",
      "2017-08-02T10:55:15.063238: step 2721, loss 0.439406, acc 0.828125\n",
      "2017-08-02T10:55:15.621294: step 2722, loss 0.165624, acc 0.953125\n",
      "2017-08-02T10:55:16.178350: step 2723, loss 0.240533, acc 0.9375\n",
      "2017-08-02T10:55:16.737405: step 2724, loss 0.236085, acc 0.921875\n",
      "2017-08-02T10:55:17.299462: step 2725, loss 0.385231, acc 0.859375\n",
      "2017-08-02T10:55:17.852517: step 2726, loss 0.393105, acc 0.875\n",
      "2017-08-02T10:55:18.411573: step 2727, loss 0.203566, acc 0.9375\n",
      "2017-08-02T10:55:18.965628: step 2728, loss 0.263478, acc 0.9375\n",
      "2017-08-02T10:55:19.524684: step 2729, loss 0.33886, acc 0.875\n",
      "2017-08-02T10:55:20.092741: step 2730, loss 0.2965, acc 0.890625\n",
      "2017-08-02T10:55:20.671799: step 2731, loss 0.398477, acc 0.875\n",
      "2017-08-02T10:55:21.230855: step 2732, loss 0.315272, acc 0.921875\n",
      "2017-08-02T10:55:21.787910: step 2733, loss 0.300177, acc 0.875\n",
      "2017-08-02T10:55:22.347966: step 2734, loss 0.234387, acc 0.9375\n",
      "2017-08-02T10:55:22.908022: step 2735, loss 0.340643, acc 0.859375\n",
      "2017-08-02T10:55:23.491081: step 2736, loss 0.207251, acc 0.921875\n",
      "2017-08-02T10:55:24.043136: step 2737, loss 0.479898, acc 0.828125\n",
      "2017-08-02T10:55:24.598191: step 2738, loss 0.361309, acc 0.8125\n",
      "2017-08-02T10:55:25.157247: step 2739, loss 0.270347, acc 0.890625\n",
      "2017-08-02T10:55:25.730305: step 2740, loss 0.219382, acc 0.921875\n",
      "2017-08-02T10:55:26.291361: step 2741, loss 0.282493, acc 0.859375\n",
      "2017-08-02T10:55:26.851417: step 2742, loss 0.280569, acc 0.890625\n",
      "2017-08-02T10:55:27.448476: step 2743, loss 0.318701, acc 0.859375\n",
      "2017-08-02T10:55:28.062538: step 2744, loss 0.304886, acc 0.890625\n",
      "2017-08-02T10:55:28.657597: step 2745, loss 0.237766, acc 0.921875\n",
      "2017-08-02T10:55:29.253657: step 2746, loss 0.35675, acc 0.90625\n",
      "2017-08-02T10:55:29.864718: step 2747, loss 0.334156, acc 0.828125\n",
      "2017-08-02T10:55:30.458777: step 2748, loss 0.262044, acc 0.9375\n",
      "2017-08-02T10:55:31.052837: step 2749, loss 0.279755, acc 0.890625\n",
      "2017-08-02T10:55:31.649897: step 2750, loss 0.256223, acc 0.921875\n",
      "2017-08-02T10:55:32.238955: step 2751, loss 0.329134, acc 0.8125\n",
      "2017-08-02T10:55:32.801012: step 2752, loss 0.326587, acc 0.890625\n",
      "2017-08-02T10:55:33.357067: step 2753, loss 0.234808, acc 0.9375\n",
      "2017-08-02T10:55:33.917123: step 2754, loss 0.313416, acc 0.890625\n",
      "2017-08-02T10:55:34.479179: step 2755, loss 0.264933, acc 0.90625\n",
      "2017-08-02T10:55:35.036235: step 2756, loss 0.276581, acc 0.90625\n",
      "2017-08-02T10:55:35.590291: step 2757, loss 0.31879, acc 0.859375\n",
      "2017-08-02T10:55:36.149346: step 2758, loss 0.316372, acc 0.890625\n",
      "2017-08-02T10:55:36.730405: step 2759, loss 0.323496, acc 0.828125\n",
      "2017-08-02T10:55:37.290461: step 2760, loss 0.313593, acc 0.859375\n",
      "2017-08-02T10:55:37.850517: step 2761, loss 0.328978, acc 0.90625\n",
      "2017-08-02T10:55:38.408572: step 2762, loss 0.264009, acc 0.921875\n",
      "2017-08-02T10:55:38.968628: step 2763, loss 0.246455, acc 0.9375\n",
      "2017-08-02T10:55:39.528684: step 2764, loss 0.262714, acc 0.921875\n",
      "2017-08-02T10:55:40.090741: step 2765, loss 0.316767, acc 0.859375\n",
      "2017-08-02T10:55:40.650797: step 2766, loss 0.301313, acc 0.890625\n",
      "2017-08-02T10:55:41.208852: step 2767, loss 0.313921, acc 0.875\n",
      "2017-08-02T10:55:41.767908: step 2768, loss 0.40455, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:55:42.337965: step 2769, loss 0.37555, acc 0.890625\n",
      "2017-08-02T10:55:42.901022: step 2770, loss 0.342435, acc 0.875\n",
      "2017-08-02T10:55:43.459077: step 2771, loss 0.204268, acc 0.921875\n",
      "2017-08-02T10:55:44.016133: step 2772, loss 0.392493, acc 0.828125\n",
      "2017-08-02T10:55:44.574189: step 2773, loss 0.411069, acc 0.875\n",
      "2017-08-02T10:55:45.129244: step 2774, loss 0.369317, acc 0.859375\n",
      "2017-08-02T10:55:45.686300: step 2775, loss 0.320571, acc 0.859375\n",
      "2017-08-02T10:55:46.239355: step 2776, loss 0.333833, acc 0.890625\n",
      "2017-08-02T10:55:46.801412: step 2777, loss 0.259121, acc 0.9375\n",
      "2017-08-02T10:55:47.362468: step 2778, loss 0.261686, acc 0.921875\n",
      "2017-08-02T10:55:47.932525: step 2779, loss 0.228704, acc 0.9375\n",
      "2017-08-02T10:55:48.489580: step 2780, loss 0.386498, acc 0.875\n",
      "2017-08-02T10:55:49.045636: step 2781, loss 0.249533, acc 0.890625\n",
      "2017-08-02T10:55:49.599691: step 2782, loss 0.372877, acc 0.859375\n",
      "2017-08-02T10:55:50.158747: step 2783, loss 0.207996, acc 0.9375\n",
      "2017-08-02T10:55:50.719803: step 2784, loss 0.310958, acc 0.875\n",
      "2017-08-02T10:55:51.278859: step 2785, loss 0.286835, acc 0.859375\n",
      "2017-08-02T10:55:51.833915: step 2786, loss 0.27732, acc 0.90625\n",
      "2017-08-02T10:55:52.395971: step 2787, loss 0.388467, acc 0.890625\n",
      "2017-08-02T10:55:52.966028: step 2788, loss 0.308853, acc 0.859375\n",
      "2017-08-02T10:55:53.528084: step 2789, loss 0.244807, acc 0.96875\n",
      "2017-08-02T10:55:54.083140: step 2790, loss 0.265115, acc 0.921875\n",
      "2017-08-02T10:55:54.640195: step 2791, loss 0.208669, acc 0.9375\n",
      "2017-08-02T10:55:55.202252: step 2792, loss 0.363571, acc 0.828125\n",
      "2017-08-02T10:55:55.762308: step 2793, loss 0.412177, acc 0.875\n",
      "2017-08-02T10:55:56.325364: step 2794, loss 0.267321, acc 0.890625\n",
      "2017-08-02T10:55:56.889420: step 2795, loss 0.264354, acc 0.921875\n",
      "2017-08-02T10:55:57.473479: step 2796, loss 0.297988, acc 0.90625\n",
      "2017-08-02T10:55:58.034535: step 2797, loss 0.280962, acc 0.9375\n",
      "2017-08-02T10:55:58.596591: step 2798, loss 0.362928, acc 0.828125\n",
      "2017-08-02T10:55:59.152647: step 2799, loss 0.341869, acc 0.859375\n",
      "2017-08-02T10:55:59.710702: step 2800, loss 0.35309, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:56:03.407072: step 2800, loss 0.412484, acc 0.8292\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2800\n",
      "\n",
      "2017-08-02T10:56:06.010332: step 2801, loss 0.260453, acc 0.90625\n",
      "2017-08-02T10:56:06.564388: step 2802, loss 0.281496, acc 0.890625\n",
      "2017-08-02T10:56:07.120443: step 2803, loss 0.339974, acc 0.859375\n",
      "2017-08-02T10:56:07.676499: step 2804, loss 0.340167, acc 0.890625\n",
      "2017-08-02T10:56:08.233555: step 2805, loss 0.329939, acc 0.890625\n",
      "2017-08-02T10:56:08.795611: step 2806, loss 0.300568, acc 0.890625\n",
      "2017-08-02T10:56:09.351666: step 2807, loss 0.404624, acc 0.828125\n",
      "2017-08-02T10:56:09.922723: step 2808, loss 0.272055, acc 0.921875\n",
      "2017-08-02T10:56:10.482779: step 2809, loss 0.27217, acc 0.9375\n",
      "2017-08-02T10:56:11.040835: step 2810, loss 0.290748, acc 0.859375\n",
      "2017-08-02T10:56:11.603892: step 2811, loss 0.386931, acc 0.8125\n",
      "2017-08-02T10:56:12.159947: step 2812, loss 0.252647, acc 0.875\n",
      "2017-08-02T10:56:12.712002: step 2813, loss 0.289722, acc 0.921875\n",
      "2017-08-02T10:56:13.270058: step 2814, loss 0.330596, acc 0.859375\n",
      "2017-08-02T10:56:13.828114: step 2815, loss 0.382076, acc 0.890625\n",
      "2017-08-02T10:56:14.379169: step 2816, loss 0.306584, acc 0.859375\n",
      "2017-08-02T10:56:14.797211: step 2817, loss 0.459842, acc 0.78125\n",
      "2017-08-02T10:56:15.367268: step 2818, loss 0.304763, acc 0.875\n",
      "2017-08-02T10:56:15.921323: step 2819, loss 0.306339, acc 0.890625\n",
      "2017-08-02T10:56:16.483379: step 2820, loss 0.197505, acc 0.921875\n",
      "2017-08-02T10:56:17.043435: step 2821, loss 0.308092, acc 0.890625\n",
      "2017-08-02T10:56:17.603491: step 2822, loss 0.226507, acc 0.9375\n",
      "2017-08-02T10:56:18.160547: step 2823, loss 0.289907, acc 0.890625\n",
      "2017-08-02T10:56:18.717603: step 2824, loss 0.246365, acc 0.90625\n",
      "2017-08-02T10:56:19.270658: step 2825, loss 0.215874, acc 0.921875\n",
      "2017-08-02T10:56:19.825714: step 2826, loss 0.270958, acc 0.90625\n",
      "2017-08-02T10:56:20.409772: step 2827, loss 0.214892, acc 0.9375\n",
      "2017-08-02T10:56:20.976829: step 2828, loss 0.234817, acc 0.90625\n",
      "2017-08-02T10:56:21.539885: step 2829, loss 0.201667, acc 0.953125\n",
      "2017-08-02T10:56:22.098941: step 2830, loss 0.227526, acc 0.90625\n",
      "2017-08-02T10:56:22.655997: step 2831, loss 0.230431, acc 0.9375\n",
      "2017-08-02T10:56:23.214052: step 2832, loss 0.164487, acc 1\n",
      "2017-08-02T10:56:23.773108: step 2833, loss 0.298703, acc 0.890625\n",
      "2017-08-02T10:56:24.333164: step 2834, loss 0.259881, acc 0.9375\n",
      "2017-08-02T10:56:24.896221: step 2835, loss 0.349256, acc 0.859375\n",
      "2017-08-02T10:56:25.456277: step 2836, loss 0.274234, acc 0.90625\n",
      "2017-08-02T10:56:26.013332: step 2837, loss 0.224275, acc 0.953125\n",
      "2017-08-02T10:56:26.575389: step 2838, loss 0.214973, acc 0.953125\n",
      "2017-08-02T10:56:27.133444: step 2839, loss 0.300714, acc 0.875\n",
      "2017-08-02T10:56:27.693500: step 2840, loss 0.226535, acc 0.90625\n",
      "2017-08-02T10:56:28.282559: step 2841, loss 0.212178, acc 0.9375\n",
      "2017-08-02T10:56:28.863617: step 2842, loss 0.409406, acc 0.84375\n",
      "2017-08-02T10:56:29.419673: step 2843, loss 0.2624, acc 0.90625\n",
      "2017-08-02T10:56:29.980729: step 2844, loss 0.231724, acc 0.953125\n",
      "2017-08-02T10:56:30.539785: step 2845, loss 0.279865, acc 0.921875\n",
      "2017-08-02T10:56:31.095841: step 2846, loss 0.222891, acc 0.953125\n",
      "2017-08-02T10:56:31.657897: step 2847, loss 0.301407, acc 0.921875\n",
      "2017-08-02T10:56:32.213952: step 2848, loss 0.268313, acc 0.890625\n",
      "2017-08-02T10:56:32.771008: step 2849, loss 0.235331, acc 0.921875\n",
      "2017-08-02T10:56:33.327064: step 2850, loss 0.282378, acc 0.9375\n",
      "2017-08-02T10:56:33.890120: step 2851, loss 0.302737, acc 0.859375\n",
      "2017-08-02T10:56:34.451176: step 2852, loss 0.225802, acc 0.953125\n",
      "2017-08-02T10:56:35.012232: step 2853, loss 0.197485, acc 0.921875\n",
      "2017-08-02T10:56:35.572288: step 2854, loss 0.368649, acc 0.828125\n",
      "2017-08-02T10:56:36.130344: step 2855, loss 0.314095, acc 0.859375\n",
      "2017-08-02T10:56:36.699401: step 2856, loss 0.28902, acc 0.953125\n",
      "2017-08-02T10:56:37.265457: step 2857, loss 0.273476, acc 0.921875\n",
      "2017-08-02T10:56:37.821513: step 2858, loss 0.230952, acc 0.890625\n",
      "2017-08-02T10:56:38.383569: step 2859, loss 0.237281, acc 0.9375\n",
      "2017-08-02T10:56:38.940625: step 2860, loss 0.243279, acc 0.90625\n",
      "2017-08-02T10:56:39.501681: step 2861, loss 0.250817, acc 0.921875\n",
      "2017-08-02T10:56:40.058737: step 2862, loss 0.191315, acc 0.96875\n",
      "2017-08-02T10:56:40.638795: step 2863, loss 0.228004, acc 0.9375\n",
      "2017-08-02T10:56:41.207852: step 2864, loss 0.312851, acc 0.84375\n",
      "2017-08-02T10:56:41.765907: step 2865, loss 0.252138, acc 0.921875\n",
      "2017-08-02T10:56:42.325963: step 2866, loss 0.209484, acc 0.96875\n",
      "2017-08-02T10:56:42.897021: step 2867, loss 0.270127, acc 0.90625\n",
      "2017-08-02T10:56:43.451076: step 2868, loss 0.224622, acc 0.90625\n",
      "2017-08-02T10:56:44.013132: step 2869, loss 0.247225, acc 0.953125\n",
      "2017-08-02T10:56:44.573188: step 2870, loss 0.226246, acc 0.9375\n",
      "2017-08-02T10:56:45.133244: step 2871, loss 0.246586, acc 0.9375\n",
      "2017-08-02T10:56:45.692300: step 2872, loss 0.236386, acc 0.9375\n",
      "2017-08-02T10:56:46.248356: step 2873, loss 0.264788, acc 0.890625\n",
      "2017-08-02T10:56:46.803411: step 2874, loss 0.198149, acc 0.96875\n",
      "2017-08-02T10:56:47.360467: step 2875, loss 0.208052, acc 0.96875\n",
      "2017-08-02T10:56:47.918523: step 2876, loss 0.200104, acc 0.953125\n",
      "2017-08-02T10:56:48.490580: step 2877, loss 0.261787, acc 0.875\n",
      "2017-08-02T10:56:49.046635: step 2878, loss 0.279976, acc 0.921875\n",
      "2017-08-02T10:56:49.596690: step 2879, loss 0.402722, acc 0.84375\n",
      "2017-08-02T10:56:50.153746: step 2880, loss 0.239292, acc 0.90625\n",
      "2017-08-02T10:56:50.713802: step 2881, loss 0.263385, acc 0.890625\n",
      "2017-08-02T10:56:51.269858: step 2882, loss 0.224325, acc 0.890625\n",
      "2017-08-02T10:56:51.824913: step 2883, loss 0.314898, acc 0.859375\n",
      "2017-08-02T10:56:52.385969: step 2884, loss 0.28043, acc 0.859375\n",
      "2017-08-02T10:56:52.938025: step 2885, loss 0.338694, acc 0.875\n",
      "2017-08-02T10:56:53.503081: step 2886, loss 0.183134, acc 0.921875\n",
      "2017-08-02T10:56:54.069138: step 2887, loss 0.227835, acc 0.9375\n",
      "2017-08-02T10:56:54.631194: step 2888, loss 0.213104, acc 0.953125\n",
      "2017-08-02T10:56:55.190250: step 2889, loss 0.214161, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:56:55.750306: step 2890, loss 0.173207, acc 0.953125\n",
      "2017-08-02T10:56:56.311362: step 2891, loss 0.172783, acc 0.96875\n",
      "2017-08-02T10:56:56.874418: step 2892, loss 0.131257, acc 1\n",
      "2017-08-02T10:56:57.431474: step 2893, loss 0.300649, acc 0.890625\n",
      "2017-08-02T10:56:57.986529: step 2894, loss 0.271783, acc 0.890625\n",
      "2017-08-02T10:56:58.544585: step 2895, loss 0.29316, acc 0.875\n",
      "2017-08-02T10:56:59.105641: step 2896, loss 0.212363, acc 0.953125\n",
      "2017-08-02T10:56:59.662697: step 2897, loss 0.225315, acc 0.953125\n",
      "2017-08-02T10:57:00.219753: step 2898, loss 0.242321, acc 0.953125\n",
      "2017-08-02T10:57:00.776808: step 2899, loss 0.240791, acc 0.90625\n",
      "2017-08-02T10:57:01.333864: step 2900, loss 0.23962, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:57:04.984229: step 2900, loss 0.424681, acc 0.8316\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-2900\n",
      "\n",
      "2017-08-02T10:57:07.561487: step 2901, loss 0.259371, acc 0.96875\n",
      "2017-08-02T10:57:08.130544: step 2902, loss 0.275267, acc 0.890625\n",
      "2017-08-02T10:57:08.688599: step 2903, loss 0.264191, acc 0.890625\n",
      "2017-08-02T10:57:09.247655: step 2904, loss 0.388828, acc 0.875\n",
      "2017-08-02T10:57:09.808711: step 2905, loss 0.206894, acc 0.90625\n",
      "2017-08-02T10:57:10.367767: step 2906, loss 0.374282, acc 0.859375\n",
      "2017-08-02T10:57:10.925823: step 2907, loss 0.208673, acc 0.9375\n",
      "2017-08-02T10:57:11.485879: step 2908, loss 0.246626, acc 0.921875\n",
      "2017-08-02T10:57:12.046935: step 2909, loss 0.197341, acc 0.96875\n",
      "2017-08-02T10:57:12.606991: step 2910, loss 0.26683, acc 0.9375\n",
      "2017-08-02T10:57:13.162047: step 2911, loss 0.234765, acc 0.9375\n",
      "2017-08-02T10:57:13.720103: step 2912, loss 0.302863, acc 0.921875\n",
      "2017-08-02T10:57:14.281159: step 2913, loss 0.28079, acc 0.9375\n",
      "2017-08-02T10:57:14.847215: step 2914, loss 0.271737, acc 0.90625\n",
      "2017-08-02T10:57:15.404271: step 2915, loss 0.240404, acc 0.953125\n",
      "2017-08-02T10:57:15.976328: step 2916, loss 0.257089, acc 0.921875\n",
      "2017-08-02T10:57:16.548385: step 2917, loss 0.248523, acc 0.9375\n",
      "2017-08-02T10:57:17.133444: step 2918, loss 0.164768, acc 0.96875\n",
      "2017-08-02T10:57:17.692500: step 2919, loss 0.3189, acc 0.859375\n",
      "2017-08-02T10:57:18.248555: step 2920, loss 0.293243, acc 0.890625\n",
      "2017-08-02T10:57:18.809611: step 2921, loss 0.358819, acc 0.875\n",
      "2017-08-02T10:57:19.371668: step 2922, loss 0.181647, acc 0.984375\n",
      "2017-08-02T10:57:19.931724: step 2923, loss 0.245801, acc 0.921875\n",
      "2017-08-02T10:57:20.488779: step 2924, loss 0.194969, acc 0.96875\n",
      "2017-08-02T10:57:21.045835: step 2925, loss 0.277551, acc 0.921875\n",
      "2017-08-02T10:57:21.606891: step 2926, loss 0.240872, acc 0.96875\n",
      "2017-08-02T10:57:22.167947: step 2927, loss 0.306515, acc 0.890625\n",
      "2017-08-02T10:57:22.729003: step 2928, loss 0.208673, acc 0.9375\n",
      "2017-08-02T10:57:23.292060: step 2929, loss 0.206955, acc 0.96875\n",
      "2017-08-02T10:57:23.855116: step 2930, loss 0.251145, acc 0.9375\n",
      "2017-08-02T10:57:24.414172: step 2931, loss 0.221889, acc 0.953125\n",
      "2017-08-02T10:57:24.970227: step 2932, loss 0.185232, acc 0.96875\n",
      "2017-08-02T10:57:25.531284: step 2933, loss 0.200434, acc 0.9375\n",
      "2017-08-02T10:57:26.091340: step 2934, loss 0.246695, acc 0.921875\n",
      "2017-08-02T10:57:26.645395: step 2935, loss 0.167746, acc 0.984375\n",
      "2017-08-02T10:57:27.218452: step 2936, loss 0.25453, acc 0.890625\n",
      "2017-08-02T10:57:27.776508: step 2937, loss 0.241186, acc 0.90625\n",
      "2017-08-02T10:57:28.338564: step 2938, loss 0.176287, acc 0.984375\n",
      "2017-08-02T10:57:28.910621: step 2939, loss 0.276933, acc 0.921875\n",
      "2017-08-02T10:57:29.482679: step 2940, loss 0.170664, acc 1\n",
      "2017-08-02T10:57:30.041735: step 2941, loss 0.203384, acc 0.9375\n",
      "2017-08-02T10:57:30.597790: step 2942, loss 0.425859, acc 0.859375\n",
      "2017-08-02T10:57:31.155846: step 2943, loss 0.214443, acc 0.90625\n",
      "2017-08-02T10:57:31.715902: step 2944, loss 0.255367, acc 0.921875\n",
      "2017-08-02T10:57:32.286959: step 2945, loss 0.435841, acc 0.875\n",
      "2017-08-02T10:57:32.871017: step 2946, loss 0.188306, acc 0.9375\n",
      "2017-08-02T10:57:33.454076: step 2947, loss 0.257428, acc 0.90625\n",
      "2017-08-02T10:57:34.027133: step 2948, loss 0.385642, acc 0.859375\n",
      "2017-08-02T10:57:34.606191: step 2949, loss 0.277869, acc 0.90625\n",
      "2017-08-02T10:57:35.185249: step 2950, loss 0.254942, acc 0.921875\n",
      "2017-08-02T10:57:35.756306: step 2951, loss 0.261764, acc 0.921875\n",
      "2017-08-02T10:57:36.327363: step 2952, loss 0.293263, acc 0.875\n",
      "2017-08-02T10:57:36.899420: step 2953, loss 0.270204, acc 0.90625\n",
      "2017-08-02T10:57:37.470477: step 2954, loss 0.320028, acc 0.875\n",
      "2017-08-02T10:57:38.040534: step 2955, loss 0.284157, acc 0.90625\n",
      "2017-08-02T10:57:38.610591: step 2956, loss 0.268746, acc 0.890625\n",
      "2017-08-02T10:57:39.188649: step 2957, loss 0.140135, acc 1\n",
      "2017-08-02T10:57:39.755706: step 2958, loss 0.249594, acc 0.9375\n",
      "2017-08-02T10:57:40.320762: step 2959, loss 0.257202, acc 0.90625\n",
      "2017-08-02T10:57:40.893820: step 2960, loss 0.160906, acc 0.96875\n",
      "2017-08-02T10:57:41.462877: step 2961, loss 0.298706, acc 0.890625\n",
      "2017-08-02T10:57:42.038934: step 2962, loss 0.257964, acc 0.921875\n",
      "2017-08-02T10:57:42.612992: step 2963, loss 0.186714, acc 0.953125\n",
      "2017-08-02T10:57:43.189049: step 2964, loss 0.17068, acc 0.984375\n",
      "2017-08-02T10:57:43.762106: step 2965, loss 0.38331, acc 0.84375\n",
      "2017-08-02T10:57:44.352165: step 2966, loss 0.248187, acc 0.90625\n",
      "2017-08-02T10:57:44.923223: step 2967, loss 0.207719, acc 0.921875\n",
      "2017-08-02T10:57:45.507281: step 2968, loss 0.186776, acc 0.953125\n",
      "2017-08-02T10:57:46.079338: step 2969, loss 0.349084, acc 0.875\n",
      "2017-08-02T10:57:46.656396: step 2970, loss 0.247308, acc 0.90625\n",
      "2017-08-02T10:57:47.225453: step 2971, loss 0.290764, acc 0.875\n",
      "2017-08-02T10:57:47.803511: step 2972, loss 0.233108, acc 0.9375\n",
      "2017-08-02T10:57:48.390569: step 2973, loss 0.224334, acc 0.9375\n",
      "2017-08-02T10:57:48.967627: step 2974, loss 0.317867, acc 0.859375\n",
      "2017-08-02T10:57:49.532683: step 2975, loss 0.370047, acc 0.828125\n",
      "2017-08-02T10:57:50.093740: step 2976, loss 0.25428, acc 0.9375\n",
      "2017-08-02T10:57:50.676798: step 2977, loss 0.21118, acc 0.953125\n",
      "2017-08-02T10:57:51.243855: step 2978, loss 0.221513, acc 0.953125\n",
      "2017-08-02T10:57:51.806911: step 2979, loss 0.167618, acc 0.96875\n",
      "2017-08-02T10:57:52.363967: step 2980, loss 0.163519, acc 0.953125\n",
      "2017-08-02T10:57:52.921022: step 2981, loss 0.168422, acc 0.953125\n",
      "2017-08-02T10:57:53.483078: step 2982, loss 0.251091, acc 0.890625\n",
      "2017-08-02T10:57:54.048135: step 2983, loss 0.319209, acc 0.890625\n",
      "2017-08-02T10:57:54.606191: step 2984, loss 0.239839, acc 0.921875\n",
      "2017-08-02T10:57:55.166247: step 2985, loss 0.344564, acc 0.890625\n",
      "2017-08-02T10:57:55.724303: step 2986, loss 0.260628, acc 0.890625\n",
      "2017-08-02T10:57:56.283358: step 2987, loss 0.258458, acc 0.875\n",
      "2017-08-02T10:57:56.883418: step 2988, loss 0.242728, acc 0.9375\n",
      "2017-08-02T10:57:57.445475: step 2989, loss 0.303063, acc 0.90625\n",
      "2017-08-02T10:57:58.011531: step 2990, loss 0.252139, acc 0.921875\n",
      "2017-08-02T10:57:58.572587: step 2991, loss 0.273116, acc 0.90625\n",
      "2017-08-02T10:57:59.130643: step 2992, loss 0.189798, acc 0.984375\n",
      "2017-08-02T10:57:59.688699: step 2993, loss 0.260412, acc 0.9375\n",
      "2017-08-02T10:58:00.248755: step 2994, loss 0.268069, acc 0.90625\n",
      "2017-08-02T10:58:00.806811: step 2995, loss 0.342152, acc 0.90625\n",
      "2017-08-02T10:58:01.361866: step 2996, loss 0.280172, acc 0.953125\n",
      "2017-08-02T10:58:01.941924: step 2997, loss 0.277215, acc 0.921875\n",
      "2017-08-02T10:58:02.503980: step 2998, loss 0.210845, acc 0.9375\n",
      "2017-08-02T10:58:03.067037: step 2999, loss 0.292921, acc 0.875\n",
      "2017-08-02T10:58:03.628093: step 3000, loss 0.246001, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:58:07.367467: step 3000, loss 0.416428, acc 0.8324\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-3000\n",
      "\n",
      "2017-08-02T10:58:10.044734: step 3001, loss 0.217875, acc 0.953125\n",
      "2017-08-02T10:58:10.599790: step 3002, loss 0.15681, acc 0.96875\n",
      "2017-08-02T10:58:11.157846: step 3003, loss 0.362155, acc 0.875\n",
      "2017-08-02T10:58:11.733903: step 3004, loss 0.357127, acc 0.828125\n",
      "2017-08-02T10:58:12.289959: step 3005, loss 0.321181, acc 0.890625\n",
      "2017-08-02T10:58:12.851015: step 3006, loss 0.305031, acc 0.90625\n",
      "2017-08-02T10:58:13.414071: step 3007, loss 0.189538, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:58:13.978128: step 3008, loss 0.28372, acc 0.9375\n",
      "2017-08-02T10:58:14.538184: step 3009, loss 0.273341, acc 0.859375\n",
      "2017-08-02T10:58:15.115241: step 3010, loss 0.230825, acc 0.90625\n",
      "2017-08-02T10:58:15.666297: step 3011, loss 0.213888, acc 0.953125\n",
      "2017-08-02T10:58:16.217352: step 3012, loss 0.27802, acc 0.921875\n",
      "2017-08-02T10:58:16.770407: step 3013, loss 0.196034, acc 0.9375\n",
      "2017-08-02T10:58:17.335463: step 3014, loss 0.231401, acc 0.90625\n",
      "2017-08-02T10:58:17.900520: step 3015, loss 0.311814, acc 0.921875\n",
      "2017-08-02T10:58:18.454575: step 3016, loss 0.311643, acc 0.859375\n",
      "2017-08-02T10:58:19.012631: step 3017, loss 0.273509, acc 0.921875\n",
      "2017-08-02T10:58:19.570687: step 3018, loss 0.22154, acc 0.921875\n",
      "2017-08-02T10:58:20.130743: step 3019, loss 0.303899, acc 0.890625\n",
      "2017-08-02T10:58:20.706801: step 3020, loss 0.189053, acc 0.9375\n",
      "2017-08-02T10:58:21.258856: step 3021, loss 0.223729, acc 0.90625\n",
      "2017-08-02T10:58:21.821912: step 3022, loss 0.343081, acc 0.890625\n",
      "2017-08-02T10:58:22.374967: step 3023, loss 0.262189, acc 0.90625\n",
      "2017-08-02T10:58:22.941024: step 3024, loss 0.224669, acc 0.953125\n",
      "2017-08-02T10:58:23.500080: step 3025, loss 0.273367, acc 0.890625\n",
      "2017-08-02T10:58:24.061136: step 3026, loss 0.25909, acc 0.90625\n",
      "2017-08-02T10:58:24.618192: step 3027, loss 0.231863, acc 0.9375\n",
      "2017-08-02T10:58:25.183248: step 3028, loss 0.246113, acc 0.90625\n",
      "2017-08-02T10:58:25.742304: step 3029, loss 0.20659, acc 0.96875\n",
      "2017-08-02T10:58:26.297360: step 3030, loss 0.215362, acc 0.953125\n",
      "2017-08-02T10:58:26.856415: step 3031, loss 0.24136, acc 0.921875\n",
      "2017-08-02T10:58:27.415471: step 3032, loss 0.247578, acc 0.921875\n",
      "2017-08-02T10:58:27.969527: step 3033, loss 0.374119, acc 0.8125\n",
      "2017-08-02T10:58:28.541584: step 3034, loss 0.253487, acc 0.90625\n",
      "2017-08-02T10:58:29.100640: step 3035, loss 0.229409, acc 0.90625\n",
      "2017-08-02T10:58:29.669697: step 3036, loss 0.218819, acc 0.9375\n",
      "2017-08-02T10:58:30.241754: step 3037, loss 0.254591, acc 0.9375\n",
      "2017-08-02T10:58:30.798810: step 3038, loss 0.203595, acc 0.96875\n",
      "2017-08-02T10:58:31.353865: step 3039, loss 0.204174, acc 0.953125\n",
      "2017-08-02T10:58:31.915921: step 3040, loss 0.239263, acc 0.953125\n",
      "2017-08-02T10:58:32.471977: step 3041, loss 0.18652, acc 0.96875\n",
      "2017-08-02T10:58:33.033033: step 3042, loss 0.209674, acc 0.921875\n",
      "2017-08-02T10:58:33.595089: step 3043, loss 0.229956, acc 0.953125\n",
      "2017-08-02T10:58:34.172147: step 3044, loss 0.241906, acc 0.921875\n",
      "2017-08-02T10:58:34.731203: step 3045, loss 0.354708, acc 0.890625\n",
      "2017-08-02T10:58:35.286258: step 3046, loss 0.240051, acc 0.9375\n",
      "2017-08-02T10:58:35.845314: step 3047, loss 0.248087, acc 0.890625\n",
      "2017-08-02T10:58:36.400370: step 3048, loss 0.229531, acc 0.9375\n",
      "2017-08-02T10:58:36.978428: step 3049, loss 0.280531, acc 0.890625\n",
      "2017-08-02T10:58:37.539484: step 3050, loss 0.285405, acc 0.890625\n",
      "2017-08-02T10:58:38.096539: step 3051, loss 0.241068, acc 0.921875\n",
      "2017-08-02T10:58:38.651595: step 3052, loss 0.244301, acc 0.890625\n",
      "2017-08-02T10:58:39.210651: step 3053, loss 0.253384, acc 0.921875\n",
      "2017-08-02T10:58:39.768707: step 3054, loss 0.285251, acc 0.890625\n",
      "2017-08-02T10:58:40.330763: step 3055, loss 0.175657, acc 0.96875\n",
      "2017-08-02T10:58:40.887818: step 3056, loss 0.300531, acc 0.890625\n",
      "2017-08-02T10:58:41.442874: step 3057, loss 0.317732, acc 0.921875\n",
      "2017-08-02T10:58:42.001930: step 3058, loss 0.269663, acc 0.921875\n",
      "2017-08-02T10:58:42.563986: step 3059, loss 0.252986, acc 0.921875\n",
      "2017-08-02T10:58:43.122042: step 3060, loss 0.208971, acc 0.921875\n",
      "2017-08-02T10:58:43.683098: step 3061, loss 0.159204, acc 0.96875\n",
      "2017-08-02T10:58:44.243154: step 3062, loss 0.267191, acc 0.90625\n",
      "2017-08-02T10:58:44.808210: step 3063, loss 0.205048, acc 0.9375\n",
      "2017-08-02T10:58:45.368266: step 3064, loss 0.302678, acc 0.875\n",
      "2017-08-02T10:58:45.925322: step 3065, loss 0.325739, acc 0.921875\n",
      "2017-08-02T10:58:46.483378: step 3066, loss 0.373277, acc 0.875\n",
      "2017-08-02T10:58:47.041434: step 3067, loss 0.177508, acc 0.984375\n",
      "2017-08-02T10:58:47.615491: step 3068, loss 0.208684, acc 0.9375\n",
      "2017-08-02T10:58:48.172547: step 3069, loss 0.167878, acc 0.984375\n",
      "2017-08-02T10:58:48.728602: step 3070, loss 0.184795, acc 0.953125\n",
      "2017-08-02T10:58:49.291659: step 3071, loss 0.246228, acc 0.9375\n",
      "2017-08-02T10:58:49.845714: step 3072, loss 0.296382, acc 0.90625\n",
      "2017-08-02T10:58:50.407770: step 3073, loss 0.412429, acc 0.796875\n",
      "2017-08-02T10:58:50.967826: step 3074, loss 0.208266, acc 0.9375\n",
      "2017-08-02T10:58:51.522882: step 3075, loss 0.251992, acc 0.90625\n",
      "2017-08-02T10:58:52.077937: step 3076, loss 0.173618, acc 0.984375\n",
      "2017-08-02T10:58:52.638993: step 3077, loss 0.25015, acc 0.96875\n",
      "2017-08-02T10:58:53.192049: step 3078, loss 0.239707, acc 0.921875\n",
      "2017-08-02T10:58:53.747104: step 3079, loss 0.236637, acc 0.890625\n",
      "2017-08-02T10:58:54.304160: step 3080, loss 0.250827, acc 0.890625\n",
      "2017-08-02T10:58:54.866216: step 3081, loss 0.183171, acc 0.953125\n",
      "2017-08-02T10:58:55.429272: step 3082, loss 0.264941, acc 0.890625\n",
      "2017-08-02T10:58:56.001330: step 3083, loss 0.349925, acc 0.84375\n",
      "2017-08-02T10:58:56.562386: step 3084, loss 0.34211, acc 0.875\n",
      "2017-08-02T10:58:57.132443: step 3085, loss 0.243886, acc 0.90625\n",
      "2017-08-02T10:58:57.691499: step 3086, loss 0.223496, acc 0.921875\n",
      "2017-08-02T10:58:58.251555: step 3087, loss 0.228448, acc 0.9375\n",
      "2017-08-02T10:58:58.809610: step 3088, loss 0.209773, acc 0.96875\n",
      "2017-08-02T10:58:59.366666: step 3089, loss 0.216388, acc 0.9375\n",
      "2017-08-02T10:58:59.925722: step 3090, loss 0.285129, acc 0.875\n",
      "2017-08-02T10:59:00.484778: step 3091, loss 0.318408, acc 0.875\n",
      "2017-08-02T10:59:01.045834: step 3092, loss 0.254822, acc 0.921875\n",
      "2017-08-02T10:59:01.611891: step 3093, loss 0.246374, acc 0.9375\n",
      "2017-08-02T10:59:02.171947: step 3094, loss 0.267796, acc 0.90625\n",
      "2017-08-02T10:59:02.727002: step 3095, loss 0.267686, acc 0.90625\n",
      "2017-08-02T10:59:03.282058: step 3096, loss 0.313647, acc 0.921875\n",
      "2017-08-02T10:59:03.833113: step 3097, loss 0.315123, acc 0.90625\n",
      "2017-08-02T10:59:04.390168: step 3098, loss 0.164238, acc 1\n",
      "2017-08-02T10:59:04.963226: step 3099, loss 0.237318, acc 0.90625\n",
      "2017-08-02T10:59:05.528282: step 3100, loss 0.243848, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-08-02T10:59:09.266656: step 3100, loss 0.414298, acc 0.832\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-3100\n",
      "\n",
      "2017-08-02T10:59:11.867916: step 3101, loss 0.26728, acc 0.875\n",
      "2017-08-02T10:59:12.436973: step 3102, loss 0.255919, acc 0.921875\n",
      "2017-08-02T10:59:12.998029: step 3103, loss 0.276002, acc 0.90625\n",
      "2017-08-02T10:59:13.557085: step 3104, loss 0.200724, acc 0.96875\n",
      "2017-08-02T10:59:14.112141: step 3105, loss 0.243473, acc 0.890625\n",
      "2017-08-02T10:59:14.671196: step 3106, loss 0.311162, acc 0.90625\n",
      "2017-08-02T10:59:15.235253: step 3107, loss 0.244471, acc 0.90625\n",
      "2017-08-02T10:59:15.798309: step 3108, loss 0.218536, acc 0.9375\n",
      "2017-08-02T10:59:16.361365: step 3109, loss 0.257771, acc 0.90625\n",
      "2017-08-02T10:59:16.925422: step 3110, loss 0.199577, acc 0.9375\n",
      "2017-08-02T10:59:17.483478: step 3111, loss 0.291159, acc 0.890625\n",
      "2017-08-02T10:59:18.039533: step 3112, loss 0.247236, acc 0.921875\n",
      "2017-08-02T10:59:18.595589: step 3113, loss 0.230358, acc 0.953125\n",
      "2017-08-02T10:59:19.169646: step 3114, loss 0.249333, acc 0.921875\n",
      "2017-08-02T10:59:19.732703: step 3115, loss 0.19723, acc 0.953125\n",
      "2017-08-02T10:59:20.288758: step 3116, loss 0.294058, acc 0.890625\n",
      "2017-08-02T10:59:20.848814: step 3117, loss 0.31098, acc 0.890625\n",
      "2017-08-02T10:59:21.409870: step 3118, loss 0.188012, acc 0.953125\n",
      "2017-08-02T10:59:21.964926: step 3119, loss 0.267569, acc 0.90625\n",
      "2017-08-02T10:59:22.524982: step 3120, loss 0.161806, acc 0.953125\n",
      "2017-08-02T10:59:23.086038: step 3121, loss 0.199163, acc 0.953125\n",
      "2017-08-02T10:59:23.642093: step 3122, loss 0.170527, acc 0.984375\n",
      "2017-08-02T10:59:24.202149: step 3123, loss 0.2292, acc 0.921875\n",
      "2017-08-02T10:59:24.767206: step 3124, loss 0.219179, acc 0.9375\n",
      "2017-08-02T10:59:25.327262: step 3125, loss 0.302456, acc 0.9375\n",
      "2017-08-02T10:59:25.891318: step 3126, loss 0.314731, acc 0.921875\n",
      "2017-08-02T10:59:26.448374: step 3127, loss 0.222619, acc 0.921875\n",
      "2017-08-02T10:59:27.014431: step 3128, loss 0.259801, acc 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-02T10:59:27.569486: step 3129, loss 0.270102, acc 0.890625\n",
      "2017-08-02T10:59:27.982527: step 3130, loss 0.310269, acc 0.9375\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=74073,#len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\E411208\\Documents\\Python Scripts\\cnn-text-classification-tf\\runs\\1501662405\\checkpoints\\model-3100\n",
      "Total number of test examples: 2500\n",
      "Accuracy: 0.8228\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(os.path.join('runs/1501662405', \"checkpoints\"))\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
