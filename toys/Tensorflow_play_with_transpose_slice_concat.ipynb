{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Try the multiplication with constant as matrix operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[0.1, 0.1],[0.1, 0.1]])\n",
    "b = np.array([[1, 2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w =np.array([[['000', '001'],['010', '011']],[['100', '101'],['110', '111']]])\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the matrix instead of transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_30_in = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/29_fc_out.npy')\n",
    "fc_30_out = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_out.npy')\n",
    "fc_30_weight = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_weight.npy')\n",
    "fc_30_bias = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_bias.npy')\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc_30_in.shape)\n",
    "print(fc_30_weight.shape)\n",
    "print(fc_30_bias.shape)\n",
    "print(fc_30_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder('float32',[1,1,1,512])\n",
    "\n",
    "def fc_layer_nolinear(idx,inputs,hiddens,alpha):\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    inputs = tf.transpose(inputs,(0,3,1,2))\n",
    "    ip = tf.layers.conv2d(inputs = inputs, filters = hiddens, kernel_size = [input_shape[3], input_shape[1]], strides=[1, 1], use_bias=True,  bias_initializer=tf.zeros_initializer(), padding='valid', name=str(idx)+'_fc')\n",
    "    return tf.maximum(tf.multiply(alpha,ip),ip,name=str(idx)+'_fc_relu')\n",
    "\n",
    "fc_30 = fc_layer_nolinear(30,inputs,4096,0.1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights\n",
    "idx = 30\n",
    "for v in tf.trainable_variables(): print(v)\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/kernel:0'][0]  \n",
    "var.load(fc_30_weight,sess)\n",
    "# load the bias\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/bias:0'][0]  \n",
    "var.load(fc_30_bias,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the fc_30 layer\n",
    "in_dict = {inputs: fc_30_in}\n",
    "out = sess.run(fc_30,feed_dict=in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(out-fc_30_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the weight and input and get same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_30_weight_shuffle = np.transpose(fc_30_weight,(1,2,0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_29_weight = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/29_fc_weight.npy')\n",
    "fc_29_weight_shuffle = np.transpose(fc_29_weight,(1,2,0,3))\n",
    "np.save('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/fc_29_weight_shuffle',fc_29_weight_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc_29_weight.shape)\n",
    "print(fc_29_weight_shuffle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(fc_30_weight_shuffle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder('float32',[1,1,1,512])\n",
    "\n",
    "def fc_layer_nolinear(idx,inputs,hiddens,alpha):\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    #inputs = tf.transpose(inputs,(0,3,1,2))\n",
    "    ip = tf.layers.conv2d(inputs = inputs, filters = hiddens, kernel_size = [input_shape[1], input_shape[2]], strides=[1, 1], use_bias=True,  bias_initializer=tf.zeros_initializer(), padding='valid', name=str(idx)+'_fc')\n",
    "    return tf.maximum(tf.multiply(alpha,ip),ip,name=str(idx)+'_fc_relu')\n",
    "\n",
    "fc_33 = fc_layer_nolinear(33,inputs,4096,0.1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights\n",
    "idx = 33\n",
    "for v in tf.trainable_variables(): print(v)\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/kernel:0'][0]  \n",
    "var.load(fc_30_weight_shuffle,sess)\n",
    "# load the bias\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/bias:0'][0]  \n",
    "var.load(fc_30_bias,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the fc_33 layer\n",
    "in_dict = {inputs: fc_30_in}\n",
    "out = sess.run(fc_33,feed_dict=in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(out-fc_30_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/fc_30_weight_shuffle',fc_30_weight_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load the alpha as another input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_30_in = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/29_fc_out.npy')\n",
    "fc_30_out = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_out.npy')\n",
    "fc_30_weight = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_weight.npy')\n",
    "fc_30_bias = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/30_fc_bias.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder('float32',[1,1,1,512])\n",
    "alpha = tf.placeholder('float32',[1,1,1,1])\n",
    "\n",
    "def fc_layer_nolinear(idx,inputs,hiddens,alpha):\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    inputs = tf.transpose(inputs,(0,3,1,2))\n",
    "    ip = tf.layers.conv2d(inputs = inputs, filters = hiddens, kernel_size = [input_shape[3], input_shape[1]], strides=[1, 1], use_bias=True,  bias_initializer=tf.zeros_initializer(), padding='valid', name=str(idx)+'_fc')\n",
    "    return tf.maximum(tf.multiply(alpha,ip),ip,name=str(idx)+'_fc_relu')\n",
    "\n",
    "fc_30 = fc_layer_nolinear(30,inputs,4096,0.1)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights\n",
    "idx = 30\n",
    "for v in tf.trainable_variables(): print(v)\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/kernel:0'][0]  \n",
    "var.load(fc_30_weight,sess)\n",
    "# load the bias\n",
    "var = [v for v in tf.trainable_variables() if v.name == str(idx) + '_fc' + '/bias:0'][0]  \n",
    "var.load(fc_30_bias,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.zeros((1,1,1,1),dtype='float32')\n",
    "a[0] = np.array([0.1])\n",
    "# run the fc_30 layer\n",
    "in_dict = {inputs: fc_30_in, alpha : a}\n",
    "out = sess.run(fc_30,feed_dict=in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(out-fc_30_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op = sess.graph.get_operations()\n",
    "for m in op:\n",
    "    print(m.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiply matrix with constant via 1D convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder('float32',[None,2,10,2])\n",
    "#W = tf.placeholder('float32',[None,1,1,1])\n",
    "\n",
    "def fc_layer(inputs):\n",
    "    #return tf.layers.conv2d(inputs = inputs, filters = 1, kernel_size = [1, 1], strides=[1, 1], padding='valid', name=str(idx)+'_alpha')\n",
    "    #input_shape = inputs.get_shape().as_list()\n",
    "    # 1D convolution as multiplication with constant\n",
    "    #inputs_reshape = tf.reshape(inputs,[-1,input_shape[1],input_shape[2]*input_shape[3],1])\n",
    "    ##conv = tf.nn.conv2d(inputs_reshape,W,strides=[1,1,1,1],padding='SAME')\n",
    "    #conv_reshape = tf.layers.conv2d(inputs = inputs_reshape, filters = 1, kernel_size = [1, 1], strides=[1, 1], padding='valid', name='alpha')    \n",
    "    #return  tf.reshape(conv,[-1,input_shape[1],input_shape[2],input_shape[3]])\n",
    "    return tf.layers.conv2d(inputs = inputs, filters = 2, kernel_size = [1, 1], strides=[1, 1], padding='valid', name='alpha')    \n",
    "    \n",
    "\n",
    "\n",
    "# def createWeight(size,Name):\n",
    "#     return tf.Variable(tf.ones(size)*0.1,name=Name)\n",
    "    \n",
    "# W_conv1_1x1_1 = createWeight([1,1,1,1],'W_conv1_1x1_1')\n",
    "#alpha_01 = fc_layer(inputs,W_conv1_1x1_1)\n",
    "alpha_01 = fc_layer(inputs)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in tf.trainable_variables(): print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.diagflat(np.ones([2]))*0.1\n",
    "w = np.reshape(w,[1,1,2,2])\n",
    "var = [v for v in tf.trainable_variables() if v.name == 'alpha/kernel:0'][0]  \n",
    "var.load(w,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diagflat(np.ones([2]))*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = np.random.rand(1,2,10,2)\n",
    "#w = np.ones([1,1,1,1])*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dict = {inputs: input}\n",
    "out = sess.run(alpha_01,feed_dict=in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  SLICE & CONCATENATE\n",
    "\n",
    "input : 1x7x7x1024\n",
    "weight: 7x7x1024x256\n",
    "bias:   256\n",
    "output: 1x1x1x256\n",
    "\n",
    "need to split the input and weight to 4 Blocks & get the same output as the original convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "fc_16_in = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/fc_15_out.npy')\n",
    "fc_16_out = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/fc_16_out.npy')\n",
    "fc_16_weight = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/16_fc_weights.npy')\n",
    "fc_16_bias = np.load('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/16_fc_biases.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 1024)\n",
      "(1, 1, 1, 256)\n",
      "(50176, 256)\n",
      "(7, 7, 1024, 256)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "input_shape = fc_16_in.shape\n",
    "hiddens = fc_16_bias.shape[0] \n",
    "w = np.reshape(fc_16_weight,(input_shape[3],input_shape[1],input_shape[2],hiddens))\n",
    "w = np.transpose(w,(1,2,0,3))\n",
    "b = fc_16_bias\n",
    "b0 = b/4\n",
    "print(input_shape)\n",
    "print(fc_16_out.shape)\n",
    "print(fc_16_weight.shape)\n",
    "print(w.shape)\n",
    "print(fc_16_bias.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.placeholder('float32',[None,7,7,1024])\n",
    "in_split = tf.placeholder('float32',[None,7,7,256])\n",
    "\n",
    "def fc_layer_linear(idx,inputs,hiddens): #,flat = False,linear = True):\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    return tf.layers.conv2d(inputs = inputs, filters = hiddens, kernel_size = [input_shape[1], input_shape[2]], strides=[1, 1], use_bias=True,  bias_initializer=tf.zeros_initializer(), padding='valid', name=str(idx)+'_fc')\n",
    "\n",
    "def fc_layer_linear_split(idx,in_split,hiddens): #,flat = False,linear = True):\n",
    "    input_shape = in_split.get_shape().as_list()\n",
    "    return tf.layers.conv2d(inputs = in_split, filters = hiddens, kernel_size = [input_shape[1], input_shape[2]], strides=[1, 1], use_bias=True,  bias_initializer=tf.zeros_initializer(), padding='valid', name=str(idx)+'_fc')\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup session\n",
    "fc_16 = fc_layer_linear(16,inputs,hiddens)\n",
    "# load weights\n",
    "var = [v for v in tf.trainable_variables() if v.name == '16_fc/kernel:0'][0]  \n",
    "var.load(w,sess)\n",
    "var = [v for v in tf.trainable_variables() if v.name == '16_fc/bias:0'][0]  \n",
    "var.load(b,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "     0.  0.  0.  0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# run session\n",
    "in_dict = {inputs: fc_16_in}\n",
    "out = sess.run(fc_16,feed_dict=in_dict)\n",
    "print(out-fc_16_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_split0, in_split1, in_split2, in_split3 = tf.split(fc_16_in, num_or_size_splits=4, axis=3)\n",
    "w_split0, w_split1, w_split2, w_split3 = tf.split(w, num_or_size_splits=4, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = sess.run(w_split3)\n",
    "np.save('c:/!work/2016/CENTAUR/code/YOLO_tensorflow/exp/163_fc_weights.npy',var)\n",
    "#print(type(var))\n",
    "#print(type(fc_16_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 7, 256)\n",
      "(1, 7, 7, 256)\n",
      "(1, 7, 7, 256)\n",
      "(1, 7, 7, 256)\n",
      "(7, 7, 256, 256)\n",
      "(7, 7, 256, 256)\n",
      "(7, 7, 256, 256)\n",
      "(7, 7, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(in_split0.shape)\n",
    "print(in_split1.shape)\n",
    "print(in_split2.shape)\n",
    "print(in_split3.shape)\n",
    "print(w_split0.shape)\n",
    "print(w_split1.shape)\n",
    "print(w_split2.shape)\n",
    "print(w_split3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup session\n",
    "fc_16_split0 = fc_layer_linear_split(160,in_split,hiddens)\n",
    "fc_16_split1 = fc_layer_linear_split(161,in_split,hiddens)\n",
    "fc_16_split2 = fc_layer_linear_split(162,in_split,hiddens)\n",
    "fc_16_split3 = fc_layer_linear_split(163,in_split,hiddens)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# load weights\n",
    "var = [v for v in tf.trainable_variables() if v.name == '160_fc/kernel:0'][0]  \n",
    "var.load(sess.run(w_split0),sess)\n",
    "var = [v for v in tf.trainable_variables() if v.name == '160_fc/bias:0'][0]  \n",
    "var.load(b0,sess)\n",
    "\n",
    "var = [v for v in tf.trainable_variables() if v.name == '161_fc/kernel:0'][0]  \n",
    "var.load(sess.run(w_split1),sess)\n",
    "var = [v for v in tf.trainable_variables() if v.name == '161_fc/bias:0'][0]  \n",
    "var.load(b0,sess)\n",
    "\n",
    "var = [v for v in tf.trainable_variables() if v.name == '162_fc/kernel:0'][0]  \n",
    "var.load(sess.run(w_split2),sess)\n",
    "var = [v for v in tf.trainable_variables() if v.name == '162_fc/bias:0'][0]  \n",
    "var.load(b0,sess)\n",
    "\n",
    "var = [v for v in tf.trainable_variables() if v.name == '163_fc/kernel:0'][0]  \n",
    "var.load(sess.run(w_split3),sess)\n",
    "var = [v for v in tf.trainable_variables() if v.name == '163_fc/bias:0'][0]  \n",
    "var.load(b0,sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable '16_fc/kernel:0' shape=(7, 7, 1024, 256) dtype=float32_ref>\n",
      "<tf.Variable '16_fc/bias:0' shape=(256,) dtype=float32_ref>\n",
      "<tf.Variable '160_fc/kernel:0' shape=(7, 7, 256, 256) dtype=float32_ref>\n",
      "<tf.Variable '160_fc/bias:0' shape=(256,) dtype=float32_ref>\n",
      "<tf.Variable '161_fc/kernel:0' shape=(7, 7, 256, 256) dtype=float32_ref>\n",
      "<tf.Variable '161_fc/bias:0' shape=(256,) dtype=float32_ref>\n",
      "<tf.Variable '162_fc/kernel:0' shape=(7, 7, 256, 256) dtype=float32_ref>\n",
      "<tf.Variable '162_fc/bias:0' shape=(256,) dtype=float32_ref>\n",
      "<tf.Variable '163_fc/kernel:0' shape=(7, 7, 256, 256) dtype=float32_ref>\n",
      "<tf.Variable '163_fc/bias:0' shape=(256,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -4.47034836e-08  -4.32133675e-07  -1.49011612e-07  -1.78813934e-07\n",
      "      2.38418579e-07   4.84287739e-08   1.19209290e-07   0.00000000e+00\n",
      "     -2.08616257e-07  -1.49011612e-07   2.98023224e-07   4.47034836e-08\n",
      "      5.96046448e-08  -5.36441803e-07   1.04308128e-07  -5.96046448e-08\n",
      "      2.98023224e-08   5.21540642e-08   7.45058060e-08  -1.49011612e-07\n",
      "      8.94069672e-07   0.00000000e+00   2.68220901e-07   9.49949026e-08\n",
      "     -9.68575478e-08   5.96046448e-07  -5.96046448e-08   7.45058060e-08\n",
      "      4.17232513e-07  -4.47034836e-08  -8.94069672e-08   2.98023224e-08\n",
      "      3.57627869e-07   3.57627869e-07   2.70083547e-08  -6.33299351e-08\n",
      "     -2.38418579e-07   3.72529030e-08  -1.26659870e-07   5.96046448e-08\n",
      "      1.49011612e-08  -1.63912773e-07  -2.68220901e-07   1.86264515e-08\n",
      "      8.04662704e-07  -3.27825546e-07   2.38418579e-07  -8.94069672e-07\n",
      "      0.00000000e+00  -1.19209290e-07   2.08616257e-07  -1.53668225e-08\n",
      "      1.49011612e-08   0.00000000e+00   4.47034836e-08  -1.04308128e-07\n",
      "      0.00000000e+00  -1.49011612e-07  -6.23986125e-08   5.77419996e-08\n",
      "     -1.78813934e-07  -1.49011612e-07  -1.78813934e-07  -5.96046448e-08\n",
      "      1.04308128e-07   8.94069672e-08  -8.19563866e-08   2.68220901e-07\n",
      "      1.19209290e-07   2.23517418e-08  -9.31322575e-08   1.63912773e-07\n",
      "      5.96046448e-08   4.47034836e-08  -4.76837158e-07  -1.63912773e-07\n",
      "     -1.49011612e-07  -2.68220901e-07  -5.96046448e-07  -4.47034836e-08\n",
      "     -1.86264515e-08   6.33299351e-08   8.94069672e-08   8.94069672e-08\n",
      "      2.38418579e-07  -2.08616257e-07   1.11758709e-08  -2.98023224e-08\n",
      "      0.00000000e+00   0.00000000e+00  -1.34110451e-07  -1.49011612e-07\n",
      "     -7.45058060e-09  -1.19209290e-07   1.49011612e-08  -1.04308128e-07\n",
      "     -1.78813934e-07   2.98023224e-08  -4.47034836e-08  -8.94069672e-08\n",
      "     -2.38418579e-07   4.09781933e-08  -5.96046448e-08  -2.08616257e-07\n",
      "     -8.94069672e-08  -1.78813934e-07   3.57627869e-07  -2.08616257e-07\n",
      "      2.08616257e-07   4.47034836e-07   1.63912773e-07   0.00000000e+00\n",
      "     -1.78813934e-07   0.00000000e+00  -1.78813934e-07   2.30967999e-07\n",
      "     -5.21540642e-08   1.37835741e-07  -1.78813934e-07   1.49011612e-08\n",
      "      9.68575478e-08  -1.19209290e-07  -4.47034836e-07  -2.23517418e-08\n",
      "      2.23517418e-07   3.72529030e-08  -5.58793545e-09   1.49011612e-08\n",
      "     -5.96046448e-08  -1.49011612e-08  -2.68220901e-07   1.49011612e-07\n",
      "     -7.15255737e-07  -3.27825546e-07   2.23517418e-07   5.36441803e-07\n",
      "      2.08616257e-07  -2.98023224e-07  -4.47034836e-08  -1.04308128e-07\n",
      "     -2.98023224e-08  -7.45058060e-09  -2.08616257e-07   5.51342964e-07\n",
      "      1.19209290e-07  -2.68220901e-07   7.45058060e-09   1.78813934e-07\n",
      "     -2.98023224e-07   1.19209290e-07   4.84287739e-08   5.96046448e-08\n",
      "      3.83704901e-07   8.19563866e-08   4.84287739e-08  -7.45058060e-08\n",
      "      2.98023224e-07  -1.17346644e-07   7.45058060e-09  -2.98023224e-08\n",
      "     -3.35276127e-08   7.45058060e-08  -2.38418579e-07  -2.98023224e-08\n",
      "      0.00000000e+00  -4.02331352e-07   2.98023224e-08  -1.49011612e-08\n",
      "      1.19209290e-07  -1.34110451e-07   5.21540642e-08   0.00000000e+00\n",
      "     -2.98023224e-08  -1.04308128e-07   6.70552254e-08  -1.78813934e-07\n",
      "     -2.38418579e-07   7.82310963e-08  -2.98023224e-08  -3.57627869e-07\n",
      "     -1.78813934e-07   0.00000000e+00   3.12924385e-07   5.66244125e-07\n",
      "      2.04890966e-08  -3.57627869e-07   1.04308128e-07   1.01327896e-06\n",
      "     -8.94069672e-08  -5.96046448e-08   1.13621354e-07  -1.78813934e-07\n",
      "     -4.76837158e-07   8.34465027e-07   6.55651093e-07   1.93715096e-07\n",
      "      5.96046448e-08   1.63912773e-07   1.49011612e-07   1.49011612e-07\n",
      "     -2.53319740e-07  -1.49011612e-07   2.38418579e-07  -2.38418579e-07\n",
      "     -1.34110451e-07   5.96046448e-08  -3.20374966e-07  -1.86264515e-08\n",
      "     -1.49011612e-08  -1.49011612e-08   4.17232513e-07  -2.83122063e-07\n",
      "     -7.82310963e-08  -1.49011612e-08   5.21540642e-08   1.49011612e-08\n",
      "      1.49011612e-08  -2.53319740e-07  -9.68575478e-08   1.49011612e-07\n",
      "      4.47034836e-08  -1.19209290e-07  -4.09781933e-08  -4.76837158e-07\n",
      "     -3.12924385e-07   8.94069672e-08  -3.35276127e-08   1.19209290e-07\n",
      "      1.49011612e-07   7.45058060e-09   1.19209290e-07   1.11758709e-08\n",
      "      1.04308128e-07  -1.11758709e-07   0.00000000e+00  -5.02914190e-08\n",
      "     -1.04308128e-07   5.96046448e-08   1.49011612e-08  -1.49011612e-08\n",
      "     -2.98023224e-08  -1.89989805e-07  -4.47034836e-08  -4.09781933e-08\n",
      "      7.45058060e-08   1.34110451e-07   2.68220901e-07  -3.72529030e-08\n",
      "      2.23517418e-08   2.38418579e-07   8.94069672e-08   1.04308128e-07\n",
      "      1.34110451e-07  -2.23517418e-08   2.98023224e-08  -1.63912773e-07]]]]\n"
     ]
    }
   ],
   "source": [
    "# run session\n",
    "var = sess.run(in_split0)\n",
    "in_split_dict = {in_split: var}\n",
    "out_0 = sess.run(fc_16_split0,feed_dict=in_split_dict)\n",
    "var = sess.run(in_split1)\n",
    "in_split_dict = {in_split: var}\n",
    "out_1 = sess.run(fc_16_split1,feed_dict=in_split_dict)\n",
    "var = sess.run(in_split2)\n",
    "in_split_dict = {in_split: var}\n",
    "out_2 = sess.run(fc_16_split2,feed_dict=in_split_dict)\n",
    "var = sess.run(in_split3)\n",
    "in_split_dict = {in_split: var}\n",
    "out_3 = sess.run(fc_16_split3,feed_dict=in_split_dict)\n",
    "#print(out_0)\n",
    "print(out_0+out_1+out_2+out_3-fc_16_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
